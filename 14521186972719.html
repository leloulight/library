<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Hadoop 指南 - 小土刀的笔记
  
  </title>
 <meta name="description" content="">
 <link href="atom.xml" rel="alternate" title="小土刀的笔记" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html">小土刀的笔记</a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; 小土刀的笔记</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>清单</label></li>

          
            <li><a title="好问题收集" href="14520379720179.html">好问题收集</a></li>
          
            <li><a title="书单合集" href="14520379720136.html">书单合集</a></li>
          
            <li><a title="日志列表" href="14520379720098.html">日志列表</a></li>
          
            <li><a title="玩物列表" href="14520379720223.html">玩物列表</a></li>
          

      
        <li class="divider"></li>
        <li><label>计算机学科经典</label></li>

          
            <li><a title="大教堂与集市" href="14520876353363.html">大教堂与集市</a></li>
          
            <li><a title="深入理解计算机系统" href="14520855371865.html">深入理解计算机系统</a></li>
          
            <li><a title="Python Algorithm" href="14521091296885.html">Python Algorithm</a></li>
          
            <li><a title="程序员的思维修炼" href="14520875018555.html">程序员的思维修炼</a></li>
          
            <li><a title="程序员修炼之道" href="14520874386531.html">程序员修炼之道</a></li>
          
            <li><a title="高效程序员的45个习惯" href="14520878121031.html">高效程序员的45个习惯</a></li>
          
            <li><a title="代码大全" href="14520855371782.html">代码大全</a></li>
          
            <li><a title="编程珠玑" href="14521308341315.html">编程珠玑</a></li>
          
            <li><a title="编写可读代码的艺术" href="14520874726715.html">编写可读代码的艺术</a></li>
          
            <li><a title="移动应用UI设计模式" href="14520886280669.html">移动应用UI设计模式</a></li>
          
            <li><a title="启示录：打造用户喜爱的产品" href="14520907410466.html">启示录：打造用户喜爱的产品</a></li>
          
            <li><a title="亲爱的界面：让用户乐于使用、爱不释手" href="14520885573483.html">亲爱的界面：让用户乐于使用、爱不释手</a></li>
          
            <li><a title="简约之美：软件设计之道" href="14520879229686.html">简约之美：软件设计之道</a></li>
          
            <li><a title="简单之美：软件开发实践者的思考" href="14520878623936.html">简单之美：软件开发实践者的思考</a></li>
          
            <li><a title="构建之法：现代软件工程" href="14520880342127.html">构建之法：现代软件工程</a></li>
          
            <li><a title="软件项目成功之道" href="14520883368873.html">软件项目成功之道</a></li>
          

      
        <li class="divider"></li>
        <li><label>Research</label></li>

          
            <li><a title="知识图谱沉思录" href="14520583101537.html">知识图谱沉思录</a></li>
          
            <li><a title="计算机视觉沉思录" href="14520483585854.html">计算机视觉沉思录</a></li>
          
            <li><a title="推荐系统沉思录" href="14520495721664.html">推荐系统沉思录</a></li>
          
            <li><a title="数据分析沉思录" href="14520494829635.html">数据分析沉思录</a></li>
          
            <li><a title="机器学习沉思录" href="14520493984651.html">机器学习沉思录</a></li>
          
            <li><a title="3D 打印沉思录" href="14520902603411.html">3D 打印沉思录</a></li>
          
            <li><a title="HMM 指南" href="14520495021796.html">HMM 指南</a></li>
          
            <li><a title="EM / GMM 指南" href="14520494287936.html">EM / GMM 指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>iOS</label></li>

          
            <li><a title="iOS 学习路径" href="14520467690026.html">iOS 学习路径</a></li>
          
            <li><a title="iOS 面试相关" href="14520467689968.html">iOS 面试相关</a></li>
          

      
        <li class="divider"></li>
        <li><label>游戏</label></li>

          
            <li><a title="深入游戏" href="14521282739436.html">深入游戏</a></li>
          
            <li><a title="游戏设计" href="14521282739338.html">游戏设计</a></li>
          
            <li><a title="英雄联盟游戏设计与运营技巧" href="14521282739242.html">英雄联盟游戏设计与运营技巧</a></li>
          
            <li><a title="游戏运营指南" href="14521282739132.html">游戏运营指南</a></li>
          
            <li><a title="游戏的架构与细节梳理" href="14521282739025.html">游戏的架构与细节梳理</a></li>
          
            <li><a title="Destiny 游戏分析" href="14521282738926.html">Destiny 游戏分析</a></li>
          
            <li><a title="辐射避难所" href="14521277966471.html">辐射避难所</a></li>
          
            <li><a title="英雄联盟攻略" href="14521196095413.html">英雄联盟攻略</a></li>
          
            <li><a title="游戏产业信息收集" href="14521195111050.html">游戏产业信息收集</a></li>
          
            <li><a title="游戏发展史" href="14521194839446.html">游戏发展史</a></li>
          
            <li><a title="辐射系列" href="14521194076173.html">辐射系列</a></li>
          

      
        <li class="divider"></li>
        <li><label>生活品质</label></li>

          
            <li><a title="成为作家" href="14521287232350.html">成为作家</a></li>
          
            <li><a title="广州美食地图" href="14521207453879.html">广州美食地图</a></li>
          
            <li><a title="聊聊钢笔" href="14521202963339.html">聊聊钢笔</a></li>
          
            <li><a title="聊聊电影" href="14521201092159.html">聊聊电影</a></li>
          

      
        <li class="divider"></li>
        <li><label>生活技巧</label></li>

          
            <li><a title="烤箱指南" href="14521207693233.html">烤箱指南</a></li>
          
            <li><a title="租房手册" href="14521200170697.html">租房手册</a></li>
          
            <li><a title="买车指南" href="14521200170757.html">买车指南</a></li>
          
            <li><a title="北美二手车购买指南" href="14521200391261.html">北美二手车购买指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>工具平台</label></li>

          
            <li><a title="Mac 指南" href="14520959875185.html">Mac 指南</a></li>
          
            <li><a title="Bash 指南" href="14520940884566.html">Bash 指南</a></li>
          
            <li><a title="Hexo 指南" href="14520953748408.html">Hexo 指南</a></li>
          
            <li><a title="VS Code 指南" href="14520964818956.html">VS Code 指南</a></li>
          
            <li><a title="Homebrew 指南" href="14520954251052.html">Homebrew 指南</a></li>
          
            <li><a title="Hadoop 指南" href="14521186972719.html">Hadoop 指南</a></li>
          
            <li><a title="Pandoc 指南" href="14520962259349.html">Pandoc 指南</a></li>
          
            <li><a title="聊聊 PPT" href="14521293989573.html">聊聊 PPT</a></li>
          
            <li><a title="Vim 指南" href="14520964365928.html">Vim 指南</a></li>
          
            <li><a title="Sublime 指南" href="14520965272426.html">Sublime 指南</a></li>
          
            <li><a title="GFW 原理指南" href="14521185605970.html">GFW 原理指南</a></li>
          
            <li><a title="Linux 的概念与体系" href="14521103801032.html">Linux 的概念与体系</a></li>
          
            <li><a title="Appstore 生存指南" href="14521185605800.html">Appstore 生存指南</a></li>
          
            <li><a title="Latex 指南" href="14520955052445.html">Latex 指南</a></li>
          
            <li><a title="Git 指南" href="14520951022304.html">Git 指南</a></li>
          
            <li><a title="Github 生活指南" href="14520951022388.html">Github 生活指南</a></li>
          
            <li><a title="iTerm2 指南" href="14520954757815.html">iTerm2 指南</a></li>
          
            <li><a title="Ubuntu 指南" href="14520961679182.html">Ubuntu 指南</a></li>
          
            <li><a title="谷歌搜索技巧" href="14520951022464.html">谷歌搜索技巧</a></li>
          
            <li><a title="Make 指南" href="14520961098604.html">Make 指南</a></li>
          
            <li><a title="GCC 简易指南" href="14520948089088.html">GCC 简易指南</a></li>
          
            <li><a title="Kinect 开发指南" href="14521185098750.html">Kinect 开发指南</a></li>
          
            <li><a title="fish shell 指南" href="14520943310829.html">fish shell 指南</a></li>
          
            <li><a title="Gradle 指南" href="14520951022555.html">Gradle 指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>编程语言</label></li>

          
            <li><a title="如何选择编程语言" href="14521020035250.html">如何选择编程语言</a></li>
          
            <li><a title="Python 学习指南" href="14521095530236.html">Python 学习指南</a></li>
          
            <li><a title="Python 编程思想" href="14521096832549.html">Python 编程思想</a></li>
          
            <li><a title="Java 精要" href="14521089938127.html">Java 精要</a></li>
          
            <li><a title="Matlab 指南" href="14521020962952.html">Matlab 指南</a></li>
          
            <li><a title="OpenMP 入门指南" href="14521021178707.html">OpenMP 入门指南</a></li>
          
            <li><a title="Javascript 学习指南" href="14521020732534.html">Javascript 学习指南</a></li>
          
            <li><a title="C++ 学习笔记" href="14520969294593.html">C++ 学习笔记</a></li>
          
            <li><a title="SQL 入门指南" href="14521021831775.html">SQL 入门指南</a></li>
          
            <li><a title="CUDA架构" href="14521020248399.html">CUDA架构</a></li>
          

      
        <li class="divider"></li>
        <li><label>Android</label></li>

          
            <li><a title="Android Studio 指南" href="14520922857884.html">Android Studio 指南</a></li>
          
            <li><a title="Google Map V2 使用指南" href="14520474070782.html">Google Map V2 使用指南</a></li>
          
            <li><a title="ActionBar 使用指南" href="14520474070736.html">ActionBar 使用指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>Web</label></li>

          
            <li><a title="网络协议指南" href="14520476779491.html">网络协议指南</a></li>
          
            <li><a title="Node.js 命令行程序实例" href="14520476779456.html">Node.js 命令行程序实例</a></li>
          
            <li><a title="Flask Study Note" href="14520476779406.html">Flask Study Note</a></li>
          

      
        <li class="divider"></li>
        <li><label>关于</label></li>

          
            <li><a title="版权声明" href="14520576386063.html">版权声明</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>清单</span></li>
                        
                          <li><a title="好问题收集" href="14520379720179.html">好问题收集</a></li>
                        
                          <li><a title="书单合集" href="14520379720136.html">书单合集</a></li>
                        
                          <li><a title="日志列表" href="14520379720098.html">日志列表</a></li>
                        
                          <li><a title="玩物列表" href="14520379720223.html">玩物列表</a></li>
                        

                    
                      <li class="side-title"><span>计算机学科经典</span></li>
                        
                          <li><a title="大教堂与集市" href="14520876353363.html">大教堂与集市</a></li>
                        
                          <li><a title="深入理解计算机系统" href="14520855371865.html">深入理解计算机系统</a></li>
                        
                          <li><a title="Python Algorithm" href="14521091296885.html">Python Algorithm</a></li>
                        
                          <li><a title="程序员的思维修炼" href="14520875018555.html">程序员的思维修炼</a></li>
                        
                          <li><a title="程序员修炼之道" href="14520874386531.html">程序员修炼之道</a></li>
                        
                          <li><a title="高效程序员的45个习惯" href="14520878121031.html">高效程序员的45个习惯</a></li>
                        
                          <li><a title="代码大全" href="14520855371782.html">代码大全</a></li>
                        
                          <li><a title="编程珠玑" href="14521308341315.html">编程珠玑</a></li>
                        
                          <li><a title="编写可读代码的艺术" href="14520874726715.html">编写可读代码的艺术</a></li>
                        
                          <li><a title="移动应用UI设计模式" href="14520886280669.html">移动应用UI设计模式</a></li>
                        
                          <li><a title="启示录：打造用户喜爱的产品" href="14520907410466.html">启示录：打造用户喜爱的产品</a></li>
                        
                          <li><a title="亲爱的界面：让用户乐于使用、爱不释手" href="14520885573483.html">亲爱的界面：让用户乐于使用、爱不释手</a></li>
                        
                          <li><a title="简约之美：软件设计之道" href="14520879229686.html">简约之美：软件设计之道</a></li>
                        
                          <li><a title="简单之美：软件开发实践者的思考" href="14520878623936.html">简单之美：软件开发实践者的思考</a></li>
                        
                          <li><a title="构建之法：现代软件工程" href="14520880342127.html">构建之法：现代软件工程</a></li>
                        
                          <li><a title="软件项目成功之道" href="14520883368873.html">软件项目成功之道</a></li>
                        

                    
                      <li class="side-title"><span>Research</span></li>
                        
                          <li><a title="知识图谱沉思录" href="14520583101537.html">知识图谱沉思录</a></li>
                        
                          <li><a title="计算机视觉沉思录" href="14520483585854.html">计算机视觉沉思录</a></li>
                        
                          <li><a title="推荐系统沉思录" href="14520495721664.html">推荐系统沉思录</a></li>
                        
                          <li><a title="数据分析沉思录" href="14520494829635.html">数据分析沉思录</a></li>
                        
                          <li><a title="机器学习沉思录" href="14520493984651.html">机器学习沉思录</a></li>
                        
                          <li><a title="3D 打印沉思录" href="14520902603411.html">3D 打印沉思录</a></li>
                        
                          <li><a title="HMM 指南" href="14520495021796.html">HMM 指南</a></li>
                        
                          <li><a title="EM / GMM 指南" href="14520494287936.html">EM / GMM 指南</a></li>
                        

                    
                      <li class="side-title"><span>iOS</span></li>
                        
                          <li><a title="iOS 学习路径" href="14520467690026.html">iOS 学习路径</a></li>
                        
                          <li><a title="iOS 面试相关" href="14520467689968.html">iOS 面试相关</a></li>
                        

                    
                      <li class="side-title"><span>游戏</span></li>
                        
                          <li><a title="深入游戏" href="14521282739436.html">深入游戏</a></li>
                        
                          <li><a title="游戏设计" href="14521282739338.html">游戏设计</a></li>
                        
                          <li><a title="英雄联盟游戏设计与运营技巧" href="14521282739242.html">英雄联盟游戏设计与运营技巧</a></li>
                        
                          <li><a title="游戏运营指南" href="14521282739132.html">游戏运营指南</a></li>
                        
                          <li><a title="游戏的架构与细节梳理" href="14521282739025.html">游戏的架构与细节梳理</a></li>
                        
                          <li><a title="Destiny 游戏分析" href="14521282738926.html">Destiny 游戏分析</a></li>
                        
                          <li><a title="辐射避难所" href="14521277966471.html">辐射避难所</a></li>
                        
                          <li><a title="英雄联盟攻略" href="14521196095413.html">英雄联盟攻略</a></li>
                        
                          <li><a title="游戏产业信息收集" href="14521195111050.html">游戏产业信息收集</a></li>
                        
                          <li><a title="游戏发展史" href="14521194839446.html">游戏发展史</a></li>
                        
                          <li><a title="辐射系列" href="14521194076173.html">辐射系列</a></li>
                        

                    
                      <li class="side-title"><span>生活品质</span></li>
                        
                          <li><a title="成为作家" href="14521287232350.html">成为作家</a></li>
                        
                          <li><a title="广州美食地图" href="14521207453879.html">广州美食地图</a></li>
                        
                          <li><a title="聊聊钢笔" href="14521202963339.html">聊聊钢笔</a></li>
                        
                          <li><a title="聊聊电影" href="14521201092159.html">聊聊电影</a></li>
                        

                    
                      <li class="side-title"><span>生活技巧</span></li>
                        
                          <li><a title="烤箱指南" href="14521207693233.html">烤箱指南</a></li>
                        
                          <li><a title="租房手册" href="14521200170697.html">租房手册</a></li>
                        
                          <li><a title="买车指南" href="14521200170757.html">买车指南</a></li>
                        
                          <li><a title="北美二手车购买指南" href="14521200391261.html">北美二手车购买指南</a></li>
                        

                    
                      <li class="side-title"><span>工具平台</span></li>
                        
                          <li><a title="Mac 指南" href="14520959875185.html">Mac 指南</a></li>
                        
                          <li><a title="Bash 指南" href="14520940884566.html">Bash 指南</a></li>
                        
                          <li><a title="Hexo 指南" href="14520953748408.html">Hexo 指南</a></li>
                        
                          <li><a title="VS Code 指南" href="14520964818956.html">VS Code 指南</a></li>
                        
                          <li><a title="Homebrew 指南" href="14520954251052.html">Homebrew 指南</a></li>
                        
                          <li><a title="Hadoop 指南" href="14521186972719.html">Hadoop 指南</a></li>
                        
                          <li><a title="Pandoc 指南" href="14520962259349.html">Pandoc 指南</a></li>
                        
                          <li><a title="聊聊 PPT" href="14521293989573.html">聊聊 PPT</a></li>
                        
                          <li><a title="Vim 指南" href="14520964365928.html">Vim 指南</a></li>
                        
                          <li><a title="Sublime 指南" href="14520965272426.html">Sublime 指南</a></li>
                        
                          <li><a title="GFW 原理指南" href="14521185605970.html">GFW 原理指南</a></li>
                        
                          <li><a title="Linux 的概念与体系" href="14521103801032.html">Linux 的概念与体系</a></li>
                        
                          <li><a title="Appstore 生存指南" href="14521185605800.html">Appstore 生存指南</a></li>
                        
                          <li><a title="Latex 指南" href="14520955052445.html">Latex 指南</a></li>
                        
                          <li><a title="Git 指南" href="14520951022304.html">Git 指南</a></li>
                        
                          <li><a title="Github 生活指南" href="14520951022388.html">Github 生活指南</a></li>
                        
                          <li><a title="iTerm2 指南" href="14520954757815.html">iTerm2 指南</a></li>
                        
                          <li><a title="Ubuntu 指南" href="14520961679182.html">Ubuntu 指南</a></li>
                        
                          <li><a title="谷歌搜索技巧" href="14520951022464.html">谷歌搜索技巧</a></li>
                        
                          <li><a title="Make 指南" href="14520961098604.html">Make 指南</a></li>
                        
                          <li><a title="GCC 简易指南" href="14520948089088.html">GCC 简易指南</a></li>
                        
                          <li><a title="Kinect 开发指南" href="14521185098750.html">Kinect 开发指南</a></li>
                        
                          <li><a title="fish shell 指南" href="14520943310829.html">fish shell 指南</a></li>
                        
                          <li><a title="Gradle 指南" href="14520951022555.html">Gradle 指南</a></li>
                        

                    
                      <li class="side-title"><span>编程语言</span></li>
                        
                          <li><a title="如何选择编程语言" href="14521020035250.html">如何选择编程语言</a></li>
                        
                          <li><a title="Python 学习指南" href="14521095530236.html">Python 学习指南</a></li>
                        
                          <li><a title="Python 编程思想" href="14521096832549.html">Python 编程思想</a></li>
                        
                          <li><a title="Java 精要" href="14521089938127.html">Java 精要</a></li>
                        
                          <li><a title="Matlab 指南" href="14521020962952.html">Matlab 指南</a></li>
                        
                          <li><a title="OpenMP 入门指南" href="14521021178707.html">OpenMP 入门指南</a></li>
                        
                          <li><a title="Javascript 学习指南" href="14521020732534.html">Javascript 学习指南</a></li>
                        
                          <li><a title="C++ 学习笔记" href="14520969294593.html">C++ 学习笔记</a></li>
                        
                          <li><a title="SQL 入门指南" href="14521021831775.html">SQL 入门指南</a></li>
                        
                          <li><a title="CUDA架构" href="14521020248399.html">CUDA架构</a></li>
                        

                    
                      <li class="side-title"><span>Android</span></li>
                        
                          <li><a title="Android Studio 指南" href="14520922857884.html">Android Studio 指南</a></li>
                        
                          <li><a title="Google Map V2 使用指南" href="14520474070782.html">Google Map V2 使用指南</a></li>
                        
                          <li><a title="ActionBar 使用指南" href="14520474070736.html">ActionBar 使用指南</a></li>
                        

                    
                      <li class="side-title"><span>Web</span></li>
                        
                          <li><a title="网络协议指南" href="14520476779491.html">网络协议指南</a></li>
                        
                          <li><a title="Node.js 命令行程序实例" href="14520476779456.html">Node.js 命令行程序实例</a></li>
                        
                          <li><a title="Flask Study Note" href="14520476779406.html">Flask Study Note</a></li>
                        

                    
                      <li class="side-title"><span>关于</span></li>
                        
                          <li><a title="版权声明" href="14520576386063.html">版权声明</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">

 <div class="markdown-body">
<h1>Hadoop 指南</h1>

<h2 id="toc_0">概述</h2>

<p>Hadoop Map/Reduce 是一个使用简易的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p>

<p>一个 Map/Reduce 作业（job） 通常会把输入的数据集切分为若干独立的数据块，由  map 任务（task）以完全并行的方式处理它们。框架会对 map 的输出先进行排序， 然后把结果输入给 reduce 任务。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p>

<p>通常，Map/Reduce 框架和分布式文件系统是运行在一组相同的节点上的，也就是说，计算节点和存储节点通常在一起。这种配置允许框架在那些已经存好数据的节点上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。</p>

<p>Map/Reduce 框架由一个单独的 master JobTracker 和每个集群节点一个 slave TaskTracker共同组成。master 负责调度构成一个作业的所有任务，这些任务分布在不同的 slave 上，master 监控它们的执行，重新执行已经失败的任务。而 slave 仅负责执行由 master 指派的任务。</p>

<p>应用程序至少应该指明输入/输出的位置（路径），并通过实现合适的接口或抽象类提供 map 和 reduce 函数。再加上其他作业的参数，就构成了作业配置（job configuration）。然后，Hadoop 的 job client 提交作业（jar包/可执行程序等）和配置信息给 JobTracker，后者负责分发这些软件和配置信息给 slave、调度任务并监控它们的执行，同时提供状态和诊断信息给 job-client。</p>

<p>虽然 Hadoop 框架是用 JavaTM 实现的，但 Map/Reduce 应用程序则不一定要用 Java 来写 。</p>

<h2 id="toc_1">输入与输出</h2>

<p>Map/Reduce框架运转在<code>&lt;key, value&gt;</code> 键值对上，也就是说， 框架把作业的输入看为是一组<code>&lt;key, value&gt;</code> 键值对，同样也产出一组 <code>&lt;key, value&gt;</code> 键值对做为作业的输出，这两组键值对的类型可能不同。</p>

<p>框架需要对 key 和 value 的类(classes)进行序列化操作， 因此，这些类需要实现 Writable接口。另外，为了方便框架执行排序操作，key 类必须实现 WritableComparable接口。</p>

<p>一个Map/Reduce 作业的输入和输出类型如下所示：</p>

<pre><code>(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)
</code></pre>

<h2 id="toc_2">核心功能描述</h2>

<p>应用程序通常会通过提供map和reduce来实现 Mapper 和 Reducer 接口，它们组成作业的核心。</p>

<h3 id="toc_3">Mapper</h3>

<p>Mapper 将输入键值对(key/value pair)映射到一组中间格式的键值对集合。</p>

<p>Map 是一类将输入记录集转换为中间格式记录集的独立任务。 这种转换的中间格式记录集不需要与输入记录集的类型一致。一个给定的输入键值对可以映射成0个或多个输出键值对。</p>

<p>Hadoop Map/Reduce 框架为每一个 InputSplit 产生一个map任务，而每个InputSplit是由该作业的InputFormat产生的。</p>

<p>概括地说，对 Mapper 的实现者需要重写 JobConfigurable.configure(JobConf) 方法，这个方法需要传递一个 JobConf 参数，目的是完成 Mapper 的初始化工作。然后，框架为这个任务的 InputSplit 中每个键值对调用一次 map(WritableComparable, Writable, OutputCollector, Reporter)操作。应用程序可以通过重写 Closeable.close() 方法来执行相应的清理工作。</p>

<p>输出键值对不需要与输入键值对的类型一致。一个给定的输入键值对可以映射成0个或多个输出键值对。通过调用 OutputCollector.collect(WritableComparable,Writable)可以收集输出的键值对。</p>

<p>应用程序可以使用Reporter报告进度，设定应用级别的状态消息，更新Counters（计数器），或者仅是表明自己运行正常。</p>

<p>框架随后会把与一个特定 key 关联的所有中间过程的值（value）分成组，然后把它们传给 Reducer 以产出最终的结果。用户可以通过 JobConf.setOutputKeyComparatorClass(Class) 来指定具体负责分组的 Comparator。</p>

<p>Mapper 的输出被排序后，就被划分给每个 Reducer。分块的总数目和一个作业的 reduce 任务的数目是一样的。用户可以通过实现自定义的 Partitioner 来控制哪个key被分配给哪个 Reducer。</p>

<p>用户可选择通过 JobConf.setCombinerClass(Class)指定一个 combiner，它负责对中间过程的输出进行本地的聚集，这会有助于降低从 Mapper 到 Reducer 数据传输量。</p>

<p>这些被排好序的中间过程的输出结果保存的格式是(key-len, key, value-len, value)，应用程序可以通过JobConf控制对这些中间结果是否进行压缩以及怎么压缩，使用哪种 CompressionCodec。</p>

<h3 id="toc_4">需要多少个Map？</h3>

<p>Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块（block）数。</p>

<p>Map 正常的并行规模大致是每个节点（node）大约 10 到 100 个 map，对于 CPU 消耗较小的map 任务可以设到300个左右。由于每个任务初始化需要一定的时间，因此，比较合理的情况是 map 执行的时间至少超过1分钟。</p>

<p>这样，如果你输入 10TB 的数据，每个块（block）的大小是128MB，你将需要大约82,000个map来完成任务，除非使用 setNumMapTasks(int)（注意：这里仅仅是对框架进行了一个提示(hint)，实际决定因素见这里）将这个数值设置得更高。</p>

<h3 id="toc_5">Reducer</h3>

<p>Reducer 将与一个 key 关联的一组中间数值集归约（reduce）为一个更小的数值集。</p>

<p>用户可以通过 JobConf.setNumReduceTasks(int) 设定一个作业中 reduce 任务的数目。</p>

<p>概括地说，对 Reducer 的实现者需要重写 JobConfigurable.configure(JobConf) 方法，这个方法需要传递一个 JobConf 参数，目的是完成 Reducer 的初始化工作。然后，框架为成组的输入数据中的每个<code>&lt;key, (list of values)&gt;</code>对调用一次 reduce(WritableComparable, Iterator, OutputCollector, Reporter)方法。之后，应用程序可以通过重写 Closeable.close() 来执行相应的清理工作。</p>

<p>Reducer 有3个主要阶段：shuffle、sort 和 reduce。</p>

<h3 id="toc_6">Shuffle</h3>

<p>Reducer 的输入就是 Mapper 已经排好序的输出。在这个阶段，框架通过 HTTP 为每个 Reducer 获得所有 Mapper 输出中与之相关的分块。</p>

<h3 id="toc_7">Sort</h3>

<p>这个阶段，框架将按照 key 的值对 Reducer 的输入进行分组 （因为不同 mapper 的输出中可能会有相同的 key）。</p>

<p>Shuffle 和 Sort 两个阶段是同时进行的；map 的输出也是一边被取回一边被合并的。</p>

<h3 id="toc_8">Secondary Sort</h3>

<p>如果需要中间过程对 key 的分组规则和 reduce 前对 key 的分组规则不同，那么可以通过 JobConf.setOutputValueGroupingComparator(Class) 来指定一个 Comparator。再加上 JobConf.setOutputKeyComparatorClass(Class)可用于控制中间过程的 key 如何被分组，所以结合两者可以实现按值的二次排序。</p>

<h3 id="toc_9">Reduce</h3>

<p>在这个阶段，框架为已分组的输入数据中的每个 <code>&lt;key, (list of values)&gt;</code>对调用一次 reduce(WritableComparable, Iterator, OutputCollector, Reporter)方法。</p>

<p>Reduce任务的输出通常是通过调用 OutputCollector.collect(WritableComparable, Writable)写入 文件系统的。</p>

<p>应用程序可以使用 Reporter 报告进度，设定应用程序级别的状态消息，更新 Counters（计数器），或者仅是表明自己运行正常。</p>

<p>Reducer 的输出是没有排序的。</p>

<h3 id="toc_10">需要多少个Reduce？</h3>

<p>Reduce的数目建议是0.95或1.75乘以 <code>&lt;no. of nodes&gt; * mapred.tasktracker.reduce.tasks.maximum</code>。</p>

<p>用 0.95，所有 reduce 可以在 maps 一完成时就立刻启动，开始传输 map 的输出结果。用1.75，速度快的节点可以在完成第一轮 reduce 任务后，可以开始第二轮，这样可以得到比较好的负载均衡的效果。</p>

<p>增加 reduce 的数目会增加整个框架的开销，但可以改善负载均衡，降低由于执行失败带来的负面影响。</p>

<p>上述比例因子比整体数目稍小一些是为了给框架中的推测性任务（speculative-tasks） 或失败的任务预留一些 reduce 的资源。</p>

<h3 id="toc_11">无 Reducer</h3>

<p>如果没有归约要进行，那么设置 reduce 任务的数目为零是合法的。</p>

<p>这种情况下，map任务的输出会直接被写入由 setOutputPath(Path)指定的输出路径。框架在把它们写入 FileSystem 之前没有对它们进行排序。</p>

<h3 id="toc_12">Partitioner</h3>

<p>Partitioner 用于划分键值空间（key space）。</p>

<p>Partitioner 负责控制 map 输出结果 key 的分割。Key（或者一个key子集）被用于产生分区，通常使用的是 Hash 函数。分区的数目与一个作业的 reduce 任务的数目是一样的。因此，它控制将中间过程的key（也就是这条记录）应该发送给 m 个 reduce 任务中的哪一个来进行 reduce操作。</p>

<p>HashPartitioner 是默认的 Partitioner。</p>

<h3 id="toc_13">Reporter</h3>

<p>Reporter 是用于 Map/Reduce 应用程序报告进度，设定应用级别的状态消息， 更新 Counters（计数器）的机制。</p>

<p>Mapper 和 Reducer 的实现可以利用Reporter 来报告进度，或者仅是表明自己运行正常。在那种应用程序需要花很长时间处理个别键值对的场景中，这种机制是很关键的，因为框架可能会以为这个任务超时了，从而将它强行杀死。另一个避免这种情况发生的方式是，将配置参数 <code>mapred.task.timeout</code> 设置为一个足够高的值（或者干脆设置为零，则没有超时限制了）。</p>

<p>应用程序可以用 Reporter 来更新 Counter（计数器）。</p>

<h3 id="toc_14">OutputCollector</h3>

<p>OutputCollector 是一个 Map/Reduce 框架提供的用于收集 Mapper 或 Reducer输出数据的通用机制（包括中间输出结果和作业的输出结果）。</p>

<p>Hadoop Map/Reduce 框架附带了一个包含许多实用型的 mapper、reducer 和 partitioner 的类库。</p>

<h2 id="toc_15">作业配置</h2>

<p>JobConf 代表一个 Map/Reduce 作业的配置。</p>

<p>JobConf 是用户向 Hadoop 框架描述一个 Map/Reduce 作业如何执行的主要接口。框架会按照 JobConf 描述的信息忠实地去尝试完成这个作业，然而：</p>

<ul>
<li>一些参数可能会被管理者标记为 final，这意味它们不能被更改。</li>
<li>一些作业的参数可以被直截了当地进行设置（例如： setNumReduceTasks(int)），而另一些参数则与框架或者作业的其他参数之间微妙地相互影响，并且设置起来比较复杂（例如： setNumMapTasks(int)）。</li>
</ul>

<p>通常，JobConf会指明Mapper、Combiner(如果有的话)、 Partitioner、Reducer、InputFormat和 OutputFormat的具体实现。JobConf还能指定一组输入文件 (setInputPaths(JobConf, Path...) /addInputPath(JobConf, Path)) 和(setInputPaths(JobConf, String) /addInputPaths(JobConf, String)) 以及输出文件应该写在哪儿 (setOutputPath(Path))。</p>

<p>JobConf可选择地对作业设置一些高级选项，例如：设置Comparator； 放到DistributedCache上的文件；中间结果或者作业输出结果是否需要压缩以及怎么压缩； 利用用户提供的脚本(setMapDebugScript(String)/setReduceDebugScript(String)) 进行调试；作业是否允许预防性（speculative）任务的执行 (setMapSpeculativeExecution(boolean))/(setReduceSpeculativeExecution(boolean)) ；每个任务最大的尝试次数 (setMaxMapAttempts(int)/setMaxReduceAttempts(int)) ；一个作业能容忍的任务失败的百分比 (setMaxMapTaskFailuresPercent(int)/setMaxReduceTaskFailuresPercent(int)) ；等等。</p>

<p>当然，用户能使用 set(String, String)/get(String, String) 来设置或者取得应用程序需要的任意参数。然而，DistributedCache的使用是面向大规模只读数据的。</p>

<h2 id="toc_16">任务的执行和环境</h2>

<p>TaskTracker是在一个单独的jvm上以子进程的形式执行 Mapper/Reducer任务（Task）的。</p>

<p>子任务会继承父TaskTracker的环境。用户可以通过JobConf中的 mapred.child.java.opts配置参数来设定子jvm上的附加选项，例如： 通过-Djava.library.path=&lt;&gt; 将一个非标准路径设为运行时的链接用以搜索共享库，等等。如果mapred.child.java.opts包含一个符号@taskid@， 它会被替换成map/reduce的taskid的值。</p>

<p>下面是一个包含多个参数和替换的例子，其中包括：记录jvm GC日志； JVM JMX代理程序以无密码的方式启动，这样它就能连接到jconsole上，从而可以查看子进程的内存和线程，得到线程的dump；还把子jvm的最大堆尺寸设置为512MB， 并为子jvm的java.library.path添加了一个附加路径。</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;mapred.child.java.opts&lt;/name&gt;
  &lt;value&gt;
     -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc
     -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false
  &lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>用户或管理员也可以使用mapred.child.ulimit设定运行的子任务的最大虚拟内存。mapred.child.ulimit的值以（KB)为单位，并且必须大于或等于-Xmx参数传给JavaVM的值，否则VM会无法启动。</p>

<p>注意：mapred.child.java.opts只用于设置task tracker启动的子任务。为守护进程设置内存选项请查看 cluster_setup.html</p>

<h1 id="toc_17">Hadoop 原理学习</h1>

<p>Hadoop 是 Apache 下的一个项目，由 HDFS、MapReduce、HBase、Hive 和 ZooKeeper 等成员组成。其中，HDFS 和 MapReduce 是两个最基础最重要的成员。</p>

<p>HDFS 是 Google GFS 的开源版本，一个高度容错的分布式文件系统，它能够提供高吞吐量的数据访问，适合存储海量(PB 级)的大文件(通常超过64M)，其原理如下图所示：</p>

<p><img src="media/14521186972719/hadoop1.jpg" alt="hadoop1"/></p>

<p>采用 Master/Slave 结构。NameNode 维护集群内的元数据，对外提供创建、打开、删除和重命名文件或目录的功能。DataNode 存储数据，并提负责处理数据的读写请求。DataNode 定期向 NameNode 上报心跳，NameNode 通过响应心跳来控制 DataNode。</p>

<p>Hadoop MapReduce的实现也采用了Master/Slave 结构。Master 叫做 JobTracker，而 Slave 叫做TaskTracker。用户提交的计算叫做 Job，每一个 Job 会被划分成若干个 Tasks。JobTracker 负责 Job 和 Tasks 的调度，而 TaskTracker 负责执行 Tasks。</p>

<h2 id="toc_18">Shuffle 和 Sort 分析</h2>

<p>MapReduce 框架的核心步骤主要分两部分：Map 和 Reduce。当你向 MapReduce 框架提交一个计算作业时，它会首先把计算作业拆分成若干个 Map 任务，然后分配到不同的节点上去执行，每一个 Map 任务处理输入数据中的一部分，当 Map 任务完成后，它会生成一些中间文件，这些中间文件将会作为 Reduce 任务的输入数据。Reduce 任务的主要目标就是把前面若干个 Map 的输出汇总到一起并输出。从高层抽象来看，MapReduce 的数据流图如下图所示：</p>

<p><img src="media/14521186972719/hadoop2.jpg" alt="hadoop2"/></p>

<p>在本文中，Shuffle 是指从 Map 产生输出开始，包括系统执行排序以及传送 Map 输出到 Reducer 作为输入的过程。在这里我们将去探究 Shuffle 是如何工作的，因为对基础的理解有助于对 MapReduce 程序进行调优。</p>

<p>首先从 Map 端开始分析，当 Map 开始产生输出的时候，他并不是简单的把数据写到磁盘，因为频繁的操作会导致性能严重下降，他的处理更加复杂，数据首先是写到内存中的一个缓冲区，并作一些预排序，以提升效率，如图：</p>

<p><img src="media/14521186972719/hadoop3.jpg" alt="hadoop3"/></p>

<p>每个 Map 任务都有一个用来写入输出数据的循环内存缓冲区，这个缓冲区默认大小是 100M，可以通过 <code>io.sort.mb</code> 属性来设置具体的大小，当缓冲区中的数据量达到一个特定的阀值 <code>(io.sort.mb * io.sort.spill.percent，其中io.sort.spill.percent 默认是0.80)</code>时，系统将会启动一个后台线程把缓冲区中的内容 spill 到磁盘。在 spill 过程中，Map 的输出将会继续写入到缓冲区，但如果缓冲区已经满了，Map 就会被阻塞直到 spill 完成。spill 线程在把缓冲区的数据写到磁盘前，会对他进行一个二次排序，首先根据数据所属的 partition 排序，然后每个 partition 中再按 Key 排序。输出包括一个索引文件和数据文件，如果设定了Combiner，将在排序输出的基础上进行。Combiner 就是一个 Mini Reducer，它在执行 Map 任务的节点本身运行，先对 Map 的输出作一次简单的 Reduce，使得 Map 的输出更紧凑，更少的数据会被写入磁盘和传送到 Reducer。Spill 文件保存在由 <code>mapred.local.dir</code> 指定的目录中，Map 任务结束后删除。</p>

<p>每当内存中的数据达到 spill 阀值的时候，都会产生一个新的 spill 文件，所以在 Map 任务写完他的最后一个输出记录的时候，可能会有多个 spill 文件，在 Map 任务完成前，所有的 spill 文件将会被归并排序为一个索引文件和数据文件。如下图所示。这是一个多路归并过程，最大归并路数由 <code>io.sort.factor</code> 控制(默认是10)。如果设定了 Combiner，并且 spill 文件的数量至少是 3（由<code>min.num.spills.for.combine</code> 属性控制），那么 Combiner 将在输出文件被写入磁盘前运行以压缩数据。</p>

<p><img src="media/14521186972719/hadoop4.jpg" alt="hadoop4"/></p>

<p>对写入到磁盘的数据进行压缩（这种压缩同 Combiner 的压缩不一样）通常是一个很好的方法，因为这样做使得数据写入磁盘的速度更快，节省磁盘空间，并减少需要传送到 Reducer 的数据量。默认输出是不被压缩的，但可以很简单的设置 <code>mapred.compress.map.output</code> 为 <code>true</code> 启用该功能。压缩所使用的库由 <code>mapred.map.output.compression.codec</code> 来设定</p>

<p>当 spill 文件归并完毕后，Map 将删除所有的临时 spill 文件，并告知 TaskTracker 任务已完成。Reducers 通过 HTTP 来获取对应的数据。用来传输 partitions 数据的工作线程个数由 <code>tasktracker.http.threads</code> 控制，这个设定是针对每一个 TaskTracker 的，并不是单个 Map，默认值为 40，在运行大作业的大集群上可以增大以提升数据传输速率。</p>

<p>现在让我们转到 Shuffle 的 Reduce 部分。Map 的输出文件放置在运行 Map 任务的 TaskTracker 的本地磁盘上（注意：Map 输出总是写到本地磁盘，但是 Reduce 输出不是，一般是写到 HDFS），它是运行 Reduce 任务的 TaskTracker 所需要的输入数据。Reduce 任务的输入数据分布在集群内的多个 Map 任务的输出中，Map 任务可能会在不同的时间内完成，只要有其中一个 Map 任务完成，Reduce 任务就开始拷贝他的输出。这个阶段称为拷贝阶段，Reduce 任务拥有多个拷贝线程，可以并行的获取 Map 输出。可以通过设定 <code>mapred.reduce.parallel.copies</code> 来改变线程数。</p>

<p>Reduce 是怎么知道从哪些 TaskTrackers 中获取 Map 的输出呢？当 Map 任务完成之后，会通知他们的父 TaskTracker，告知状态更新，然后 TaskTracker 再转告 JobTracker，这些通知信息是通过心跳通信机制传输的，因此针对以一个特定的作业，jobtracker 知道 Map 输出与 tasktrackers 的映射关系。Reducer 中有一个线程会间歇的向 JobTracker 询问 Map 输出的地址，直到把所有的数据都取到。在 Reducer 取走了 Map 输出之后，TaskTracker 不会立即删除这些数据，因为 Reducer 可能会失败，他们会在整个作业完成之后，JobTracker 告知他们要删除的时候才去删除。</p>

<p>如果 Map 输出足够小，他们会被拷贝到 Reduce TaskTracker 的内存中（缓冲区的大小由 <code>mapred.job.shuffle.input.buffer.percnet</code> 控制），或者达到了 Map 输出的阀值的大小(由 <code>mapred.inmem.merge.threshold</code> 控制)，缓冲区中的数据将会被归并然后 spill 到磁盘。</p>

<p>拷贝来的数据叠加在磁盘上，有一个后台线程会将它们归并为更大的排序文件，这样做节省了后期归并的时间。对于经过压缩的 Map 输出，系统会自动把它们解压到内存方便对其执行归并。</p>

<p>当所有的 Map 输出都被拷贝后，Reduce 任务进入排序阶段（更恰当的说应该是归并阶段，因为排序在 Map 端就已经完成），这个阶段会对所有的 Map 输出进行归并排序，这个工作会重复多次才能完成。</p>

<p>假设这里有 50 个 Map 输出（可能有保存在内存中的），并且归并因子是 10（由 <code>io.sort.factor</code> 控制，就像 Map 端的 merge 一样），那最终需要 5 次归并。每次归并会把 10 个文件归并为一个，最终生成 5 个中间文件。在这一步之后，系统不再把 5 个中间文件归并成一个，而是排序后直接“喂”给 Reduce 函数，省去向磁盘写数据这一步。最终归并的数据可以是混合数据，既有内存上的也有磁盘上的。由于归并的目的是归并最少的文件数目，使得在最后一次归并时总文件个数达到归并因子的数目，所以每次操作所涉及的文件个数在实际中会更微妙些。譬如，如果有 40 个文件，并不是每次都归并 10 个最终得到 4 个文件，相反第一次只归并 4 个文件，然后再实现三次归并，每次 10 个，最终得到 4 个归并好的文件和 6 个未归并的文件。要注意，这种做法并没有改变归并的次数，只是最小化写入磁盘的数据优化措施，因为最后一次归并的数据总是直接送到 Reduce 函数那里。在Reduce 阶段，Reduce 函数会作用在排序输出的每一个 key 上。这个阶段的输出被直接写到输出文件系统，一般是 HDFS。在 HDFS 中，因为 TaskTracker 节点也运行着一个 DataNode 进程，所以第一个块备份会直接写到本地磁盘。到此，MapReduce 的 Shuffle 和 Sort 分析完毕。</p>

<h2 id="toc_19">工作原理</h2>

<ul>
<li>Client: 作业提交发起者。</li>
<li>JobTracker: 初始化作业，分配作业，与 TaskTracker 通信，协调整个作业。</li>
<li>TaskTracker: 保持 JobTracker 通信，在分配的数据片段上执行 MapReduce 任务。</li>
</ul>

<p>提交作业</p>

<ul>
<li>在作业提交之前，需要对作业进行配置</li>
<li>程序代码，主要是自己书写的 MapReduce 程序。</li>
<li>输入输出路径</li>
<li>其他配置，如输出压缩等。</li>
<li>配置完成后，通过 JobClinet 来提交</li>
</ul>

<p>作业的初始化</p>

<ul>
<li>客户端提交完成后，JobTracker 会将作业加入队列，然后进行调度，默认的调度方法是 FIFO 调试方式。</li>
</ul>

<p>任务的分配</p>

<ul>
<li>TaskTracker 和 JobTracker 之间的通信与任务的分配是通过心跳机制完成的。</li>
<li>TaskTracker 会主动向 JobTracker 询问是否有作业要做，如果自己可以做，那么就会申请到作业任务，这个任务可以是 Map 也可能是 Reduce 任务。</li>
</ul>

<p>任务的执行</p>

<ul>
<li>申请到任务后，TaskTracker 会做如下事情：

<ul>
<li>拷贝代码到本地</li>
<li>拷贝任务的信息到本地</li>
<li>启动JVM运行任务</li>
</ul></li>
</ul>

<p>状态与任务的更新</p>

<ul>
<li>任务在运行过程中，首先会将自己的状态汇报给 TaskTracker，然后由 TaskTracker 汇总告知 JobTracker。</li>
<li>任务进度是通过计数器来实现的。</li>
</ul>

<p>作业的完成</p>

<ul>
<li>JobTracker 是在接受到最后一个任务运行完成后，才会将任务标志为成功。</li>
<li>此时会做删除中间结果等善后处理工作。</li>
</ul>

<h2 id="toc_20">错误处理</h2>

<p>任务失败</p>

<ul>
<li>MapReduce 在设计之出，就假象任务会失败，所以做了很多工作，来保证容错。</li>
<li>一种情况: 子任务失败</li>
<li>另一种情况: 子任务的 JVM 突然退出</li>
<li>任务的挂起</li>
</ul>

<p>TaskTracker 失败</p>

<ul>
<li>TaskTracker 崩溃后会停止向 Jobtracker 发送心跳信息。</li>
<li>Jobtracker 会将该 TaskTracker 从等待的任务池中移除。并将该 TaskTracker 上的任务，移动到其他地方去重新运行。</li>
<li>TaskTracker 可以被 JobTracker 放入到黑名单，即使它没有失败。</li>
</ul>

<p>JobTracker失败</p>

<ul>
<li>单点故障，Hadoop 新的0.23版本解决了这个问题。</li>
</ul>

<h2 id="toc_21">作业调度</h2>

<ul>
<li>FIFO: Hadoop 中默认的调度器，它先按照作业的优先级高低，再按照到达时间的先后选 择被执行的作业</li>
<li>公平调度器: 为任务分配资源的方法，其目的是随着时间的推移，让提交的作业获取等量的集 群共享资源，让用户公平地共享集群。具体做法是：当集群上只有一个任务在运行 时，它将使用整个集群，当有其他作业提交时，系统会将 TaskTracker 节点空间的时 间片分配给这些新的作业，并保证每个任务都得到大概等量的CPU时间。</li>
<li>容量调度器: 支持多个队列，每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略，为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。调度时，首先按以下策略选择一个合适队列：计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列；然后按以下策略选择该队列中一个作业：按照作业优先级和提交时间顺序选择 ，同时考虑用户资源量限制和内存限制。但是不可剥夺式</li>
</ul>

<h2 id="toc_22">Shuffle &amp; Sort</h2>

<p>Mapreduce 的 map 结束后，把数据重新组织，作为 reduce 阶段的输入，该过程称之为 shuffle -- 洗牌。而数据在 Map 与 Reduce 端都会做排序。</p>

<p>Map</p>

<ul>
<li>Map 的输出是由 collector 控制的</li>
<li>我们从 collect 函数入手</li>
</ul>

<p>Reduce</p>

<ul>
<li>reduce 的 Shuffle 过程，分成三个阶段：复制 Map 输出、排序合并、reduce 处理。</li>
<li>主要代码在 reduce的 run 函数</li>
</ul>

<p>Shuffle优化</p>

<ul>
<li>首先 Hadoop 的 Shuffle 在某些情况并不是最优的，例如，如果需要对2集合合并，那么其实排序操作时不需要的。</li>
<li>我们可以通过调整参数来优化 Shuffle</li>
<li>Map端: io.sort.mb</li>
<li>Reduce端: mapred.job.reduce.input.buffer.percent</li>
</ul>

<h2 id="toc_23">任务的执行时的一些特有的概念</h2>

<p>推测式执行</p>

<ul>
<li>每一道作业的任务都有运行时间，而由于机器的异构性，可能会会造成某些任务会比所有任务的平均运行时间要慢很多。</li>
<li>这时 MapReduce 会尝试在其他机器上重启慢的任务。为了是任务快速运行完成。</li>
<li>该属性默认是启用的。</li>
</ul>

<p>JVM重用</p>

<ul>
<li>启动 JVM 是一个比较耗时的工作，所以在 MapReduce 中有 JVM 重用的机制。</li>
<li>条件是统一个作业的任务。</li>
<li>可以通过 <code>mapred.job.reuse.jvm.num.tasks</code>定义重用次数，如果属性是 -1 那么为无限制。</li>
</ul>

<p>跳过坏记录</p>

<ul>
<li>数据的一些记录不符合规范，处理时抛出异常，MapReduce 可以讲次记录标为坏记录。重启任务时会跳过该记录。</li>
<li>默认情况下该属性是关闭的。</li>
</ul>

<p>任务执行环境</p>

<ul>
<li>Hadoop 为 Map 与 Reduce 任务提供运行环境。</li>
<li>如：Map 可以知道自己的处理的文件</li>
<li>问题：多个任务可能会同时写一个文件</li>
<li>解决办法：将输出写到任务的临时文件夹。目录为：{mapred.out. put.dir}/temp/${mapred.task.id}</li>
</ul>

<h2 id="toc_24">流程分析</h2>

<p>Map端：</p>

<ol>
<li>每个输入分片会让一个 map 任务来处理，默认情况下，以 HDFS 的一个块的大小（默认为 64M）为一个分片，当然我们也可以设置块的大小。map 输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为 100M，由 <code>io.sort.mb</code> 属性控制），当该缓冲区快要溢出时（默认为缓冲区大小的80%，由 <code>io.sort.spill.percent</code> 属性控制），会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。</li>
<li>在写入磁盘之前，线程首先根据 reduce 任务的数目将数据划分为相同数目的分区，也就是一个 reduce 任务对应一个分区的数据。这样做是为了避免有些 reduce 任务分配到大量数据，而有些 reduce 任务却分到很少数据，甚至没有分到数据的尴尬局面。其实分区就是对数据进行 hash 的过程。然后对每个分区中的数据进行排序，如果此时设置了 Combiner，将排序后的结果进行 Combiner 操作，这样做的目的是让尽可能少的数据写入到磁盘。</li>
<li>当 map 任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和 combiner 操作，目的有两个：1.尽量减少每次写入磁盘的数据量；2.尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。为了减少网络传输的数据量，这里可以将数据压缩，只要将 <code>mapred.compress.map.out</code> 设置为 true 就可以了。</li>
<li>将分区中的数据拷贝给相对应的 reduce 任务。有人可能会问：分区中的数据怎么知道它对应的 reduce 是哪个呢？其实 map 任务一直和其父 TaskTracker 保持联系，而 TaskTracker 又一直和 JobTracker 保持心跳。所以 JobTracker 中保存了整个集群中的宏观信息。只要 reduce 任务向 JobTracker 获取对应的 map 输出位置就ok了哦。</li>
</ol>

<p>到这里，map 端就分析完了。那到底什么是 Shuffle 呢？Shuffle 的中文意思是“洗牌”，如果我们这样看：一个 map 产生的数据，结果通过 hash 过程分区却分配给了不同的 reduce 任务，是不是一个对数据洗牌的过程呢？呵呵。</p>

<p>Reduce端：</p>

<ol>
<li>Reduce 会接收到不同 map 任务传来的数据，并且每个 map 传来的数据都是有序的。如果 reduce 端接受的数据量相当小，则直接存储在内存中（缓冲区大小由 <code>mapred.job.shuffle.input.buffer.percent</code> 属性控制，表示用作此用途的堆空间的百分比），如果数据量超过了该缓冲区大小的一定比例（由 <code>mapred.job.shuffle.merge.percent</code> 决定），则对数据合并后溢写到磁盘中。</li>
<li>随着溢写文件的增多，后台线程会将它们合并成一个更大的有序的文件，这样做是为了给后面的合并节省时间。其实不管在 map 端还是 reduce 端，MapReduce 都是反复地执行排序，合并操作，现在终于明白了有些人为什么会说：排序是 hadoop 的灵魂。</li>
<li>合并的过程中会产生许多的中间文件（写入磁盘了），但 MapReduce 会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到 reduce 函数。</li>
</ol>

<h1 id="toc_25">Hadoop Tutorial</h1>

<!-- MarkdownTOC -->

<ul>
<li>The Hadoop Approach

<ul>
<li>Data Distribution</li>
<li>MapReduce: Isolated Processes</li>
</ul></li>
<li>The Hadoop Distributed File System

<ul>
<li>Architecture</li>
<li>Namenode</li>
<li>Datanode</li>
<li>Block</li>
<li>Detail</li>
</ul></li>
<li>MapReduce

<ul>
<li>MapReduce Basics

<ul>
<li>Functional Programming Concepts</li>
<li>List Processing</li>
<li>Mapping Lists</li>
<li>Reducing Lists</li>
<li>Putting Them Together in MapReduce</li>
</ul></li>
<li>An Example Application: Word Count</li>
<li>The Driver Method</li>
</ul></li>
<li>MapReduce Data Flow

<ul>
<li>A Closer Look</li>
<li>Additional MapReduce Functionality</li>
<li>Fault Tolerance</li>
</ul></li>
</ul>

<!-- /MarkdownTOC -->

<h2 id="toc_26">The Hadoop Approach</h2>

<p>Hadoop is designed to efficiently process large volumes of information by connecting many commodity computers together to work in parallel.</p>

<h3 id="toc_27">Data Distribution</h3>

<p>In a Hadoop cluster, data is distributed to all the nodes of the cluster as it is being loaded in. The Hadoop Distributed File System (HDFS) will split large data files into chunks which are managed by different nodes in the cluster. In addition to this each chunk is replicated across several machines, so that a single machine failure does not result in any data being unavailable. An active monitoring system then re-replicates the data in response to system failures which can result in partial storage. Even though the file chunks are replicated and distributed across several machines, they form a single namespace, so their contents are universally accessible.</p>

<h3 id="toc_28">MapReduce: Isolated Processes</h3>

<p>Hadoop limits the amount of communication which can be performed by the processes, as each individual record is processed by a task in isolation from one another. Programs must be written to conform to a particular programming model, named <q>MapReduce.</q></p>

<p>In MapReduce, records are processed in isolation by tasks called Mappers. The output from the Mappers is then brought together into a second set of tasks called Reducers, where results from different mappers can be merged together.</p>

<p><img src="media/14521186972719/ht1.png" alt="ht1"/></p>

<p>Separate nodes in a Hadoop cluster still communicate with one another. However, communication in Hadoop is performed implicitly. Pieces of data can be tagged with key names which inform Hadoop how to send related bits of information to a common destination node. Hadoop internally manages all of the data transfer and cluster topology issues.</p>

<p>By restricting the communication between nodes, Hadoop makes the distributed system much more reliable. Individual node failures can be worked around by restarting tasks on other machines. Since user-level tasks do not communicate explicitly with one another, no messages need to be exchanged by user programs, nor do nodes need to roll back to pre-arranged checkpoints to partially restart the computation. The other workers continue to operate as though nothing went wrong, leaving the challenging aspects of partially restarting the program to the underlying Hadoop layer.</p>

<h2 id="toc_29">The Hadoop Distributed File System</h2>

<h3 id="toc_30">Architecture</h3>

<p>Given below is the architecture of a Hadoop File System.</p>

<p><img src="media/14521186972719/ht8.jpg" alt="ht8"/></p>

<p>HDFS follows the master-slave architecture and it has the following elements.</p>

<h3 id="toc_31">Namenode</h3>

<p>The system having the namenode acts as the master server and it does the following tasks:</p>

<ul>
<li>Manages the file system namespace.</li>
<li>Regulates client’s access to files.</li>
<li>It also executes file system operations such as renaming, closing, and opening files and directories.</li>
</ul>

<h3 id="toc_32">Datanode</h3>

<p>For every node (Commodity hardware/System) in a cluster, there will be a datanode. These nodes manage the data storage of their system.</p>

<ul>
<li>Datanodes perform read-write operations on the file systems, as per client request.</li>
<li>They also perform operations such as block creation, deletion, and replication according to the instructions of the namenode.</li>
</ul>

<h3 id="toc_33">Block</h3>

<p>Generally the user data is stored in the files of HDFS. The file in a file system will be divided into one or more segments and/or stored in individual data nodes. These file segments are called as blocks. In other words, the minimum amount of data that HDFS can read or write is called a Block. The default block size is 64MB, but it can be increased as per the need to change in HDFS configuration.</p>

<h3 id="toc_34">Detail</h3>

<p>HDFS, the Hadoop Distributed File System, is a distributed file system designed to hold very large amounts of data (terabytes or even petabytes), and provide high-throughput access to this information. Files are stored in a redundant fashion across multiple machines to ensure their durability to failure and high availability to very parallel applications.</p>

<p>But while HDFS is very scalable, its high performance design also restricts it to a particular class of applications. There are a large number of additional decisions and trade-offs that were made with HDFS. In particular:</p>

<ul>
<li>Applications that use HDFS are assumed to perform long sequential streaming reads from files. HDFS is optimized to provide streaming read performance; this comes at the expense of random seek times to arbitrary positions in files.</li>
<li>Data will be written to the HDFS once and then read several times; updates to files after they have already been closed are not supported.</li>
<li>Due to the large size of files, and the sequential nature of reads, the system does not provide a mechanism for local caching of data. The overhead of caching is great enough that data should simply be re-read from HDFS source.</li>
<li>Individual machines are assumed to fail on a frequent basis, both permanently and intermittently. While performance may degrade proportional to the number of machines lost, the system as a whole should not become overly slow, nor should information be lost. Data replication strategies combat this problem.</li>
</ul>

<p>HDFS is a block-structured file system: individual files are broken into blocks of a fixed size. These blocks are stored across a cluster of one or more machines with data storage capacity. Individual machines in the cluster are referred to as <strong>DataNodes</strong>. A file can be made of several blocks, and they are not necessarily stored on the same machine; the target machines which hold each block are chosen randomly on a block-by-block basis.</p>

<p>If several machines must be involved in the serving of a file, then a file could be rendered unavailable by the loss of any one of those machines. HDFS combats this problem by replicating each block across a number of machines (3, by default).</p>

<p>The default block size in HDFS is 64MB. This allows HDFS to decrease the amount of metadata storage required per file (the list of blocks per file will be smaller as the size of individual blocks increases). Furthermore, it allows for fast streaming reads of data, by keeping large amounts of data sequentially laid out on the disk. The consequence of this decision is that HDFS expects to have very large files, and expects them to be read sequentially. HDFS expects to store a modest number of very large files: hundreds of megabytes, or gigabytes each. After all, a 100 MB file is not even two full blocks. HDFS expects to read a block start-to-finish for a program.</p>

<p>It is important for this file system to store its metadata reliably. Furthermore, while the file data is accessed in a <strong>write once and read many</strong> model, the metadata structures (e.g., the names of files and directories) can be modified by a large number of clients concurrently. It is important that this information is never desynchronized. Therefore, it is all handled by a single machine, called the NameNode. The NameNode stores all the metadata for the file system. Because of the relatively low amount of metadata per file (it only tracks file names, permissions, and the locations of each block of each file), all of this information can be stored in the main memory of the NameNode machine, allowing fast access to the metadata.</p>

<p>To open a file, a client contacts the NameNode and retrieves a list of locations for the blocks that comprise the file. These locations identify the DataNodes which hold each block. Clients then read file data directly from the DataNode servers, possibly in parallel. The NameNode is not directly involved in this bulk data transfer, keeping its overhead to a minimum.</p>

<p>NameNode failure is more severe for the cluster than DataNode failure. While individual DataNodes may crash and the entire cluster will continue to operate, the loss of the NameNode will render the cluster inaccessible until it is manually restored.</p>

<h2 id="toc_35">MapReduce</h2>

<p>MapReduce is a programming model designed for processing large volumes of data in parallel by dividing the work into a set of independent tasks. MapReduce programs are written in a particular style influenced by functional programming constructs, specifically idioms for processing lists of data.</p>

<h3 id="toc_36">MapReduce Basics</h3>

<h4 id="toc_37">Functional Programming Concepts</h4>

<p>MapReduce programs are designed to compute large volumes of data in a parallel fashion. This requires dividing the workload across a large number of machines. This model would not scale to large clusters (hundreds or thousands of nodes) if the components were allowed to share data arbitrarily. The communication overhead required to keep the data on the nodes synchronized at all times would prevent the system from performing reliably or efficiently at large scale.</p>

<p>Instead, all data elements in MapReduce are immutable, meaning that they cannot be updated. If in a mapping task you change an input (key, value) pair, it does not get reflected back in the input files; communication occurs only by generating new output (key, value) pairs which are then forwarded by the Hadoop system into the next phase of execution.</p>

<h4 id="toc_38">List Processing</h4>

<p>Conceptually, MapReduce programs transform lists of input data elements into lists of output data elements. A MapReduce program will do this twice, using two different list processing idioms: map, and reduce.</p>

<h4 id="toc_39">Mapping Lists</h4>

<p>The first phase of a MapReduce program is called mapping. A list of data elements are provided, one at a time, to a function called the Mapper, which transforms each element individually to an output data element.</p>

<p><img src="media/14521186972719/ht2.png" alt="ht2"/></p>

<p>As an example of the utility of map: Suppose you had a function toUpper(str) which returns an uppercase version of its input string. You could use this function with map to turn a list of strings into a list of uppercase strings. Note that we are not modifying the input string here: we are returning a new string that will form part of a new output list.</p>

<h4 id="toc_40">Reducing Lists</h4>

<p>Reducing lets you aggregate values together. A reducer function receives an iterator of input values from an input list. It then combines these values together, returning a single output value.</p>

<p><img src="media/14521186972719/ht3.png" alt="ht3"/></p>

<p>Reducing is often used to produce <q>summary</q> data, turning a large volume of data into a smaller summary of itself. For example, <q>+</q> can be used as a reducing function, to return the sum of a list of input values.</p>

<h4 id="toc_41">Putting Them Together in MapReduce</h4>

<p>A MapReduce program has two components: one that implements the mapper, and another that implements the reducer. The Mapper and Reducer idioms described above are extended slightly to work in this environment, but the basic principles are the same.</p>

<p><strong>Keys and values</strong>: In MapReduce, no value stands on its own. Every value has a key associated with it. Keys identify related values. For example, a log of time-coded speedometer readings from multiple cars could be keyed by license-plate number; it would look like:</p>

<pre><code>AAA-123   65mph, 12:00pm
ZZZ-789   50mph, 12:02pm
AAA-123   40mph, 12:05pm
CCC-456   25mph, 12:15pm
</code></pre>

<p>The mapping and reducing functions receive not just values, but (key, value) pairs. The output of each of these functions is the same: both a key and a value must be emitted to the next list in the data flow.</p>

<p>MapReduce is also less strict than other languages about how the Mapper and Reducer work. In MapReduce, an arbitrary number of values can be output from each phase; a mapper may map one input into zero, one, or one hundred outputs. A reducer may compute over an input list and emit one or a dozen different outputs.</p>

<p><strong>Keys divide the reduce space</strong>: A reducing function turns a large list of values into one (or a few) output values. All of the values with the same key are presented to a single reducer together. This is performed independently of any reduce operations occurring on other lists of values, with different keys attached.</p>

<p><img src="media/14521186972719/ht4.png" alt="ht4"/></p>

<h3 id="toc_42">An Example Application: Word Count</h3>

<p>A simple MapReduce program can be written to determine how many times different words appear in a set of files. For example, if we had the files:</p>

<p><strong>foo.txt</strong>: Sweet, this is the foo file</p>

<p><strong>bar.txt</strong>: This is the bar file</p>

<p>We would expect the output to be:</p>

<pre><code>sweet 1
this  2
is    2
the   2
foo   1
bar   1
file  2
</code></pre>

<p>Naturally, we can write a program in MapReduce to compute this output. The high-level structure would look like this:</p>

<pre><code>mapper (filename, file-contents):
  for each word in file-contents:
    emit (word, 1)

reducer (word, values):
  sum = 0
  for each value in values:
    sum = sum + value
  emit (word, sum)
// Listing 4.1: High-Level MapReduce Word Count
</code></pre>

<p>Several instances of the mapper function are created on the different machines in our cluster. Each instance receives a different input file (it is assumed that we have many such files). The mappers output (word, 1) pairs which are then forwarded to the reducers. Several instances of the reducer method are also instantiated on the different machines. Each reducer is responsible for processing the list of values associated with a different word. The list of values will be a list of 1&#39;s; the reducer sums up those ones into a final count associated with a single word. The reducer then emits the final (word, count) output which is written to an output file.</p>

<p>We can write a very similar program to this in Hadoop MapReduce; it is included in the Hadoop distribution in <code>src/examples/org/apache/hadoop/examples/WordCount.java</code>. It is partially reproduced below:</p>

<pre><code>public static class MapClass extends MapReduceBase
    implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(LongWritable key, Text value,
                    OutputCollector&lt;Text, IntWritable&gt; output,
                    Reporter reporter) throws IOException {
      String line = value.toString();
      StringTokenizer itr = new StringTokenizer(line);
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        output.collect(word, one);
      }
    }
}

/**
 * A reducer class that just emits the sum of the input values.
 */
public static class Reduce extends MapReduceBase
    implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {

    public void reduce(Text key, Iterator&lt;IntWritable&gt; values,
                       OutputCollector&lt;Text, IntWritable&gt; output,
                       Reporter reporter) throws IOException {
      int sum = 0;
      while (values.hasNext()) {
        sum += values.next().get();
      }
      output.collect(key, new IntWritable(sum));
    }
}
// Listing 4.2: Hadoop MapReduce Word Count Source
</code></pre>

<p>The <code>OutputCollector</code> object you are given as an input will receive values to emit to the next stage of execution. he default input format used by Hadoop presents each line of an input file as a separate input to the mapper function, not the entire file at a time. It also uses a <code>StringTokenizer</code> object to break up the line into words. This does not perform any normalization of the input, so <q>cat</q>, <q>Cat</q> and <q>cat,</q> are all regarded as different strings. Note that the class-variable <code>word</code> is reused each time the mapper outputs another (word, 1) pairing; this saves time by not allocating a new variable for each output. The <code>output.collect()</code> method will copy the values it receives as input, so you are free to overwrite the variables you use.</p>

<p>同样 key 的 pair 会被送到一个共同的 reducer 处理</p>

<h3 id="toc_43">The Driver Method</h3>

<p>There is one final component of a Hadoop MapReduce program, called the <code>Driver</code>. The driver initializes the job and instructs the Hadoop platform to execute your code on a set of input files, and controls where the output files are placed.</p>

<pre><code>public void run(String inputPath, String outputPath) throws Exception {
    JobConf conf = new JobConf(WordCount.class);
    conf.setJobName(&quot;wordcount&quot;);

    // the keys are words (strings)
    conf.setOutputKeyClass(Text.class);
    // the values are counts (ints)
    conf.setOutputValueClass(IntWritable.class);

    conf.setMapperClass(MapClass.class);
    conf.setReducerClass(Reduce.class);

    FileInputFormat.addInputPath(conf, new Path(inputPath));
    FileOutputFormat.setOutputPath(conf, new Path(outputPath));

    JobClient.runJob(conf);
}
// Listing 4.3: Hadoop MapReduce Word Count Driver
</code></pre>

<p>This method sets up a job to execute the word count program across all the files in a given input directory (the <code>inputPath</code> argument). The output from the reducers are written into files in the directory identified by <code>outputPath</code>. The configuration information to run the job is captured in the <code>JobConf</code> object. The mapping and reducing functions are identified by the <code>setMapperClass()</code> and <code>setReducerClass()</code> methods. The data types emitted by the reducer are identified by <code>setOutputKeyClass()</code> and <code>setOutputValueClass()</code>. By default, it is assumed that these are the output types of the mapper as well. If this is not the case, the methods <code>setMapOutputKeyClass()</code> and <code>setMapOutputValueClass()</code> methods of the <code>JobConf</code> class will override these. The input types fed to the mapper are controlled by the InputFormat used. Input formats are discussed in more detail below. The default input format, <q>TextInputFormat,</q> will load data in as <code>(LongWritable, Text)</code> pairs. The long value is the byte offset of the line in the file. The Text object holds the string contents of the line of the file.</p>

<p>The call to <code>JobClient.runJob(conf)</code> will submit the job to MapReduce. This call will block until the job completes. If the job fails, it will throw an IOException. JobClient also provides a non-blocking version called <code>submitJob()</code>.</p>

<h2 id="toc_44">MapReduce Data Flow</h2>

<p>Now that we have seen the components that make up a basic MapReduce job, we can see how everything works together at a higher level:</p>

<p><img src="media/14521186972719/ht5.png" alt="ht5"/></p>

<p>MapReduce inputs typically come from input files loaded onto our processing cluster in HDFS. These files are evenly distributed across all our nodes. Running a MapReduce program involves running mapping tasks on many or all of the nodes in our cluster. Each of these mapping tasks is equivalent: no mappers have particular <q>identities</q> associated with them. Therefore, any mapper can process any input file. Each mapper loads the set of files local to that machine and processes them.</p>

<p>When the mapping phase has completed, the intermediate (key, value) pairs must be exchanged between machines to <strong>send all values with the same key to a single reducer</strong>. The reduce tasks are spread across the same nodes in the cluster as the mappers. <strong>This is the only communication step in MapReduce</strong>. Individual map tasks do not exchange information with one another, nor are they aware of one another&#39;s existence. Similarly, different reduce tasks do not communicate with one another. The user never explicitly marshals information from one machine to another; all data transfer is handled by the Hadoop MapReduce platform itself, guided implicitly by the different keys associated with values. This is a fundamental element of Hadoop MapReduce&#39;s reliability. If nodes in the cluster fail, tasks must be able to be restarted. If they have been performing side-effects, e.g., communicating with the outside world, then the shared state must be restored in a restarted task. By eliminating communication and side-effects, restarts can be handled more gracefully.</p>

<h3 id="toc_45">A Closer Look</h3>

<p>The previous figure described the high-level view of Hadoop MapReduce. From this diagram, you can see where the mapper and reducer components of the Word Count application fit in, and how it achieves its objective.</p>

<p><img src="media/14521186972719/ht6.png" alt="ht6"/></p>

<p>The above figure shows the pipeline with more of its mechanics exposed. While only two nodes are depicted, the same pipeline can be replicated across a very large number of nodes.</p>

<p><strong>Input files</strong>: This is where the data for a MapReduce task is initially stored. While this does not need to be the case, the input files typically reside in HDFS. The format of these files is arbitrary; while line-based log files can be used, we could also use a binary format, multi-line input records, or something else entirely. It is typical for these input files to be very large -- tens of gigabytes or more. 怎么 Input 基本都行</p>

<p><strong>InputFormat</strong>: How these input files are split up and read is defined by the InputFormat. An InputFormat is a class that provides the following functionality:</p>

<ul>
<li>Selects the files or other objects that should be used for input</li>
<li>Defines the InputSplits that break a file into tasks</li>
<li>Provides a factory for RecordReader objects that read the file</li>
</ul>

<p>Several InputFormats are provided with Hadoop. An abstract type is called <code>FileInputFormat</code>; all InputFormats that operate on files inherit functionality and properties from this class. When starting a Hadoop job, <code>FileInputFormat</code> is provided with a path containing files to read. The <code>FileInputFormat</code> will read all files in this directory. It then divides these files into one or more <code>InputSplits</code> each. You can choose which InputFormat to apply to your input files for a job by calling the <code>setInputFormat()</code> method of the JobConf object that defines the job. A table of standard InputFormats is given below.</p>

<table>
<thead>
<tr>
<th>InputFormat</th>
<th>Description</th>
<th>Key</th>
<th>Value</th>
</tr>
</thead>

<tbody>
<tr>
<td>TextInputFormat</td>
<td>Default format; reads lines of text files</td>
<td>The byte offset of the line</td>
<td>The line contents</td>
</tr>
<tr>
<td>KeyValueInputFormat</td>
<td>Parses lines into key, val pairs</td>
<td>Everything up to the first tab character</td>
<td>The remainder of the line</td>
</tr>
<tr>
<td>SequenceFileInputFormat</td>
<td>A Hadoop-specific high-performance binary format</td>
<td>user-defined</td>
<td>user-defined</td>
</tr>
</tbody>
</table>

<p>The default InputFormat is the <code>TextInputFormat</code>. This treats each line of each input file as a separate record, and performs no parsing. This is useful for unformatted data or line-based records like log files. A more interesting input format is the <code>KeyValueInputFormat</code>. This format also treats each line of input as a separate record. While the <code>TextInputFormat</code> treats the entire line as the value, the <code>KeyValueInputFormat</code> breaks the line itself into the key and value by searching for a <code>tab</code> character. This is particularly useful for reading the output of one MapReduce job as the input to another, as the default <code>OutputFormat</code> formats its results in this manner. Finally, the <code>SequenceFileInputFormat</code> reads special binary files that are specific to Hadoop. These files include many features designed to allow data to be rapidly read into Hadoop mappers. Sequence files are block-compressed and provide direct serialization and deserialization of several arbitrary data types (not just text). Sequence files can be generated as the output of other MapReduce tasks and are an efficient intermediate representation for data that is passing from one MapReduce job to anther. 三种不同的读入方式，其中 KeyValue 会按照 tab 来分开 key 和 value 进行读取</p>

<p><strong>InputSplits</strong>: An <code>InputSplit</code> describes a unit of work that comprises a single map task in a MapReduce program. A MapReduce program applied to a data set, collectively referred to as a Job, is made up of several (possibly several hundred) tasks. Map tasks may involve reading a whole file; they often involve reading only part of a file. By default, the FileInputFormat and its descendants break a file up into 64 MB chunks (the same size as blocks in HDFS). You can control this value by setting the <code>mapred.min.split.size</code> parameter in <code>hadoop-site.xml</code>, or by overriding the parameter in the <code>JobConf</code> object used to submit a particular MapReduce job.</p>

<p>By processing a file in chunks, we allow several map tasks to operate on a single file in parallel. If the file is very large, this can improve performance significantly through parallelism. Even more importantly, since the various blocks that make up the file may be spread across several different nodes in the cluster, it allows tasks to be scheduled on each of these different nodes; the individual blocks are thus all processed locally, instead of needing to be transferred from one node to another. Of course, while log files can be processed in this piece-wise fashion, some file formats are not amenable to chunked processing. By writing a custom InputFormat, you can control how the file is broken up (or is not broken up) into splits.</p>

<p>The InputFormat defines the list of tasks that make up the mapping phase; each task corresponds to a single input split. The tasks are then assigned to the nodes in the system based on where the input file chunks are physically resident. An individual node may have several dozen tasks assigned to it. The node will begin working on the tasks, attempting to perform as many in parallel as it can. The on-node parallelism is controlled by the <code>mapred.tasktracker.map.tasks.maximum parameter</code>.</p>

<p><strong>RecordReader</strong>: The <code>InputSplit</code> has defined a slice of work, but does not describe how to access it. The <code>RecordReader</code> class actually <strong>loads the data from its source and converts it into (key, value) pairs</strong> suitable for reading by the Mapper. The <code>RecordReader</code> instance is defined by the InputFormat. The default <code>InputFormat</code>, <code>TextInputFormat</code>, provides a <code>LineRecordReader</code>, which treats each line of the input file as a new value. The key associated with each line is its byte offset in the file. The <code>RecordReader</code> is invoke repeatedly on the input until the entire InputSplit has been consumed. Each invocation of the <code>RecordReader</code> leads to another call to the map() method of the Mapper.</p>

<p><strong>Mapper</strong>: The <code>Mapper</code> performs the interesting user-defined work of the first phase of the MapReduce program. Given a key and a value, the <code>map()</code> method emits <code>(key, value)</code> pair(s) which are forwarded to the Reducers. A new instance of <code>Mapper</code> is instantiated in a separate Java process for <strong>each map task</strong> (InputSplit) that makes up part of the total job input. The individual mappers are intentionally not provided with a mechanism to communicate with one another in any way. This allows the reliability of each map task to be governed solely by the reliability of the local machine. The <code>map()</code> method receives two parameters in addition to the key and the value:</p>

<ul>
<li>The <code>OutputCollector</code> object has a method named <code>collect()</code> which will forward a <code>(key, value)</code> pair to the reduce phase of the job.</li>
<li>The <code>Reporter</code> object provides information about the current task; its <code>getInputSplit()</code> method will return an object describing the current InputSplit. It also allows the map task to provide additional information about its progress to the rest of the system. The <code>setStatus()</code> method allows you to emit a status message back to the user. The <code>incrCounter()</code> method allows you to increment shared performance counters. You may define as many arbitrary counters as you wish. Each mapper can increment the counters, and the JobTracker will collect the increments made by the different processes and aggregate them for later retrieval when the job ends.</li>
</ul>

<p><strong>Partition &amp; Shuffle</strong>: After the first map tasks have completed, the nodes may still be performing several more map tasks each. But they also begin exchanging the intermediate outputs from the map tasks to where they are required by the reducers. <strong>This process of moving map outputs to the reducers is known as shuffling</strong>. A different subset of the intermediate key space is assigned to each reduce node; these subsets (known as <q>partitions</q>) are the inputs to the reduce tasks. Each map task may emit (key, value) pairs to any partition; <strong>all values for the same key are always reduced together regardless of which mapper is its origin</strong>. Therefore, the map nodes must all agree on where to send the different pieces of the intermediate data. The Partitioner class determines which partition a given (key, value) pair will go to. The default partitioner computes a hash value for the key and assigns the partition based on this result.</p>

<p><strong>Sort</strong>: Each reduce task is responsible for reducing the values associated with several intermediate keys. The set of intermediate keys on a single node is <strong>automatically sorted</strong> by Hadoop before they are presented to the Reducer.</p>

<p><strong>Reduce</strong>: A Reducer instance is created for each reduce task. This is an instance of user-provided code that performs the second important phase of job-specific work. For each key in the partition assigned to a Reducer, the Reducer&#39;s <code>reduce()</code> method is called once. This receives a key as well as an iterator over all the values associated with the key. The values associated with a key are returned by the iterator in an undefined order. The Reducer also receives as parameters <code>OutputCollector</code> and <code>Reporter</code> objects; they are used in the same manner as in the <code>map()</code> method. 对于每个 partition，也就是里面放着同样 key 的一个 set，执行一次 <code>reduce()</code>。</p>

<p><strong>OutputFormat</strong>: The (key, value) pairs provided to this <code>OutputCollector</code> are then written to output files. The way they are written is governed by the <code>OutputFormat</code>. The <code>OutputFormat</code> functions much like the <code>InputFormat</code> class described earlier. The instances of <code>OutputFormat</code> provided by Hadoop write to files on the local disk or in HDFS; they all inherit from a common <code>FileOutputFormat</code>. <strong>Each Reducer writes a separate file in a common output directory</strong>. These files will typically be named <code>part-nnnnn</code>, where nnnnn is the partition id associated with the reduce task. The output directory is set by the <code>FileOutputFormat.setOutputPath()</code> method. You can control which particular OutputFormat is used by calling the <code>setOutputFormat()</code> method of the <code>JobConf</code> object that defines your MapReduce job. A table of provided OutputFormats is given below.</p>

<table>
<thead>
<tr>
<th>OutputFormat</th>
<th>Description</th>
</tr>
</thead>

<tbody>
<tr>
<td>TextOutputFormat</td>
<td>Default; writes lines in <q>key \t value</q> form</td>
</tr>
<tr>
<td>SequenceFileOutputFormat</td>
<td>Writes binary files suitable for reading into subsequent MapReduce jobs</td>
</tr>
<tr>
<td>NullOutputFormat</td>
<td>Disregards its inputs</td>
</tr>
</tbody>
</table>

<p>Hadoop provides some OutputFormat instances to write to files. The basic (default) instance is <code>TextOutputFormat</code>, which writes <code>(key, value)</code> pairs on individual lines of a text file. This can be easily re-read by a later MapReduce task using the <code>KeyValueInputFormat</code> class, and is also human-readable. A better intermediate format for use between MapReduce jobs is the <code>SequenceFileOutputFormat</code> which rapidly serializes arbitrary data types to the file; the corresponding SequenceFileInputFormat will deserialize the file into the same types and presents the data to the next Mapper in the same manner as it was emitted by the previous Reducer. The <code>NullOutputFormat</code> generates no output files and disregards any (key, value) pairs passed to it by the <code>OutputCollector</code>. This is useful if you are explicitly writing your own output files in the reduce() method, and do not want additional empty output files generated by the Hadoop framework.</p>

<p><strong>RecordWriter</strong>: Much like how the InputFormat actually reads individual records through the RecordReader implementation, the OutputFormat class is a factory for RecordWriter objects; these are used to write the individual records to the files as directed by the OutputFormat.</p>

<p>The <strong>output files</strong> written by the Reducers are then left in HDFS for your use, either by another MapReduce job, a separate program, for for human inspection.</p>

<h3 id="toc_46">Additional MapReduce Functionality</h3>

<p><img src="media/14521186972719/ht7.png" alt="ht7"/></p>

<p><strong>Combiner</strong>: The pipeline showed earlier omits a processing step which can be used for optimizing bandwidth usage by your MapReduce job. Called the <code>Combiner</code>, this pass runs after the Mapper and before the Reducer. Usage of the Combiner is optional. If this pass is suitable for your job, instances of the <code>Combiner</code> class are run on every node that has run map tasks. The Combiner will receive as input all data emitted by the Mapper instances on a given node. The output from the Combiner is then sent to the Reducers, instead of the output from the Mappers. The Combiner is <strong>a <q>mini-reduce</q> process which operates only on data generated by one machine</strong>.</p>

<p>Word count is a prime example for where a Combiner is useful. The Word Count program in listings 1--3 emits a (word, 1) pair for every instance of every word it sees. So if the same document contains the word <q>cat</q> 3 times, the pair (<q>cat</q>, 1) is emitted three times; all of these are then sent to the Reducer. By using a Combiner, these can be condensed into a single (<q>cat</q>, 3) pair to be sent to the Reducer. Now each node only sends a single value to the reducer for each word -- drastically <strong>reducing the total bandwidth required for the shuffle process</strong>, and speeding up the job. The best part of all is that we do not need to write any additional code to take advantage of this! If a reduce function is both commutative and associative, then it can be used as a Combiner as well. You can enable combining in the word count program by adding the following line to the driver:</p>

<pre><code>conf.setCombinerClass(Reduce.class);
</code></pre>

<p>The Combiner should be an instance of the Reducer interface. If your Reducer itself cannot be used directly as a Combiner because of commutativity or associativity, you might still be able to write a third class to use as a Combiner for your job.</p>

<h3 id="toc_47">Fault Tolerance</h3>

<p>One of the primary reasons to use Hadoop to run your jobs is due to its <strong>high degree of fault tolerance</strong>. Even when running jobs on a large cluster where individual nodes or network components may experience high rates of failure, Hadoop can guide jobs toward a successful completion.</p>

<p>The primary way that Hadoop achieves fault tolerance is through <strong>restarting tasks</strong>. Individual task nodes (TaskTrackers) are in constant communication with the head node of the system, called the <code>JobTracker</code>. If a TaskTracker fails to communicate with the JobTracker for a period of time (by default, 1 minute), the JobTracker will assume that the TaskTracker in question has crashed. The JobTracker knows which map and reduce tasks were assigned to each TaskTracker.</p>

<p>If the job is still in the mapping phase, then other TaskTrackers will be asked to re-execute all map tasks previously run by the failed TaskTracker. If the job is in the reducing phase, then other TaskTrackers will re-execute all reduce tasks that were in progress on the failed TaskTracker.</p>

<p>Reduce tasks, once completed, have been written back to HDFS. Thus, if a TaskTracker has already completed two out of three reduce tasks assigned to it, only the third task must be executed elsewhere. Map tasks are slightly more complicated: even if a node has completed ten map tasks, the reducers may not have all copied their inputs from the output of those map tasks. If a node has crashed, then its mapper outputs are inaccessible. So any already-completed map tasks must be re-executed to make their results available to the rest of the reducing machines. All of this is handled automatically by the Hadoop platform.</p>

<p>This fault tolerance underscores the need for program execution to be side-effect free. If Mappers and Reducers had individual identities and communicated with one another or the outside world, then restarting a task would require the other nodes to communicate with the new instances of the map and reduce tasks, and the re-executed tasks would need to reestablish their intermediate state. This process is notoriously complicated and error-prone in the general case. MapReduce simplifies this problem drastically by eliminating task identities or the ability for task partitions to communicate with one another. An individual task sees only its own direct inputs and knows only its own outputs, to make this failure and restart process clean and dependable.</p>


</div>

<br /><br />
<hr />

<div class="row clearfix">
  <div class="large-6 columns">
	<div class="text-left" style="padding:15px 0px;">
		
	        <a href="14520954251052.html"  title="Previous Post: Homebrew 指南">&laquo; Homebrew 指南</a>
	    
	</div>
  </div>
  <div class="large-6 columns">
	<div class="text-right" style="padding:15px 0px;">
		
	        <a href="14521020035250.html" 
	        title="Next Post: 如何选择编程语言">如何选择编程语言 &raquo;</a>
	    
	</div>
  </div>
</div>

<div class="row">
<div style="padding:0px 0.93em;" class="share-comments">

</div>
</div>
<script type="text/javascript">
	$(function(){
		var currentURL = '14521186972719.html';
		$('#side-nav a').each(function(){
			if($(this).attr('href') == currentURL){
				$(this).parent().addClass('active');
			}
		});
	});
</script>  
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>
    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>
</html>
