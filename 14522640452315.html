<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  机器学习指南 - 小土刀的笔记
  
  </title>
 <meta name="description" content="">
 <link href="atom.xml" rel="alternate" title="小土刀的笔记" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />

    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
    <script src="asset/highlightjs/highlight.pack.js"></script>
    <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
    <script>hljs.initHighlightingOnLoad();</script>
    
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>

<div id="header">
    <h1><a href="index.html">小土刀的笔记</a></h1>
</div>

</nav>
        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; 小土刀的笔记</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
      <li><a href="index.html">Home</a></li>
      
        <li class="divider"></li>
        <li><label>清单</label></li>

          
            <li><a title="好问题收集" href="14520379720179.html">好问题收集</a></li>
          
            <li><a title="书单合集" href="14520379720136.html">书单合集</a></li>
          
            <li><a title="日志列表" href="14520379720098.html">日志列表</a></li>
          
            <li><a title="玩物列表" href="14520379720223.html">玩物列表</a></li>
          

      
        <li class="divider"></li>
        <li><label>Research</label></li>

          
            <li><a title="编程算法面试指南" href="14523493109485.html">编程算法面试指南</a></li>
          
            <li><a title="机器学习指南" href="14522640452315.html">机器学习指南</a></li>
          
            <li><a title="推荐系统沉思录" href="14520495721664.html">推荐系统沉思录</a></li>
          
            <li><a title="知识图谱沉思录" href="14520583101537.html">知识图谱沉思录</a></li>
          
            <li><a title="计算机视觉沉思录" href="14520483585854.html">计算机视觉沉思录</a></li>
          
            <li><a title="数据分析沉思录" href="14520494829635.html">数据分析沉思录</a></li>
          
            <li><a title="机器学习沉思录" href="14520493984651.html">机器学习沉思录</a></li>
          
            <li><a title="3D 打印沉思录" href="14520902603411.html">3D 打印沉思录</a></li>
          
            <li><a title="HMM 指南" href="14520495021796.html">HMM 指南</a></li>
          
            <li><a title="EM / GMM 指南" href="14520494287936.html">EM / GMM 指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>工具平台</label></li>

          
            <li><a title="Mac 指南" href="14520959875185.html">Mac 指南</a></li>
          
            <li><a title="Bash 指南" href="14520940884566.html">Bash 指南</a></li>
          
            <li><a title="Hexo 指南" href="14520953748408.html">Hexo 指南</a></li>
          
            <li><a title="VS Code 指南" href="14520964818956.html">VS Code 指南</a></li>
          
            <li><a title="Homebrew 指南" href="14520954251052.html">Homebrew 指南</a></li>
          
            <li><a title="Hadoop 指南" href="14521186972719.html">Hadoop 指南</a></li>
          
            <li><a title="Pandoc 指南" href="14520962259349.html">Pandoc 指南</a></li>
          
            <li><a title="聊聊 PPT" href="14521293989573.html">聊聊 PPT</a></li>
          
            <li><a title="Vim 指南" href="14520964365928.html">Vim 指南</a></li>
          
            <li><a title="Sublime 指南" href="14520965272426.html">Sublime 指南</a></li>
          
            <li><a title="GFW 原理指南" href="14521185605970.html">GFW 原理指南</a></li>
          
            <li><a title="Linux 的概念与体系" href="14521103801032.html">Linux 的概念与体系</a></li>
          
            <li><a title="Appstore 生存指南" href="14521185605800.html">Appstore 生存指南</a></li>
          
            <li><a title="Latex 指南" href="14520955052445.html">Latex 指南</a></li>
          
            <li><a title="Git 指南" href="14520951022304.html">Git 指南</a></li>
          
            <li><a title="SVN 指南" href="14521379903411.html">SVN 指南</a></li>
          
            <li><a title="Github 生活指南" href="14520951022388.html">Github 生活指南</a></li>
          
            <li><a title="iTerm2 指南" href="14520954757815.html">iTerm2 指南</a></li>
          
            <li><a title="Ubuntu 指南" href="14520961679182.html">Ubuntu 指南</a></li>
          
            <li><a title="谷歌搜索技巧" href="14520951022464.html">谷歌搜索技巧</a></li>
          
            <li><a title="Make 指南" href="14520961098604.html">Make 指南</a></li>
          
            <li><a title="GCC 简易指南" href="14520948089088.html">GCC 简易指南</a></li>
          
            <li><a title="Kinect 开发指南" href="14521185098750.html">Kinect 开发指南</a></li>
          
            <li><a title="fish shell 指南" href="14520943310829.html">fish shell 指南</a></li>
          
            <li><a title="Gradle 指南" href="14520951022555.html">Gradle 指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>计算机学科经典</label></li>

          
            <li><a title="大教堂与集市" href="14520876353363.html">大教堂与集市</a></li>
          
            <li><a title="深入理解计算机系统" href="14520855371865.html">深入理解计算机系统</a></li>
          
            <li><a title="Python Algorithm" href="14521091296885.html">Python Algorithm</a></li>
          
            <li><a title="程序员的思维修炼" href="14520875018555.html">程序员的思维修炼</a></li>
          
            <li><a title="程序员修炼之道" href="14520874386531.html">程序员修炼之道</a></li>
          
            <li><a title="高效程序员的45个习惯" href="14520878121031.html">高效程序员的45个习惯</a></li>
          
            <li><a title="代码大全" href="14520855371782.html">代码大全</a></li>
          
            <li><a title="编程珠玑" href="14521308341315.html">编程珠玑</a></li>
          
            <li><a title="编写可读代码的艺术" href="14520874726715.html">编写可读代码的艺术</a></li>
          
            <li><a title="移动应用UI设计模式" href="14520886280669.html">移动应用UI设计模式</a></li>
          
            <li><a title="启示录：打造用户喜爱的产品" href="14520907410466.html">启示录：打造用户喜爱的产品</a></li>
          
            <li><a title="亲爱的界面：让用户乐于使用、爱不释手" href="14520885573483.html">亲爱的界面：让用户乐于使用、爱不释手</a></li>
          
            <li><a title="简约之美：软件设计之道" href="14520879229686.html">简约之美：软件设计之道</a></li>
          
            <li><a title="简单之美：软件开发实践者的思考" href="14520878623936.html">简单之美：软件开发实践者的思考</a></li>
          
            <li><a title="构建之法：现代软件工程" href="14520880342127.html">构建之法：现代软件工程</a></li>
          
            <li><a title="软件项目成功之道" href="14520883368873.html">软件项目成功之道</a></li>
          

      
        <li class="divider"></li>
        <li><label>iOS</label></li>

          
            <li><a title="iOS 学习路径" href="14520467690026.html">iOS 学习路径</a></li>
          
            <li><a title="iOS 面试相关" href="14520467689968.html">iOS 面试相关</a></li>
          

      
        <li class="divider"></li>
        <li><label>游戏</label></li>

          
            <li><a title="深入游戏" href="14521282739436.html">深入游戏</a></li>
          
            <li><a title="游戏设计" href="14521282739338.html">游戏设计</a></li>
          
            <li><a title="英雄联盟游戏设计与运营技巧" href="14521282739242.html">英雄联盟游戏设计与运营技巧</a></li>
          
            <li><a title="游戏运营指南" href="14521282739132.html">游戏运营指南</a></li>
          
            <li><a title="游戏的架构与细节梳理" href="14521282739025.html">游戏的架构与细节梳理</a></li>
          
            <li><a title="Destiny 游戏分析" href="14521282738926.html">Destiny 游戏分析</a></li>
          
            <li><a title="辐射避难所" href="14521277966471.html">辐射避难所</a></li>
          
            <li><a title="英雄联盟攻略" href="14521196095413.html">英雄联盟攻略</a></li>
          
            <li><a title="游戏产业信息收集" href="14521195111050.html">游戏产业信息收集</a></li>
          
            <li><a title="游戏发展史" href="14521194839446.html">游戏发展史</a></li>
          
            <li><a title="辐射系列" href="14521194076173.html">辐射系列</a></li>
          

      
        <li class="divider"></li>
        <li><label>生活品质</label></li>

          
            <li><a title="成为作家" href="14521287232350.html">成为作家</a></li>
          
            <li><a title="广州美食地图" href="14521207453879.html">广州美食地图</a></li>
          
            <li><a title="聊聊钢笔" href="14521202963339.html">聊聊钢笔</a></li>
          
            <li><a title="聊聊电影" href="14521201092159.html">聊聊电影</a></li>
          

      
        <li class="divider"></li>
        <li><label>生活技巧</label></li>

          
            <li><a title="烤箱指南" href="14521207693233.html">烤箱指南</a></li>
          
            <li><a title="租房手册" href="14521200170697.html">租房手册</a></li>
          
            <li><a title="买车指南" href="14521200170757.html">买车指南</a></li>
          
            <li><a title="北美二手车购买指南" href="14521200391261.html">北美二手车购买指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>编程语言</label></li>

          
            <li><a title="如何选择编程语言" href="14521020035250.html">如何选择编程语言</a></li>
          
            <li><a title="Python 学习指南" href="14521095530236.html">Python 学习指南</a></li>
          
            <li><a title="Python 编程思想" href="14521096832549.html">Python 编程思想</a></li>
          
            <li><a title="Java 精要" href="14521089938127.html">Java 精要</a></li>
          
            <li><a title="Matlab 指南" href="14521020962952.html">Matlab 指南</a></li>
          
            <li><a title="OpenMP 入门指南" href="14521021178707.html">OpenMP 入门指南</a></li>
          
            <li><a title="Javascript 学习指南" href="14521020732534.html">Javascript 学习指南</a></li>
          
            <li><a title="C++ 学习笔记" href="14520969294593.html">C++ 学习笔记</a></li>
          
            <li><a title="SQL 入门指南" href="14521021831775.html">SQL 入门指南</a></li>
          
            <li><a title="CUDA架构" href="14521020248399.html">CUDA架构</a></li>
          

      
        <li class="divider"></li>
        <li><label>Android</label></li>

          
            <li><a title="Android Studio 指南" href="14520922857884.html">Android Studio 指南</a></li>
          
            <li><a title="Google Map V2 使用指南" href="14520474070782.html">Google Map V2 使用指南</a></li>
          
            <li><a title="ActionBar 使用指南" href="14520474070736.html">ActionBar 使用指南</a></li>
          

      
        <li class="divider"></li>
        <li><label>Web</label></li>

          
            <li><a title="网络协议指南" href="14520476779491.html">网络协议指南</a></li>
          
            <li><a title="Node.js 命令行程序实例" href="14520476779456.html">Node.js 命令行程序实例</a></li>
          
            <li><a title="Flask Study Note" href="14520476779406.html">Flask Study Note</a></li>
          

      
        <li class="divider"></li>
        <li><label>关于</label></li>

          
            <li><a title="版权声明" href="14520576386063.html">版权声明</a></li>
          

      
      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>

        <section id="main-content" role="main" class="scroll-container">

          <div class="row">
            <div class="large-3 medium-3 columns">
              <div class="hide-for-small">
                <div class="sidebar">
                <nav>
                  <ul id="side-nav" class="side-nav">

                    
                      <li class="side-title"><span>清单</span></li>
                        
                          <li><a title="好问题收集" href="14520379720179.html">好问题收集</a></li>
                        
                          <li><a title="书单合集" href="14520379720136.html">书单合集</a></li>
                        
                          <li><a title="日志列表" href="14520379720098.html">日志列表</a></li>
                        
                          <li><a title="玩物列表" href="14520379720223.html">玩物列表</a></li>
                        

                    
                      <li class="side-title"><span>Research</span></li>
                        
                          <li><a title="编程算法面试指南" href="14523493109485.html">编程算法面试指南</a></li>
                        
                          <li><a title="机器学习指南" href="14522640452315.html">机器学习指南</a></li>
                        
                          <li><a title="推荐系统沉思录" href="14520495721664.html">推荐系统沉思录</a></li>
                        
                          <li><a title="知识图谱沉思录" href="14520583101537.html">知识图谱沉思录</a></li>
                        
                          <li><a title="计算机视觉沉思录" href="14520483585854.html">计算机视觉沉思录</a></li>
                        
                          <li><a title="数据分析沉思录" href="14520494829635.html">数据分析沉思录</a></li>
                        
                          <li><a title="机器学习沉思录" href="14520493984651.html">机器学习沉思录</a></li>
                        
                          <li><a title="3D 打印沉思录" href="14520902603411.html">3D 打印沉思录</a></li>
                        
                          <li><a title="HMM 指南" href="14520495021796.html">HMM 指南</a></li>
                        
                          <li><a title="EM / GMM 指南" href="14520494287936.html">EM / GMM 指南</a></li>
                        

                    
                      <li class="side-title"><span>工具平台</span></li>
                        
                          <li><a title="Mac 指南" href="14520959875185.html">Mac 指南</a></li>
                        
                          <li><a title="Bash 指南" href="14520940884566.html">Bash 指南</a></li>
                        
                          <li><a title="Hexo 指南" href="14520953748408.html">Hexo 指南</a></li>
                        
                          <li><a title="VS Code 指南" href="14520964818956.html">VS Code 指南</a></li>
                        
                          <li><a title="Homebrew 指南" href="14520954251052.html">Homebrew 指南</a></li>
                        
                          <li><a title="Hadoop 指南" href="14521186972719.html">Hadoop 指南</a></li>
                        
                          <li><a title="Pandoc 指南" href="14520962259349.html">Pandoc 指南</a></li>
                        
                          <li><a title="聊聊 PPT" href="14521293989573.html">聊聊 PPT</a></li>
                        
                          <li><a title="Vim 指南" href="14520964365928.html">Vim 指南</a></li>
                        
                          <li><a title="Sublime 指南" href="14520965272426.html">Sublime 指南</a></li>
                        
                          <li><a title="GFW 原理指南" href="14521185605970.html">GFW 原理指南</a></li>
                        
                          <li><a title="Linux 的概念与体系" href="14521103801032.html">Linux 的概念与体系</a></li>
                        
                          <li><a title="Appstore 生存指南" href="14521185605800.html">Appstore 生存指南</a></li>
                        
                          <li><a title="Latex 指南" href="14520955052445.html">Latex 指南</a></li>
                        
                          <li><a title="Git 指南" href="14520951022304.html">Git 指南</a></li>
                        
                          <li><a title="SVN 指南" href="14521379903411.html">SVN 指南</a></li>
                        
                          <li><a title="Github 生活指南" href="14520951022388.html">Github 生活指南</a></li>
                        
                          <li><a title="iTerm2 指南" href="14520954757815.html">iTerm2 指南</a></li>
                        
                          <li><a title="Ubuntu 指南" href="14520961679182.html">Ubuntu 指南</a></li>
                        
                          <li><a title="谷歌搜索技巧" href="14520951022464.html">谷歌搜索技巧</a></li>
                        
                          <li><a title="Make 指南" href="14520961098604.html">Make 指南</a></li>
                        
                          <li><a title="GCC 简易指南" href="14520948089088.html">GCC 简易指南</a></li>
                        
                          <li><a title="Kinect 开发指南" href="14521185098750.html">Kinect 开发指南</a></li>
                        
                          <li><a title="fish shell 指南" href="14520943310829.html">fish shell 指南</a></li>
                        
                          <li><a title="Gradle 指南" href="14520951022555.html">Gradle 指南</a></li>
                        

                    
                      <li class="side-title"><span>计算机学科经典</span></li>
                        
                          <li><a title="大教堂与集市" href="14520876353363.html">大教堂与集市</a></li>
                        
                          <li><a title="深入理解计算机系统" href="14520855371865.html">深入理解计算机系统</a></li>
                        
                          <li><a title="Python Algorithm" href="14521091296885.html">Python Algorithm</a></li>
                        
                          <li><a title="程序员的思维修炼" href="14520875018555.html">程序员的思维修炼</a></li>
                        
                          <li><a title="程序员修炼之道" href="14520874386531.html">程序员修炼之道</a></li>
                        
                          <li><a title="高效程序员的45个习惯" href="14520878121031.html">高效程序员的45个习惯</a></li>
                        
                          <li><a title="代码大全" href="14520855371782.html">代码大全</a></li>
                        
                          <li><a title="编程珠玑" href="14521308341315.html">编程珠玑</a></li>
                        
                          <li><a title="编写可读代码的艺术" href="14520874726715.html">编写可读代码的艺术</a></li>
                        
                          <li><a title="移动应用UI设计模式" href="14520886280669.html">移动应用UI设计模式</a></li>
                        
                          <li><a title="启示录：打造用户喜爱的产品" href="14520907410466.html">启示录：打造用户喜爱的产品</a></li>
                        
                          <li><a title="亲爱的界面：让用户乐于使用、爱不释手" href="14520885573483.html">亲爱的界面：让用户乐于使用、爱不释手</a></li>
                        
                          <li><a title="简约之美：软件设计之道" href="14520879229686.html">简约之美：软件设计之道</a></li>
                        
                          <li><a title="简单之美：软件开发实践者的思考" href="14520878623936.html">简单之美：软件开发实践者的思考</a></li>
                        
                          <li><a title="构建之法：现代软件工程" href="14520880342127.html">构建之法：现代软件工程</a></li>
                        
                          <li><a title="软件项目成功之道" href="14520883368873.html">软件项目成功之道</a></li>
                        

                    
                      <li class="side-title"><span>iOS</span></li>
                        
                          <li><a title="iOS 学习路径" href="14520467690026.html">iOS 学习路径</a></li>
                        
                          <li><a title="iOS 面试相关" href="14520467689968.html">iOS 面试相关</a></li>
                        

                    
                      <li class="side-title"><span>游戏</span></li>
                        
                          <li><a title="深入游戏" href="14521282739436.html">深入游戏</a></li>
                        
                          <li><a title="游戏设计" href="14521282739338.html">游戏设计</a></li>
                        
                          <li><a title="英雄联盟游戏设计与运营技巧" href="14521282739242.html">英雄联盟游戏设计与运营技巧</a></li>
                        
                          <li><a title="游戏运营指南" href="14521282739132.html">游戏运营指南</a></li>
                        
                          <li><a title="游戏的架构与细节梳理" href="14521282739025.html">游戏的架构与细节梳理</a></li>
                        
                          <li><a title="Destiny 游戏分析" href="14521282738926.html">Destiny 游戏分析</a></li>
                        
                          <li><a title="辐射避难所" href="14521277966471.html">辐射避难所</a></li>
                        
                          <li><a title="英雄联盟攻略" href="14521196095413.html">英雄联盟攻略</a></li>
                        
                          <li><a title="游戏产业信息收集" href="14521195111050.html">游戏产业信息收集</a></li>
                        
                          <li><a title="游戏发展史" href="14521194839446.html">游戏发展史</a></li>
                        
                          <li><a title="辐射系列" href="14521194076173.html">辐射系列</a></li>
                        

                    
                      <li class="side-title"><span>生活品质</span></li>
                        
                          <li><a title="成为作家" href="14521287232350.html">成为作家</a></li>
                        
                          <li><a title="广州美食地图" href="14521207453879.html">广州美食地图</a></li>
                        
                          <li><a title="聊聊钢笔" href="14521202963339.html">聊聊钢笔</a></li>
                        
                          <li><a title="聊聊电影" href="14521201092159.html">聊聊电影</a></li>
                        

                    
                      <li class="side-title"><span>生活技巧</span></li>
                        
                          <li><a title="烤箱指南" href="14521207693233.html">烤箱指南</a></li>
                        
                          <li><a title="租房手册" href="14521200170697.html">租房手册</a></li>
                        
                          <li><a title="买车指南" href="14521200170757.html">买车指南</a></li>
                        
                          <li><a title="北美二手车购买指南" href="14521200391261.html">北美二手车购买指南</a></li>
                        

                    
                      <li class="side-title"><span>编程语言</span></li>
                        
                          <li><a title="如何选择编程语言" href="14521020035250.html">如何选择编程语言</a></li>
                        
                          <li><a title="Python 学习指南" href="14521095530236.html">Python 学习指南</a></li>
                        
                          <li><a title="Python 编程思想" href="14521096832549.html">Python 编程思想</a></li>
                        
                          <li><a title="Java 精要" href="14521089938127.html">Java 精要</a></li>
                        
                          <li><a title="Matlab 指南" href="14521020962952.html">Matlab 指南</a></li>
                        
                          <li><a title="OpenMP 入门指南" href="14521021178707.html">OpenMP 入门指南</a></li>
                        
                          <li><a title="Javascript 学习指南" href="14521020732534.html">Javascript 学习指南</a></li>
                        
                          <li><a title="C++ 学习笔记" href="14520969294593.html">C++ 学习笔记</a></li>
                        
                          <li><a title="SQL 入门指南" href="14521021831775.html">SQL 入门指南</a></li>
                        
                          <li><a title="CUDA架构" href="14521020248399.html">CUDA架构</a></li>
                        

                    
                      <li class="side-title"><span>Android</span></li>
                        
                          <li><a title="Android Studio 指南" href="14520922857884.html">Android Studio 指南</a></li>
                        
                          <li><a title="Google Map V2 使用指南" href="14520474070782.html">Google Map V2 使用指南</a></li>
                        
                          <li><a title="ActionBar 使用指南" href="14520474070736.html">ActionBar 使用指南</a></li>
                        

                    
                      <li class="side-title"><span>Web</span></li>
                        
                          <li><a title="网络协议指南" href="14520476779491.html">网络协议指南</a></li>
                        
                          <li><a title="Node.js 命令行程序实例" href="14520476779456.html">Node.js 命令行程序实例</a></li>
                        
                          <li><a title="Flask Study Note" href="14520476779406.html">Flask Study Note</a></li>
                        

                    
                      <li class="side-title"><span>关于</span></li>
                        
                          <li><a title="版权声明" href="14520576386063.html">版权声明</a></li>
                        

                    
                  </ul>
                </nav>
                </div>
              </div>
            </div>
            <div class="large-9 medium-9 columns">

 <div class="markdown-body">
<h1>机器学习指南</h1>

<p>借着面试的机会，总结一些机器学习相关知识点，算是小小的复习。</p>

<h2 id="toc_0">基本概念理解</h2>

<ul>
<li>监督学习与非监督学习

<ul>
<li>监督学习需要标注数据（KNN, NB, SVM, DT, BP, RF, GBRT），这类算法必须知道预测什么，即目标变量的分类信息。对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。</li>
<li>非监督学习（KMEANS, DL）数据没有类别信息，也不会给定目标值，对未标记的样本进行训练学习，比发现这些样本中的结构知识。将数据集合分成由类似的对象组成的多个类的过程被称为<strong>聚类</strong></li>
</ul></li>
<li>半监督式学习

<ul>
<li>在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。</li>
</ul></li>
<li>强化学习

<ul>
<li>在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。<br/></li>
</ul></li>
<li>在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。 而强化学习更多的应用在机器人控制及其他需要进行系统控制的领域。</li>
<li>离散数据与连续数据

<ul>
<li>离散数据（标称型）的目标变量结果只在有限目标集中取值，比方说真与假，一般用于<strong>分类</strong></li>
<li>连续数据（数值型）目标变量主要用于<strong>回归</strong>分析，通过给定数据点的最优拟合曲线</li>
</ul></li>
<li>生成方法和判别方法

<ul>
<li>判别方法：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。典型的判别模型包括k近邻，感知级，决策树，支持向量机等。</li>
<li>判别方法的特点：判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X)。不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。直接面对预测，往往学习的准确率更高。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。</li>
<li>k 近邻，决策树</li>
<li>生成方法：由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类，就像上面说的那样。</li>
<li>生成方法的特点：上面说到，生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。生成方法可以还原出联合概率分布P(Y|X)，而判别方法不能。生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。 </li>
<li>NB</li>
</ul></li>
<li>由生成模型可以得到判别模型，但由判别模型得不到生成模型。 </li>
<li>过拟合

<ul>
<li>如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。</li>
<li>产生原因

<ul>
<li>因为参数太多，会导致我们的模型复杂度上升，容易过拟合</li>
<li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征</li>
<li>解决方法

<ul>
<li>交叉验证法</li>
<li>减少特征</li>
<li>正则化</li>
<li>权值衰减</li>
<li>验证数据</li>
</ul></li>
</ul></li>
</ul></li>
<li>泛化能力是指模型对未知数据的预测能力</li>
<li>线性分类器与非线性分类器

<ul>
<li>如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是</li>
<li>常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归</li>
<li>常见的非线性分类器：决策树、RF、GBDT、多层感知机</li>
<li>SVM两种都有(看线性核还是高斯核)</li>
<li>线性分类器速度快、编程方便，但是可能拟合效果不会很好</li>
<li>非线性分类器编程复杂，但是效果拟合能力强</li>
</ul></li>
<li>特征比数据量还大时，选择什么样的分类器？

<ul>
<li>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</li>
</ul></li>
<li>对于维度很高的特征，你是选择线性还是非线性分类器？

<ul>
<li>线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能线性可分</li>
</ul></li>
<li>对于维度极低的特征，你是选择线性还是非线性分类器？

<ul>
<li>非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分</li>
</ul></li>
</ul>

<h2 id="toc_1">朴素贝叶斯(NB)</h2>

<ul>
<li>优点

<ul>
<li>对小规模的数据表现很好</li>
<li>适合多分类任务</li>
<li>适合增量式训练</li>
</ul></li>
<li>缺点

<ul>
<li>对输入数据的表达形式很敏感</li>
</ul></li>
<li>适用数据范围

<ul>
<li>标称型</li>
</ul></li>
<li>算法类型

<ul>
<li>分类算法</li>
</ul></li>
</ul>

<p>朴素贝叶斯是贝叶斯理论的一部分，贝叶斯决策理论的核心思想，即选择具有高概率的决策。朴素贝叶斯之所以冠以朴素开头，是因为其在贝叶斯理论的基础上做出了两点假设：</p>

<ol>
<li>每个特征之间相互独立。</li>
<li>每个特征同等重要。</li>
</ol>

<p>贝叶斯准则是构建在条件概率的基础之上的，其公式如下：</p>

<p>\[P(H|X)=\frac{P(X|H)}{P(X)}\]</p>

<p>ps：P(H|X）是根据X参数值判断其属于类别H的概率，称为后验概率。P(H)是直接判断某个样本属于H的概率，称为先验概率。P(X|H)是在类别H中观测到X的概率（后验概率），P(X)是在数据库中观测到X的概率。可见贝叶斯准则是基于条件概率并且和观测到样本的先验概率和后验概率是分不开的。</p>

<p>总结：对于分类而言，使用概率有事要比使用硬规则更为有效。贝叶斯概率及贝叶斯准则提供了一种利用已知值来估计未知概率的有效方法。可以通过特征之间的条件独立性假设，降低对数据量的需求。尽管条件独立性的假设并不正确，但是朴素贝叶斯仍然是一种有效的分类器。</p>

<p>一些要注意的地方：</p>

<ul>
<li>给出的特征向量长度可能不同，所以需要归一化为统一长度的向量。比如说文本分类，如果特征是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数</li>
<li>利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率。如果其中一个概率值为0，那么最后乘积也为0。为了降低这种影响，可以将所有词出现数字初始化为1，并将分母初始化为2。拉普拉斯平滑法将每个k值出现次数事先都加1，通俗讲就是假设他们都出现过一次。</li>
<li>另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的，这里取对数，就可以把乘法变为加法，并且对最后结果没有影响。</li>
<li>遇到特征之间不独立问题，参考改进的贝叶斯网络，使用DAG来进行概率图的描述</li>
</ul>

<h2 id="toc_2">线性回归(Linear Regression)</h2>

<ul>
<li>优点

<ul>
<li>结果易于理解</li>
<li>计算上不复杂。</li>
</ul></li>
<li>缺点

<ul>
<li>对非线性数据拟合不好。</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
<li>算法类型

<ul>
<li>回归算法</li>
</ul></li>
</ul>

<p>在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>

<p>线性方程的模型函数的向量表示形式为：\(h_\theta(x)=\theta^TX\)</p>

<p>通过训练数据集寻找向量系数的最优解，即为求解模型参数。其中求解模型系数的优化器方法可以用“最小二乘法”、“梯度下降”算法，来求解损失函数的最优解：</p>

<p>\[J(\theta)=\frac{1}{2}\sum_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})\]</p>

<p>\[min_\theta \; J_\theta\]</p>

<h3 id="toc_3">最小二乘法</h3>

<p>将训练特征表示为X矩阵，结果表示成y向量，仍然是线性回归模型，误差函数不变。那么θ可以直接由下面公式得出</p>

<p>\[\theta=(X^TX)^{-1}X^Ty\]</p>

<p>这里 y 是向量，此方法要求X是列满秩的，而且求矩阵的逆比较慢。</p>

<p>而在LWLR（局部加权线性回归）中，参数的计算表达式为:</p>

<p>\[\theta=(X^TX)^{-1}X^TWy\]</p>

<p>因为此时优化的是 \(\sum_i w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2\)</p>

<p>由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。</p>

<h3 id="toc_4">岭回归（ridge regression）</h3>

<p>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。</p>

<p>岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数K（1&gt;K&gt;0），并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。</p>

<p>总结：与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型的变量，而后者预测离散型的变量。回归是统计学中最有力的工具之一。在回归方程里，求得特征对应的最佳回归系统的方法是最小化误差的平方和。</p>

<h2 id="toc_5">k-近邻算法(kNN)</h2>

<ul>
<li>优点

<ul>
<li>精度高</li>
<li>对异常值不敏感</li>
<li>无数据输入假定</li>
</ul></li>
<li>缺点

<ul>
<li>计算复杂度高</li>
<li>空间复杂度高</li>
</ul></li>
<li>适用数据范围

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
<li>算法类型

<ul>
<li>分类算法。</li>
</ul></li>
</ul>

<p>存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中对应的特征进行比较，然后算法提取样本集中前 k 个最相似的数据。最后，选择 k 个最相似数据中出现次数最多的分类，作为新数据的分类。</p>

<p>为了避免不同的特征的数值不同所导致的影响不同，可能需要进行归一化，也就是把特征值转换成[0,1]值</p>

<p>k-近邻算法是分类数据最简单最有效的算法，必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用可能非常耗时。<strong>k决策树</strong>是其优化版本，可以节省大量的计算开销。</p>

<p>另一个却显示它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。使用<strong>概率测量方法</strong>可以解决这个问题。</p>

<p>三要素</p>

<ul>
<li>k值的选择</li>
<li>距离的度量（常见的距离度量有欧式距离，马氏距离等）</li>
<li>分类决策规则 （多数表决规则）</li>
</ul>

<p>k值的选择</p>

<ul>
<li>k值越小表明模型越复杂，更加容易过拟合</li>
<li>但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类</li>
</ul>

<p>所以一般k会取一个较小的值，然后用过交叉验证来确定。这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k</p>

<h3 id="toc_6">KD树</h3>

<p>KD树是一个二叉树，表示对K维空间的一个划分，可以进行快速检索（那KNN计算的时候不需要对全样本进行距离的计算了）</p>

<p>在k维的空间上循环找子区域的中位数进行划分的过程。假设现在有K维空间的数据集T={x1,x2,x3,…xn},xi={a1,a2,a3..ak}</p>

<ol>
<li>首先构造根节点，以坐标a1的中位数b为切分点，将根结点对应的矩形局域划分为两个区域，区域1中a1b</li>
<li>构造叶子节点，分别以上面两个区域中a2的中位数作为切分点，再次将他们两两划分，作为深度1的叶子节点，（如果a2=中位数，则a2的实例落在切分面）</li>
<li>不断重复2的操作，深度为j的叶子节点划分的时候，索取的ai 的i=j%k+1，直到两个子区域没有实例时停止</li>
</ol>

<p>KD树的搜索</p>

<ol>
<li>首先从根节点开始递归往下找到包含x的叶子节点，每一层都是找对应的xi</li>
<li>将这个叶子节点认为是当前的“近似最近点”</li>
<li>递归向上回退，如果以x圆心，以“近似最近点”为半径的球与根节点的另一半子区域边界相交，则说明另一半子区域中存在与x更近的点，则进入另一个子区域中查找该点并且更新”近似最近点“</li>
<li>重复3的步骤，直到另一子区域与球体不相交或者退回根节点</li>
<li>最后更新的”近似最近点“与x真正的最近点</li>
</ol>

<p><strong>KD树进行KNN查找</strong></p>

<p>通过KD树的搜索找到与搜索目标最近的点，这样KNN的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。</p>

<p><strong>KD树搜索的复杂度</strong></p>

<p>当实例随机分布的时候，搜索的复杂度为log(N)，N为实例的个数，KD树更加适用于实例数量远大于空间维度的KNN搜索，如果实例的空间维度与实例个数差不多时，它的效率基于等于线性扫描。</p>

<h2 id="toc_7">决策树</h2>

<p>决策树的主要优势在于数据形式非常容易理解。决策树很多任务都是为了数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，机器学习算法最终将使用这些机器从数据集中创造的规则。</p>

<ul>
<li>优点

<ul>
<li>计算复杂度不高</li>
<li>输出结果易于理解</li>
<li>对中间值的缺失不敏感</li>
<li>可以处理不相关特征数据</li>
</ul></li>
<li>缺点

<ul>
<li>容易过拟合（后面出现了随机森林）</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
<li>算法类型

<ul>
<li>分类算法。</li>
</ul></li>
<li>数据要求

<ul>
<li>树的构造只适用于标称型的数据，因此数值型数据必须离散化。</li>
</ul></li>
</ul>

<p>在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。</p>

<p>创建分支的伪代码如下：</p>

<pre><code>检测数据集中的每个子项是否属于同一分类：
   if so return 类标签；
   else
       寻找数据集的最好特征
       划分数据集
       创建分支结点
           for 每个划分的子集
               调用函数createBranch并增加返回结果到分支结点中
           return 分支结点
</code></pre>

<p>划分数据集的大原则是：将无序的数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息。<strong>信息增益(information gain)</strong>和<strong>熵(entropy)</strong></p>

<p>信息熵的计算公式为：</p>

<p>\[H=-\sum P(x_i)log_2P(x_i)\]</p>

<p>其中 \(P(x_i)\) 表示选择该分类的概率。</p>

<p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，直到数据集中的所有数据属于同一分类。</p>

<p>ID3 算法可以用于划分标称型数据集。构造决策树时，我们通常采用递归的方法将数据集转化为决策树。</p>

<p>决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题。我们可以通过裁剪决策树，合并相邻的无法产生大量信息增益的叶节点，消除过度匹配的问题。</p>

<h3 id="toc_8">ID3、C4.5&amp;CART</h3>

<p>其实不同的决策树学习算法只是它们选择特征的依据不同，决策树的生成过程都是一样的（根据当前环境对特征进行贪婪的选择）。</p>

<p>ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，每一次都选择使得信息增益最大的特征进行分裂，递归地构建决策树。</p>

<p>ID3算法以信息增益作为划分训练数据集的特征，有一个致命的缺点。选择取值比较多的特征往往会具有较大的信息增益，所以ID3偏向于选择取值较多的特征。</p>

<p>针对ID3算法的不足，C4.5算法根据信息增益比来选择特征，对这一问题进行了校正。</p>

<p>CART指的是分类回归树，它既可以用来分类，又可以被用来进行回归。CART用作回归树时用平方误差最小化作为选择特征的准则，用作分类树时采用基尼指数最小化原则，进行特征选择，递归地生成二叉树。</p>

<p>决策树的剪枝：我们知道，决策树在生成的过程中采用了贪婪的方法来选择特征，从而达到对训练数据进行更好地拟合（其实从极端角度来看，决策树对训练集的拟合可以达到零误差）。而决策树的剪枝是为了简化模型的复杂度，防止决策树的过拟合问题。具体的决策树剪枝策略可以参见李航的《统计学习方法》。</p>

<h2 id="toc_9">Logistic 回归</h2>

<ul>
<li>优点

<ul>
<li>计算代价不高</li>
<li>易于理解和实现</li>
</ul></li>
<li>缺点

<ul>
<li>容易欠拟合</li>
<li>分类精度可能不高</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
<li>算法类别

<ul>
<li>分类算法</li>
</ul></li>
<li>适用场景

<ul>
<li>二分类问题</li>
</ul></li>
</ul>

<p>Logistic 回归的目的是寻找一个非线性函数 Sigmoid 的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。</p>

<p>logistic函数表达式为：</p>

<p>\[h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}\]</p>

<p>Sigmoid 函数的定义为 </p>

<p>\[g(z) = \frac{1}{1+e^{-z}}\]</p>

<p>导数形式为</p>

<p>\[g&#39;(z)=g(z)(1-g(z))\]</p>

<p>函数值域范围(0,1)。可以用来做分类器。Sigmoid函数的函数曲线如下：</p>

<p><img src="media/14522640452315/14522842012796.jpg" alt=""/></p>

<p>逻辑回归模型分解如下：</p>

<p>(1)首先将不同维度的属性值和对应的一组权重加和，公式如下： </p>

<p>\[z=w_0+w_1x_1+w_2x_2+\dots+w_mx_m\]</p>

<p>其中\(x_1,x_2,\dots,x_m\)是某样本数据的各个特征，维度为m</p>

<p>这里就是一个线性回归。W权重值就是需要经过训练学习到的数值，具体W向量的求解，就需要用到极大似然估计和将似然估计函数代入到 优化算法来求解。最常用的最后化算法有 梯度上升算法。</p>

<p>单个样本的后验概率为：</p>

<p>\[p(y\;|\;x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{(1-y)}\]</p>

<p>整个样本的后验概率：</p>

<p>\[L(\theta)=\sum_{i=1}^mp(y^{(i)}\;|\;x^{(i)},\theta)\]</p>

<p>其中 </p>

<p>\[P(y=1\;|\;x, \theta)=h_\theta(x)\]</p>

<p>\[P(y=0\;|\;x, \theta)=1-h_\theta(x)\]</p>

<p>对整个样本的后验概率取对数得到：</p>

<p>\[\ell(\theta)=logL(\theta)=\sum_{i=1}^my^{(i)}log\;h(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))\]</p>

<p>然后利用梯度下降来进行求解，得到最终的 \(\theta\)</p>

<p>由上面可见：逻辑回归函数虽然是一个非线性的函数，但其实其去除Sigmoid映射函数之后，其他步骤都和线性回归一致。</p>

<p>(2)然后将上述的线性目标函数 z 代入到sigmond逻辑回归函数，可以得到值域为（0,0.5)和（0.5,1）两类值，等于0.5的怎么处理还以自己定。这样其实就得到了2类数据，也就体现了二分类的概念。</p>

<p>总结：Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，参数的求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法有可以简化为随机梯度上升算法。</p>

<p>随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，而不需要重新读取整个数据集来进行批处理运算。</p>

<p>处理数据中的缺失值的技巧：</p>

<ul>
<li>使用可用特征的均值来填补缺失值</li>
<li>使用特殊值来填补缺失值，如 -1</li>
<li>忽略有缺失值的样本</li>
<li>使用相似样本的均值填补缺失值</li>
<li>使用另外的机器学习算法预测缺失值</li>
</ul>

<h3 id="toc_10">关于LR的过拟合问题</h3>

<p>如果我们有很多的特性，在训练集上拟合得很好，但是在预测集上却达不到这种效果</p>

<ol>
<li>减少feature个数（人工定义留多少个feature、算法选取这些feature）</li>
<li>正则化（留下所有的feature，但对于部分feature定义其parameter非常小）</li>
</ol>

<h3 id="toc_11">关于LR的多分类：softmax</h3>

<p>softmax:假设离散型随机变量Y的取值集合是{1,2,..,k},则多分类的LR为</p>

<p>\[P(Y=a\;|\;x)=exp(w_ax)/(1-\sum_{i=1}^kw_ix)\]</p>

<p>这里会输出当前样本下属于哪一类的概率，并且满足全部概率加起来=1</p>

<h3 id="toc_12">关于softmax和k个LR的选择</h3>

<p>如果类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种）就用softmax</p>

<p>否则类别之前有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），这个时候使用k个LR更为合适</p>

<h2 id="toc_13">树回归</h2>

<ul>
<li>优点

<ul>
<li>可以对复杂和非线性的数据建模。</li>
</ul></li>
<li>缺点

<ul>
<li>结果不易理解。</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
<li>算法类型

<ul>
<li>回归算法</li>
</ul></li>
</ul>

<p>简述：线性回归方法可以有效的拟合所有样本点(局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的回归算法是比较困难的。此外，实际中很多问题为非线性的，例如常见的分段函数，不可能用全局线性模型类进行拟合。树回归将数据集切分成多份易建模的数据，然后利用线性回归进行建模和拟合。较为经典的树回归算法为CART（classification and regreesion trees 分类回归树）。</p>

<p>分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：</p>

<ul>
<li>分类树：基尼指数最小化(gini_index)</li>
<li>回归树：平方误差最小化</li>
</ul>

<p>分类树：</p>

<ol>
<li>首先是根据当前特征计算他们的基尼增益</li>
<li>选择基尼增益最小的特征作为划分特征</li>
<li>从该特征中查找基尼指数最小的分类类别作为最优划分点</li>
<li>将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于</li>
<li>针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值</li>
</ol>

<p>gini用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）</p>

<h3 id="toc_14">解决决策树的过拟合</h3>

<ol>
<li>剪枝

<ol>
<li>前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）</li>
<li>后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）</li>
</ol></li>
<li>交叉验证</li>
<li>随机森林</li>
</ol>

<h2 id="toc_15">随机森林 RF</h2>

<p><strong>优缺点</strong></p>

<ol>
<li>能够处理大量特征的分类，并且还不用做特征选择</li>
<li>能够处理具有高维特征的输入样本，而且不需要降维</li>
<li>在训练完成之后能给出哪些feature的比较重要</li>
<li>在生成过程中，能够获取到内部生成误差的一种无偏估计</li>
<li>对于缺省值问题也能够获得很好得结果</li>
<li>训练速度很快，很容易并行</li>
<li>实现相对来说较为简单</li>
</ol>

<p>随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想--集成思想的体现。</p>

<p>其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。</p>

<p>bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，所以bagging改进了预测准确率但损失了解释性。</p>

<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>

<p>随机森林分类效果（错误率）与两个因素有关：</p>

<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>

<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>

<p>构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。</p>

<p>随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。</p>

<p>我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。<br/>
而这样的采样特点就允许我们进行oob估计，它的计算方式如下：</p>

<ol>
<li>对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；</li>
<li>然后以简单多数投票作为该样本的分类结果；</li>
<li>最后用误分个数占样本总数的比率作为随机森林的oob误分率。</li>
</ol>

<p><strong>学习过程</strong></p>

<ol>
<li>现在有N个训练样本，每个样本的特征为M个，需要建K颗树</li>
<li>从N个训练样本中有放回的取N个样本作为一组训练集（其余未取到的样本作为预测分类，评估其误差），每棵树的训练集都是不同的，而且里面包含重复的训练样本</li>
<li>从M个特征中取m个特征左右子集特征(m&lt;&lt;M)，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li>
<li>对采样的数据使用完全分裂的方式来建立决策树，这样的决策树每个节点要么无法分裂，要么所有的样本都指向同一个分类，每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
<li>重复2的过程K次，即可建立森林</li>
</ol>

<p><strong>预测过程</strong></p>

<ol>
<li>将预测样本输入到K颗树分别进行预测</li>
<li>如果是分类问题，直接使用投票的方式选择分类频次最高的类别</li>
<li>如果是回归问题，使用分类之后的均值作为结果</li>
</ol>

<p><strong>参数问题</strong></p>

<ol>
<li>这里的一般取m=sqrt(M)</li>
<li>关于树的个数K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量）</li>
<li>树的最大深度，（太深可能可能导致过拟合）</li>
<li>节点上的最小样本数、最小信息增益</li>
</ol>

<p><strong>学习算法</strong></p>

<ol>
<li>ID3算法：处理离散值的量</li>
<li>C45算法：处理连续值的量</li>
<li>Cart算法：离散和连续 </li>
</ol>

<p>随机森林是一种集成学习+决策树的分类模型，它可以利用集成的思想（投票选择的策略）来提升单颗决策树的分类性能（通俗来讲就是“三个臭皮匠，顶一个诸葛亮”）。</p>

<p>集集成学习和决策树于一身，随机森林算法具有众多的优点，其中最为重要的就是在随机森林算法中每棵树都尽最大程度的生长，并且没有剪枝过程。</p>

<p>随机森林引入了两个随机性——随机选择样本（bootstrap sample）和随机选择特征进行训练。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）</p>

<h3 id="toc_16">GDBT</h3>

<p>迭代决策树GBDT（Gradient Boosting Decision Tree）也被称为是MART（Multiple Additive Regression Tree)）或者是GBRT（Gradient Boosting Regression Tree），也是一种基于集成思想的决策树模型，但是它和Random Forest有着本质上的区别。不得不提的是，GBDT是目前竞赛中最为常用的一种机器学习算法，因为它不仅可以适用于多种场景，更难能可贵的是，GBDT有着出众的准确率。这也是为什么很多人称GBDT为机器学习领域的“屠龙刀”。</p>

<p>这么牛叉的算法，到底是怎么做到的呢？说到这里，就不得不说一下GBDT中的“GB”（Gradient Boosting）。Gradient Boosting的原理相当的复杂，但是看不懂它也不妨碍我们对GBDT的理解和认识</p>

<p>Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？难道是每棵树独立训练一遍，比如A这个人，第一棵树认为是10岁，第二棵树认为是0岁，第三棵树认为是20岁，我们就取平均值10岁做最终结论？当然不是！且不说这是投票方法并不是GBDT，只要训练集不变，独立训练三次的三棵树必定完全相同，这样做完全没有意义。之前说过，GBDT是把所有树的结论累加起来做最终结论的，所以可以想到每棵树的结论并不是年龄本身，而是年龄的一个累加量。GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义。</p>

<p>一直认为李航博士讲的机器学习更加贴近算法的本质，我们先来看一下他是如何对GBDT进行定义的（在《统计学习方法中》，GBDT又被称为是提升树boosting tree）。</p>

<p>提升方法实际采用了加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树，对分类问题决策树是二叉分类树，而对于回归问题决策树是二叉回归树。提升树模型可以表示为决策树的加法模型：</p>

<p><img src="media/14522640452315/14524372472070.jpg" alt=""/></p>

<p>其中，<img src="media/14522640452315/14524372588248.jpg" alt=""/><br/>
表示决策树；<img src="media/14522640452315/14524372642480.jpg" alt=""/><br/>
表示决策树的参数；M为树的个数。</p>

<p>针对不同问题的提升树（GBDT），其主要区别在于使用的损失函数不同，包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。</p>

<p>提升树的流程：</p>

<p><img src="media/14522640452315/14524372906494.jpg" alt=""/></p>

<h2 id="toc_17">支持向量机</h2>

<ul>
<li>优点

<ul>
<li>泛化错误率低</li>
<li>计算开销不大</li>
<li>结果易解释</li>
</ul></li>
<li>缺点

<ul>
<li>对参数调节和核函数的选择敏感</li>
<li>原始分类器不加修改仅适用于处理二元分类问题</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
<li>类别

<ul>
<li>分类算法</li>
</ul></li>
<li>适用场景

<ul>
<li>解决二分类问题。</li>
</ul></li>
</ul>

<p>将数据集分隔开来的直线称为<strong>分隔超平面(separating hyperplane)</strong>。如果数据对象是1024维的，那么就需要一个1023维的某某对象来对数据进行分隔，这个对象就叫<strong>超平面(hyperplane)</strong>，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。</p>

<p>我们希望能找到离分隔超平面最近的点，确保它们离分隔面的距离尽可能远。这里点到分隔面的距离被称为<strong>间隔(margin)</strong>。我们希望间隔尽可能大，因为如果我们犯错或者在有限数据上训练分类器的话，我们希望分类器尽可能健壮。</p>

<p><strong>支持向量(support vector)</strong>就是离分隔超平面最近的那些点。接下来要试着最大化支持向量到分隔面的距离。</p>

<p>分隔超平面的形式可以写成 \(w^T+b\)。要计算点 A 到分隔超平面的距离，就必须给出点到分隔面的法线或垂线的长度，值为 \[\frac{|w^TA+b|}{||w||}\]。这里的常数 b 类似于 Logistic 回归中的截距 \(w_0\)。</p>

<p>当计算数据点到分隔面的距离并确定分隔面的放置位置时，间隔是通过 \(label\times(w^T+b)\)来计算的。如果数据点处于正方向(+1)，\(w^Tx+b\) 会是一个很大的正数，同时 \(label\times(w^T+b)\)也会是一个很大的正数；而处于负方向时(-1)，\(label\times(w^T+b)\) 仍然会是一个很大的正数。</p>

<p>现在的目标就是找出分类器定义中的 w 和 b。为此，我们必须找到具有最小间隔的数据点，而这些数据点也就是前面提到的支持向量。一旦找到具有最小间隔的数据点，我们就需要对该间隔最大化：</p>

<p>\[arg max_{w,b}\{min_n(label·(w^Tx+b))·\frac{1}{||w||}\}\]</p>

<p>直接求解上述问题相当困难，所以需要将它转换成为另一种更容易求解的形式。</p>

<p>先考察一下大括号中的部分。由于对乘积进行优化是一件很讨厌的事情，因此我们要做的是固定其中一个因子而最大化其他因子。如果令所有支持向量的 \(label\times(w^T+b)\)  都为 1，那么就可以通过求 ||w|| 的最大值来得到最终解。但是，并非所有数据点的 \(label\times(w^T+b)\)  都等于 1，只有那些离分割超平面最近的点得到的值才为 1。而离超平面越远的数据点，其 \(label\times(w^T+b)\)  的值也就越大。</p>

<p>这里的约束条件就是 \(label\times(w^T+b) \ge 1\) 。对于这类优化问题，有一个非常著名的求解方法，拉格朗日乘子法。通过引入拉格朗日乘子，我们就可以基于约束条件来表述原来的问题。由于这里的约束条件都是基于数据点的，因此我们就可以将超平面写成数据点的形式，优化函数就变成</p>

<p>\[max_\alpha[\sum_{i=1}^m\alpha-\frac{1}{2}label^{(i)}·label^{(j)}·\alpha_i·\alpha_j\langle x^{(i)},x^{(j)}\rangle]\]</p>

<blockquote>
<p>\(label\times(w^T+b)\) 被称为点到分隔面的函数间隔，\(\frac{|w^TA+b|}{||w||}\) 称为点到分隔面的几何间隔<br/>
尖括号表示两个向量的内积</p>
</blockquote>

<p>约束条件为</p>

<p>\[\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0\]</p>

<p>考虑到数据不可能非常完美，就需要引入<strong>松弛变量(slack variable)</strong>来允许有些数据点可以处于分隔面错误的一侧。这样我们的优化目标就能保持仍然不变，但约束条件变成：</p>

<p>\[C \ge\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0\]</p>

<p>这里的常数 C 用于控制“最大化间隔”和“保证大部分点的函数间隔小于1.0”这两个目标的权重。在优化算法的实现代码中，常数 C 是一个参数，因此我们就可以通过调节该参数得到不同的结果。一旦求出了所有的 α，那么分隔超平面就可以通过这些 α 来表达。这一结论十分直接，SVM 中的主要工作就是求解这些 α。</p>

<h3 id="toc_18">SMO 高效优化算法（Sequential Minimal Optimization，SMO）</h3>

<p><strong>Platt 的 SMO 算法</strong></p>

<p>SMO 表示<strong>序列最小化(Sequential Minimal Optimization)</strong>。Platt 的 SMO 算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果是完全一致的。</p>

<p>SMO 算法的目标是求出一系列 α 和 b，一旦求出了这些 α，就很容易计算出权重向量 w 并得到分隔超平面。</p>

<p>它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：</p>

<ol>
<li>其中一个是严重违反KKT条件的一个变量</li>
<li>另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。</li>
</ol>

<p>SMO 算法的工作原理是：每次循环中选择两个 α 进行优化处理。一旦找到一对合适的 α，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个 α 必须要符合一定的条件，条件之一就是这两个 α 必须要在间隔边界之外，第二个条件则是这两个 α 还没有进行过区间化处理或者不在边界上。</p>

<p>Platt SMO 算法中的外循环确定要优化的最佳 α 对。而简化版会跳过这一部分，首先在数据集上遍历每一个 α，然后在剩下的 α 集合中随机选择另一个 α，从而构建 α 对。这一点相当重要，要同时改变，因为我们有一个约束条件：</p>

<p>\[\alpha \ge 0 \; and \; \sum_{i=1}^m \alpha_i·label^{(i)} = 0\]</p>

<p>由于改变一个 α 可能会导致该约束条件失效，因此我们总是同时改变两个 α。</p>

<p>伪代码：</p>

<pre><code>创建一个 α 向量并将其初始化为 0 向量
当迭代次数小于最大迭代次数时(外循环)
    对数据集中的每个数据向量(内循环):
        如果该数据向量可以被优化:
            随机选择另外一个数据向量
            同时优化这两个向量
            如果两个向量都不能被优化，退出内循环
    如果所有向量都没有被优化，增加迭代数量，继续下一次循环
</code></pre>

<p><strong>完整的Platt SMO算法</strong></p>

<p>在在选择第一个 α 值后，算法会通过一个内循环来选择第二个 α 值。在优化过程中，会通过<strong>最大化步长</strong>的方式来获得第二个 α 值。</p>

<h3 id="toc_19">在复杂数据上应用核函数</h3>

<p>核函数(kernel) 和 径向基函数(fadial basis function)</p>

<h4 id="toc_20">利用核函数将数据映射到高维空间</h4>

<p>从某个特征空间到另一个特征空间的映射是通过核函数来实现的。可以把核函数想象成一个<strong>包装器(wrapper)</strong>或者是<strong>接口(interface)</strong>，它能把数据从某个很难处理的形式转换成另一种较易处理的形式。</p>

<p>SVM 优化中一个特别好的地方是，所有的运算都可以写成<strong>内积(inner product)</strong>的形式。向量的内积指的是两个向量相乘，之后得到单个标量或者数值。我们可以把内积运算替换成核函数，而不必做简化处理。将内积替换成核函数的方式被称为<strong>核技巧(kernel trick)</strong>或者<strong>核变电(kernel substation)</strong>。</p>

<h4 id="toc_21">径向基核函数</h4>

<p>采用向量作为自变量的函数，基于向量距离运算输出一个标量。这个距离可以是从<0, 0>向量或者其他向量开始计算的距离，我们使用径向基函数的高斯版本，公式如下：</p>

<p>\[k(x,y)=exp(\frac{-||x-y||^2}{2\sigma^2})\]</p>

<p>上述高斯核函数将数据从其特征空间映射到更高维的空间，具体来说这里是映射到一个无穷维的空间。</p>

<p>支持向量的数目存在一个最优值。SVM 的优点在于它能对数据进行高效分类。如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类方法称为 k近邻。</p>

<p>可以这么看 SVM 比 k 近邻好的地方在于，从很多数据中找到最有代表性的数据点来作为分类的依据，可以有效减少多余的计算。</p>

<h3 id="toc_22">SVM 多分类方法</h3>

<p><strong>一对多</strong></p>

<p>其中某个类为一类，其余n-1个类为另一个类，比如A,B,C,D四个类，第一次A为一个类，{B,C,D}为一个类训练一个分类器，第二次B为一个类,{A,C,D}为另一个类,按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x),f2(x),f3(x)和f4(x),取其最大值为分类器(这种方式由于是1对M分类，会存在偏置，很不实用)</p>

<p><strong>一对一(libsvm实现的方式)</strong></p>

<p>任意两个类都训练一个分类器，那么n个类就需要<code>n*(n-1)/2</code>个svm分类器。</p>

<p>还是以A,B,C,D为例,那么需要{A,B},{A,C},{A,D},{B,C},{B,D},{C,D}为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要<code>n*(n-1)/2</code>个分类器代价太大，不过有好像使用循环图来进行改进）</p>

<h3 id="toc_23">小结</h3>

<p>支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，且学到的结果具有很好的推广性。这些优点使得向量机十分流行，有些人认为它是监督学习中最好的定式算法。</p>

<p>支持向量机试图通过求解一个二次优化问题来最大化分类间隔。</p>

<p>和方法或者说核技巧会将数据(有时是非线性数据)从一个低维空间映射到一个高维空间，可以将一个在低维空间中的非线性问题转换成高维空间下的线性问题来求解。和方法不止在 SVM 中适用，还可以用于其他算法中。而其中径向基函数是一个常用的度量两个向量距离的核函数。</p>

<p>支持向量机是一个二元分类器。当用其解决多元问题时，则需要额外的方法对其进行扩展。SVM 的效果也对优化参数和所用核函数中的参数敏感。</p>

<h2 id="toc_24">Bagging 和 Boosting</h2>

<p>使用机器学习方法解决问题时，有较多模型可供选择。 一般的思路是先根据数据的特点，快速尝试某种模型，选定某种模型后， 再进行模型参数的选择（当然时间允许的话，可以对模型和参数进行双向选择）</p>

<p>因为不同的模型具有不同的特点， 所以有时也会将多个模型进行组合，以发挥<q>三个臭皮匠顶一个诸葛亮的作用</q>， 这样的思路， 反应在模型中，主要有两种思路：Bagging和Boosting</p>

<h3 id="toc_25">Bagging</h3>

<p>Bagging 可以看成是一种圆桌会议， 或是投票选举的形式，其中的思想是：<q>群众的眼光是雪亮的</q>，可以训练多个模型，之后将这些模型进行加权组合，一般这类方法的效果，都会好于单个模型的效果。 在实践中， 在特征一定的情况下，大家总是使用Bagging的思想去提升效果。 例如kaggle上的问题解决，因为大家获得的数据都是一样的，特别是有些数据已经过预处理。</p>

<p>基本的思路比较简单，就是：训练时，使用replacement的sampling方法， sampling一部分训练数据k次并训练k个模型；预测时，使用k个模型，如果为分类，则让k个模型均进行分类并选择出现次数最多的类(每个类出现的次数占比可以视为置信度)；如为回归，则为各类器返回的结果的平均值。</p>

<p>在该处，Bagging算法可以认为每个分类器的权重都一样。</p>

<h3 id="toc_26">Boosting</h3>

<p>在Bagging方法中，我们假设每个训练样本的权重都是一致的； 而Boosting算法则更加关注错分的样本，越是容易错分的样本，约要花更多精力去关注。对应到数据中，就是该数据对模型的权重越大，后续的模型就越要拼命将这些经常分错的样本分正确。 最后训练出来的模型也有不同权重，所以boosting更像是会整，级别高，权威的医师的话语权就重些。</p>

<p>Bagging和Boosting都可以视为比较传统的集成学习思路。 现在常用的Random Forest，GBDT，GBRank其实都是更加精细化，效果更好的方法</p>

<h2 id="toc_27">AdaBoost</h2>

<ul>
<li>优点

<ul>
<li>泛化错误率低</li>
<li>易编码，可以应用在大部分分类器上</li>
<li>无参数调整</li>
</ul></li>
<li>缺点

<ul>
<li>对离群点敏感</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
<li>标称型</li>
</ul></li>
</ul>

<p>boosting 是一种与 bagging 很类似的技术。不论是在 boosting 还是 bagging 当中，所使用的多个分类器的类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting 是通过集中关注被已有分类器错分的那些数据来获得新的分类器。</p>

<p>由于 boosting 分类的结果是基于所有分类器的加权求和结果的，因此 boosting 与 bagging 不太一样。bagging 中的分类器权重是相等的，而 boosting 中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。</p>

<p>boosting 方法拥有多个版本，这里只关注最流行的 AdaBoost</p>

<p>能否使用弱分类器和多个实例来构建一个强分类器？这是一个非常有趣的理论问题。这里的“弱”意味着分类器的性能比所及猜测要略好，但是也不会好太多。AdaBoost 算法即脱胎于上述理论问题。</p>

<p>AdaBoost 是 adaptive boosting(自适应 boosting)的缩写，其运行过程如下：训练数据中的每个样本各有一个权重，这些权重构成了向量 D。一开始，这些权重都初始化成相等值。首先在训练数据上训练处一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分对的样本的权重将会降低，而第一次分错的样本的权重将会提高。</p>

<p>为了从所有弱分类器中得到最终的分类结果，AdaBoost 为每个分类器都分配了一个权重值 α，这些 α 值是基于每个弱分类器的错误率进行计算的。错误率和 α 的公式为</p>

<p>\[\epsilon=\frac{未正确分类的样本数目}{所有样本数目}\]</p>

<p>\[\alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon})\]</p>

<p>计算出 α 值之后，可以对权重向量 D 进行更新，以使那些正确分类的样本的权重降低而错分样本的权重升高。D 的计算方法如下：</p>

<p>正确情况：</p>

<p>\[D_i^{(i+1)}=\frac{D_i^{(i)}e^{-\alpha}}{\sum D}\]</p>

<p>错误情况：</p>

<p>\[D_i^{(i+1)}=\frac{D_i^{(i)}e^{\alpha}}{\sum D}\]</p>

<p>计算出 D 之后，AdaBoost 又开始进入下一轮迭代。AdaBoost 算法会不断地重复训练和调整权重的过程，直到训练错误率为 0 或者弱分类器的数目达到用户的指定值为止。</p>

<p>构建弱分类器：<strong>单层决策树(decision stump, 决策树桩)</strong>是一种简单的决策树，仅基于单个特征来做决策。</p>

<h2 id="toc_28">KMeans</h2>

<ul>
<li>优点

<ul>
<li>容易实现</li>
<li>对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k&lt;&lt;n。这个算法通常局部收敛</li>
<li>算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好</li>
</ul></li>
<li>缺点

<ul>
<li>可能收敛到局部最小值</li>
<li>在大规模数据集上收敛较慢</li>
<li>k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合</li>
<li>要求用户必须事先给出要生成的簇的数目k</li>
<li>不适合于发现非凸面形状的簇，或者大小差别很大的簇</li>
<li>对于<q>噪声</q>和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响</li>
</ul></li>
<li>使用数据类型

<ul>
<li>数值型</li>
</ul></li>
<li>算法类型

<ul>
<li>聚类算法</li>
</ul></li>
</ul>

<p>K-Means的基本步骤：</p>

<ol>
<li>从数据对象中随机的初始化K个初始点作为质心。然后将数据集中的每个点分配到一个簇中，具体来讲每个点找到距其最近的质心，并将其分配给该质心所对应的簇。</li>
<li>计算每个簇中样本点的均值，然后用均值更新掉该簇的质心。然后划分簇结点。</li>
<li>迭代重复（2）过程，当簇对象不再发生变化时，或者误差在评测函数预估的范围时，停止迭代。</li>
</ol>

<p>选择批次距离尽可能远的K个点</p>

<p>首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的点作为第三个中心点，以此类推，直至选取大k个</p>

<p>选用层次聚类或者Canopy算法进行初始聚类</p>

<p>聚类属于无监督学习，以往的回归、朴素贝叶斯、SVM等都是有类别标签y的，也就是说样例中已经给出了样例的分类。而聚类的样本中却没有给定y，只有特征x，比如假设宇宙中的星星可以表示成三维空间中的点集(x,y,z)。聚类的目的是找到每个样本x潜在的类别y，并将同类别y的样本x放在一起。</p>

<p>在聚类问题中，给我们的训练样本是 {x(1),...,x(m)}，每个 x(i)∈ R<sup>n，没有了y。</sup></p>

<p>K-means算法是将样本聚类成k个簇（cluster），具体算法描述如下：</p>

<p><img src="media/14522640452315/14522846545067.jpg" alt=""/></p>

<p>下面累述一下K-means与EM的关系，首先回到初始问题，我们目的是将样本分成k个类，其实说白了就是求每个样例x的隐含类别y，然后利用隐含类别将x归类。由于我们事先不知道类别y，那么我们首先可以对每个样例假定一个y吧，但是怎么知道假定的对不对呢？怎么评价假定的好不好呢？我们使用样本的极大似然估计来度量，这里是就是x和y的联合分布P(x,y)了。如果找到的y能够使P(x,y)最大，那么我们找到的y就是样例x的最佳类别了，x顺手就聚类了。但是我们第一次指定的y不一定会让P(x,y)最大，而且P(x,y)还依赖于其他未知参数，当然在给定y的情况下，我们可以调整其他参数让P(x,y)最大。但是调整完参数后，我们发现有更好的y可以指定，那么我们重新指定y，然后再计算P(x,y)最大时的参数，反复迭代直至没有更好的y可以指定。</p>

<p>这个过程有几个难点，第一怎么假定y？是每个样例硬指派一个y还是不同的y有不同的概率，概率如何度量。第二如何估计P(x,y)，P(x,y)还可能依赖很多其他参数，如何调整里面的参数让P(x,y)最大。这些问题在以后的篇章里回答。</p>

<p>这里只是指出EM的思想，E步就是估计隐含类别y的期望值，M步调整其他参数使得在给定类别y的情况下，极大似然估计P(x,y)能够达到极大值。然后在其他参数确定的情况下，重新估计y，周而复始，直至收敛。</p>

<p>上面的阐述有点费解，对应于K-\(x^{(i)}\)对应隐含变量也就是最佳类别 \(c^{(i)}\)。最开始可以随便指定一个 \(c^{(i)}\) 给它，然后为了让 P(x,y) 最大（这里是要让 J 最小），我们求出在给定 c 情况下，J最小时的 \(u_j\)（前面提到的其他未知参数），然而此时发现，可以有更好的 \(c^{(i)}\)（质心与样例\(x^{(i)}\) 距离最小的类别）指定给样例 \(x^{(i)}\)，那么 \(c^{(i)}\) 得到重新调整，上述过程就开始重复了，直到没有更好的 \(c^{(i)}\) 指定。</p>

<p>这样从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量 \(c^{(i)}\)，M步更新其他参数 <code>u</code> 来使J最小化。这里的隐含类别变量指定方法比较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。总体思想还是一个迭代优化过程，有目标函数，也有参数变量，只是多了个隐含变量，确定其他参数估计隐含变量，再确定隐含变量估计其他参数，直至目标函数最优。</p>

<h2 id="toc_29">混合高斯模型 GMM</h2>

<p><img src="media/14522640452315/14522851410262.jpg" alt=""/></p>

<p>这个式子的最大值是不能通过前面使用的求导数为0的方法解决的，因为求的结果不是close form。但是假设我们知道了每个样例的 \(z^{(i)}\)，那么上式可以简化为：</p>

<p><img src="media/14522640452315/14522852444521.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522852533164.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522852586766.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522852657178.jpg" alt=""/></p>

<p>对比K-means可以发现，这里使用了“软”指定，为每个样例分配的类别 <code>z^{(i)}</code> 是有一定的概率的，同时计算量也变大了，每个样例i都要计算属于每一个类别j的概率。与K-means相同的是，结果仍然是局部最优解。对其他参数取不同的初始值进行多次计算不失为一种好方法。</p>

<p>EM 就是鸡生蛋蛋生鸡问题的一个解法，适合处理有隐变量的问题。</p>

<h2 id="toc_30">EM 算法</h2>

<p>给定的训练样本是 \(\{x^{(1)},\dots,x^{(m)}\}\)，样例间独立，我们想找到每个样例隐含的类别z，能使得p(x,z)最大。p(x,z)的最大似然估计如下：</p>

<p><img src="media/14522640452315/14522854715846.jpg" alt=""/></p>

<p>第一步是对极大似然取对数，第二步是对每个样例的每个可能类别z求联合分布概率和。但是直接求 \(\theta\) 一般比较困难，因为有隐藏变量 z 存在，但是一般确定了 z 后，求解就容易了。</p>

<p><img src="media/14522640452315/14522854939797.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522855108997.jpg" alt=""/></p>

<p>（1）到（2）比较直接，就是分子分母同乘以一个相等的函数。（2）到（3）利用了Jensen不等式，考虑到 log(x) 是凹函数（二阶导数小于0），而且</p>

<p><img src="media/14522640452315/14522855230444.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522855292211.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522855337248.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522855380219.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522855422700.jpg" alt=""/></p>

<p><img src="media/14522640452315/14522855500436.jpg" alt=""/></p>

<h3 id="toc_31">总结</h3>

<p>如果将样本看作观察值，潜在类别看作是隐藏变量，那么聚类问题也就是参数估计问题，只不过聚类问题中参数分为隐含类别变量和其他参数，这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，E步估计隐含变量，M步估计其他参数，交替将极值推向最大。EM中还有“硬”指定和“软”指定的概念，“软”指定看似更为合理，但计算量要大，“硬”指定在某些场合如K-means中更为实用（要是保持一个样本点到其他所有中心的概率，就会很麻烦）。</p>

<p>另外，EM的收敛性证明方法确实很牛，能够利用log的凹函数性质，还能够想到利用创造下界，拉平函数下界，优化下界的方法来逐步逼近极大值。而且每一步迭代都能保证是单调的。最重要的是证明的数学公式非常精妙，硬是分子分母都乘以z的概率变成期望来套上Jensen不等式，前人都是怎么想到的。</p>

<h2 id="toc_32">PCA 主成分分析</h2>

<ul>
<li>优点

<ul>
<li>降低数据的复杂性</li>
<li>识别最重要的多个特征。</li>
</ul></li>
<li>缺点

<ul>
<li>不一定需要</li>
<li>且可能损失有用信息。</li>
</ul></li>
<li>适用适用类型

<ul>
<li>数值型</li>
</ul></li>
<li>技术类型

<ul>
<li>降维技术</li>
</ul></li>
</ul>

<p>按照一定的数学变换方法，把给定的一组相关变量（维度）通过线性变换转成另一组不相关的变量，这些新的变量按照方差依次递减的顺序排列。在数学变换中保持变量的<code>总方差</code>不变，使<code>第一变量</code>具有<code>最大的方差</code>，称为<code>第一主成分</code>，<code>第二变量</code>的<code>方差次大</code>，并且和第一变量不相关，称为第二主成分。依次类推，I个变量就有I个主成分。</p>

<p><strong>通过低维表征的向量和特征向量矩阵，可以基本重构出所对应的原始高维向量</strong></p>

<h3 id="toc_33">PCA 计算过程</h3>

<p>首先介绍PCA的计算过程，假设我们得到的2维数据如下：</p>

<p><img src="media/14522640452315/14522859731850.jpg" alt=""/></p>

<p>行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，x是10篇文档中“learn”出现的TF-IDF，y是10篇文档中“study”出现的TF-IDF。也可以认为有10辆汽车，x是千米/小时的速度，y是英里/小时的速度，等等。</p>

<p>第一步分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到</p>

<p><img src="media/14522640452315/14522859868016.jpg" alt=""/></p>

<p>第二步，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是</p>

<p><img src="media/14522640452315/14522859978087.jpg" alt=""/></p>

<p>这里只有x和y，求解得</p>

<p><img src="media/14522640452315/14522860078418.jpg" alt=""/></p>

<p>对角线上分别是x和y的方差，非对角线上是协方差。协方差大于0表示x和y若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。</p>

<p>第三步，求协方差的特征值和特征向量，得到</p>

<p><img src="media/14522640452315/14522860244107.jpg" alt=""/></p>

<p>上面是两个特征值，下面是对应的特征向量，特征值 0.0490833989 对应特征向量为 \((-0.735178656, 0.677873399)^T\)，这里的特征向量都归一化为单位向量。</p>

<p>第四步，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的 k 个特征向量分别作为列向量组成特征向量矩阵。</p>

<p>这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是 \((-0.677873399, -0.735178656)^T\)</p>

<p>第五步，将样本点投影到选取的特征向量上。假设样例数为 m，特征数为 n，减去均值后的样本矩阵为 \(DataAdjust(m\times n)\)，协方差矩阵是 \(n\times n\)，选取的 k 个特征向量组成的矩阵为 \(EigenVectors(n\times k)\)。那么投影后的数据 FinalData 为</p>

<p>\[FinalData(m \times k) = DataAdjust(m \times n) \times EigenVectors(n \times k)\]</p>

<p>这里是</p>

<p>\[FinalData(10\times 1) = DataAdjust(10\times 2\;matrix) \times 特征向量(-0.677873399, -0.735178656)^T\]</p>

<p>得到结果是</p>

<p><img src="media/14522640452315/14522860485143.jpg" alt=""/></p>

<p>这样，就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影。</p>

<p>上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。上述过程有个图描述：</p>

<p><img src="media/14522640452315/14522861854664.jpg" alt=""/></p>

<p>正号表示预处理后的样本点，斜着的两条线就分别是正交的特征向量（由于协方差矩阵是对称的，因此其特征向量正交），最后一步的矩阵乘法就是将原始样本点分别往特征向量对应的轴上做投影。</p>

<p>如果取的k=2，那么结果是</p>

<p><img src="media/14522640452315/14522862019285.jpg" alt=""/></p>

<p>这就是经过PCA处理后的样本数据，水平轴（上面举例为LS特征）基本上可以代表全部样本点。整个过程看起来就像将坐标系做了旋转，当然二维可以图形化表示，高维就不行了。上面的如果k=1，那么只会留下这里的水平轴，轴上是所有点在该轴的投影。</p>

<p>这样PCA的过程基本结束。在第一步减均值之后，其实应该还有一步对特征做方差归一化。比如一个特征是汽车速度（0到100），一个是汽车的座位数（2到6），显然第二个的方差比第一个小。因此，如果样本特征中存在这种情况，那么在第一步之后，求每个特征的标准差 \(\sigma\)，然后对每个样例在该特征下的数据除以 \(\sigma\)。</p>

<p>归纳一下，使用我们之前熟悉的表示方法，在求协方差之前的步骤是：</p>

<p><img src="media/14522640452315/14522862442409.jpg" alt=""/></p>

<p>其中 \(x^{(i)}\) 是样例，共 m 个，每个样例 n 个特征，也就是说 \(x^{(i)}\) 是 n 维向量。\(x_j^{(i)}\)是第i个样例的第j个特征。\(\mu\) 是样例均值。\(\sigma_j\) 是第 j 个特征的标准差。</p>

<p>整个PCA过程貌似极其简单，就是求协方差的特征值和特征向量，然后做数据转换。但是有没有觉得很神奇，为什么求协方差的特征向量就是最理想的k维向量？其背后隐藏的意义是什么？整个PCA的意义是什么？</p>

<h3 id="toc_34">PCA 理论基础</h3>

<p>要解释为什么协方差矩阵的特征向量就是k维理想特征，我看到的有三个理论：分别是最大方差理论、最小错误理论和坐标轴相关度理论。这里简单探讨前两种，最后一种在讨论PCA意义时简单概述。</p>

<h4 id="toc_35">最大方差理论</h4>

<p>在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如前面的图，样本在横轴上的投影方差较大，在纵轴上的投影方差较小，那么认为纵轴上的投影是由噪声引起的。</p>

<p>因此我们认为，最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。</p>

<p>比如下图有5个样本点：（已经做过预处理，均值为0，特征方差归一）</p>

<p><img src="media/14522640452315/14522863534684.jpg" alt=""/></p>

<p>下面将样本投影到某一维上，这里用一条过原点的直线表示（前处理的过程实质是将原点移到样本点的中心点）。</p>

<p><img src="media/14522640452315/14522863615889.jpg" alt=""/></p>

<p>假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大。</p>

<p>这里先解释一下投影的概念：</p>

<p><img src="media/14522640452315/14522863722935.jpg" alt=""/></p>

<p>两个向量的点积（内积），等于一个向量在另一个向量上的投影长度，等于两个向量对应坐标分量之积的代数和。</p>

<p>从内积数值上我们可以看出两个向量的在方向上的接近程度。当内积值为正值时，两个向量大致指向相同的方向（方向夹角小于90度）；当内积值为负值时，两个向量大致指向相反的方向（方向角大于90度）；当内积值为0时，两个向量互相垂直</p>

<p>红色点表示样例 \(x^{(i)}\)，蓝色点表示 \(x^{(i)}\) 在 u 上的投影，u 是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是 \(x^{(i)}\) 在 u 上的投影点，离原点的距离是 \(\langle x^{(i)}, u\rangle\) （即 \((x^{(i)})^T u\) 或者 \(u^T x^{(i)}\)）由于这些样本点（样例）的每一维特征均值都为 0，因此投影到 u 上的样本点（只有一个到原点的距离值）的均值仍然是0。</p>

<p>回到上面左右图中的左图，我们要求的是最佳的u，使得投影后的样本点方差最大。</p>

<p>由于投影后均值为0，因此方差为：</p>

<p><img src="media/14522640452315/14522865206635.jpg" alt=""/></p>

<p>中间那部分很熟悉啊，不就是样本特征的协方差矩阵么（\(x^{(i)}\)的均值为0，一般协方差矩阵都除以m-1，这里用m）。</p>

<p><img src="media/14522640452315/14522865558036.jpg" alt=""/></p>

<p>因此，我们只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，样例 <code>x^(i)</code> 通过以下变换可以得到新的样本。</p>

<p><img src="media/14522640452315/14522865679052.jpg" alt=""/></p>

<p>其中的第j维就是 \(x^{(i)}\) 在 \(u_j\)上的投影。</p>

<p>通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。</p>

<p>这是其中一种对PCA的解释，第二种是<a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020216.html">错误最小化</a>。</p>

<h3 id="toc_36">总结与讨论</h3>

<p>PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>

<p>PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>

<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>

<h2 id="toc_37">SVD(singular value decomposition) 奇异值分解</h2>

<ul>
<li>优点

<ul>
<li>简化数据</li>
<li>去除噪声</li>
<li>提高算法的结果。</li>
</ul></li>
<li>缺点

<ul>
<li>数据转换可能难以理解。</li>
</ul></li>
<li>适用数据类型

<ul>
<li>数值型</li>
</ul></li>
<li>SVD是矩阵分解的一种类型。</li>
</ul>

<p>总结：SVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留矩阵80%~90%的能量，就可以得到重要的特征并去掉噪声。SVD已经运用到多个应用中，其中一个成功的应用案例就是推荐引擎。推荐引擎将物品推荐给用户，协同过滤则是一种基于用户喜好和行为数据的推荐和实现方法。协同过滤的核心是相似度计算方法，有很多相似度计算方法都可以用于计算物品或用户之间的相似度。通过在低维空间下计算相似度，SVD提高了推荐引擎的效果。</p>

<p>SVD将矩阵分解为三个矩阵的乘积，公式如下所示：</p>

<p>\[Data_{m\times n}=U_{m\times m}\Sigma_{m\times n}V_{n\times n}^T\]</p>

<p>中间的矩阵sigma为对角矩阵，对角元素的值为Data矩阵的奇异值(注意奇异值和特征值是不同的)，且已经从大到小排列好了。即使去掉特征值小的那些特征，依然可以很好的重构出原始矩阵。如下图所示：</p>

<p><img src="media/14520495721664/14523464830696.jpg" alt=""/></p>

<p>其中更深的颜色代表去掉小特征值重构时的三个矩阵。</p>

<p>如果 m 代表商品的个数，n 代表用户的个数，则 U 矩阵的每一行代表商品的属性，现在通过降维 U 矩阵（取深色部分）后，每一个商品的属性可以用更低的维度表示（假设为 k 维）。这样当新来一个用户的商品推荐向量 X，则可以根据公式 \(X&#39;U_1\Sigma_1^{-1}\) 得到一个 k 维的向量，然后在 V’ 中寻找最相似的那一个用户（相似度测量可用余弦公式等），根据这个用户的评分来推荐（主要是推荐新用户未打分的那些商品）。</p>

<p>SVD++ 可以说是SVD模型的加强版，除了打分关系，SVD++还可以对隐含的回馈(implicit feedback) 进行建模。</p>

<p>这种隐含的回馈可以是打分动作（谁对某个商品打过分），或者是浏览记录等。 只要有类似的隐含回馈，客观上也表示了user对某个item的偏好。 毕竟，user不会无缘无故地浏览一个item，肯定有什么原因， 比如user喜欢紫色，恰恰这个item也是紫色的，那通过隐含回馈就可以对user对紫色的偏好建模出来。</p>

<p>现实中，隐含回馈的原因比较复杂，专门给一部分参数空间去建模，肯定对用户的建模有一些帮助。</p>

<p>除了在SVD中定义的向量外，每个item对应一个向量 yi ，来通过user隐含回馈过的item的集合来刻画用户的偏好。</p>

<h2 id="toc_38">特征工程</h2>

<p>特征工程大概包括两个部分：特征提取和特征选择。</p>

<p>一般来说，领域内的知识主要是应用在特征提取。举个例子的话，比如说基于内容的购物推荐，性别就是一个很重要的领域知识，男性和女性关注的物品差别就比较大，推荐也应该体现出这种差别，那么这个特征就是这个问题一个重要特征，应该重点从已知数据提取，甚至专门为它再构建一个机器学习问题。</p>

<p>一般简单且常见的都是卡方检验，互信息和信息增益这三种</p>

<p>关于特征工程（Feature Engineering），已经是很古老很常见的话题了，坊间常说：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。纵观Kaggle、KDD等国内外大大小小的比赛，每个竞赛的冠军其实并没有用到很高深的算法，大多数都是在特征工程这个环节做出了出色的工作，然后使用一些常见的算法，比如LR，就能得到出色的性能。遗憾的是，在很多的书籍中并没有直接提到Feature Engineering，更多的是Feature selection。这也并不，很多ML书籍都是以讲解算法为主，他们的目的是从理论到实践来理解算法，所以用到的数据要么是使用代码生成的，要么是已经处理好的数据，并没有提到特征工程。在这篇文章，我打算自我总结下特征工程，让自己对特征工程有个全面的认识。在这我要说明一下，我并不是说那些书写的不好，其实都很有不错，主要是因为它们的目的是理解算法，所以直接给出数据相对而言对于学习和理解算法效果更佳。</p>

<p>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</p>

<p>简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。</p>

<p>（1）特征越好，灵活性越强</p>

<p>只要特征选得好，即使是一般的模型（或算法）也能获得很好的性能，因为大多数模型（或算法）在好的数据特征下表现的性能都还不错。好特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易理解和维护。</p>

<p>（2）特征越好，构建的模型越简单</p>

<p>有了好的特征，即便你的参数不是最优的，你的模型性能也能仍然会表现的很nice，所以你就不需要花太多的时间去寻找最有参数，这大大的降低了模型的复杂度，使模型趋于简单。</p>

<p>（3）特征越好，模型的性能越出色</p>

<p>显然，这一点是毫无争议的，我们进行特征工程的最终目的就是提升模型的性能。</p>

<h3 id="toc_39">特征选择 Feature Selection</h3>

<p>首先，从特征开始说起，假设你现在有一个标准的Excel表格数据，它的每一行表示的是一个观测样本数据，表格数据中的每一列就是一个特征。在这些特征中，有的特征携带的信息量丰富，有的（或许很少）则属于无关数据（irrelevant data），我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量。比如，在实际应用中，常用的方法就是使用一些评价指标单独地计算出单个特征跟类别变量之间的关系。如Pearson相关系数，Gini-index（基尼指数），IG（信息增益）等，下面举Pearson指数为例，它的计算方式如下：</p>

<p>\[r_{xy}^2=(\frac{con(x,y)}{\sqrt{var(x)var(y)}})\]</p>

<p>其中，x属于X，X表一个特征的多个观测值，y表示这个特征观测值对应的类别列表。</p>

<p>Pearson相关系数的取值在0到1之间，如果你使用这个评价指标来计算所有特征和类别标号的相关性，那么得到这些相关性之后，你可以将它们从高到低进行排名，然后选择一个子集作为特征子集（比如top 10%），接着用这些特征进行训练，看看性能如何。此外，你还可以画出不同子集的一个精度图，根据绘制的图形来找出性能最好的一组特征。</p>

<p>这就是特征工程的子问题之一——特征选择，它的目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。</p>

<p>做特征选择的原因是因为这些特征对于目标类别的作用并不是相等的，一些无关的数据需要删掉。做特征选择的方法有多种，上面提到的这种特征子集选择的方法属于filter（刷选器）方法，它主要侧重于单个特征跟目标变量的相关性。优点是计算时间上较高效,对于过拟合问题也具有较高的鲁棒性。缺点就是倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果。另外做特征子集选取的方法还有wrapper（封装器）和Embeded(集成方法)。wrapper方法实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准,经过比较选出最好的特征子集。常用的有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）。它的优点是考虑了特征与特征之间的关联性，缺点是：当观测数据较少时容易过拟合，而当特征数量较多时,计算时间又会增长。对于Embeded集成方法，它是学习器自身自主选择特征，如使用Regularization做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用Random Forest和Gradient boosting做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。</p>

<h3 id="toc_40">特征提取</h3>

<p>原则上来讲，特征提取应该在特征选择之前。特征提取的对象是原始数据（raw data），它的目的是自动地构建新的特征，将原始特征转换为一组具有明显物理意义（Gabor、几何特征[角点、不变量]、纹理[LBP HOG]）或者统计意义或核的特征。比如通过变换特征取值来减少原始数据中某个特征的取值个数等。对于表格数据，你可以在你设计的特征矩阵上使用主要成分分析（Principal Component Analysis，PCA)来进行特征提取从而创建新的特征。对于图像数据，可能还包括了线或边缘检测。</p>

<p>常用的方法有：</p>

<ul>
<li>PCA (Principal component analysis，主成分分析)</li>
<li>ICA (Independent component analysis，独立成分分析)</li>
<li>LDA （Linear Discriminant Analysis，线性判别分析）</li>
</ul>

<p>对于图像识别中，还有SIFT方法。</p>

<p>用中文来说就是：特征工程是一个超集，它包括特征提取、特征构建和特征选择这三个子模块。在实践当中，每一个子模块都非常重要，忽略不得。根据答主的经验，他将这三个子模块的重要性进行了一个排名，即：特征构建&gt;特征提取&gt;特征选择。</p>

<p>事实上，真的是这样，如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能。</p>

<h2 id="toc_41">深度学习</h2>

<p>我们先说什么是深度学习。其实从整体上来讲，Deep Learning就是曾经的多层神经网络，整体的思想认为每一个层次都可以被作为一个独立的特征抽象存在，所以最广泛地被用作特征工程上，而GPU的存在更是解决了几十年前的ANN的训练效率问题。那么简单来说，Deep Learning可以对抽取出的特征进行非线性组合形成更有效的特征表示。确实，从这一点来说，Deep Learning确实从理论上很好的解决了机器学习领域很麻烦的“特征抽取”问题，但是在实际的工业界，“特征工程”到底有多复杂？我们看看Deep Learning表现最好的IR领域吧，曾经是怎么做的呢？据了解微软有个小Team专门做的事儿就是从图片上找各种各样的特征，因为算法本身其实已经被锁死在Random Forest上了，往往特征的微调就能带来算法效果的极大提升，那么Deep Learning的出现当然可以很好地取代这项工作(实际效果确实无法得知)，那么总结下Deep Learning的好处：从海量的特征中通过特征工程抽取出有效的特征组合。</p>

<p>但是刨除掉语音和图像领域，转向离我们更近的工作，无论是推荐系统还是数据挖掘，特征是怎么出来的呢？对于一个电影，对于一个用户，满打满算一共就那么多特征，这个时候Deep Learning根本无从发挥。那么再退一步说，就算把User对于Item的标定作为Item的特征，由于在实际中大部分的缺失值存在，那么如果你希望用Deep Learning来对该矩阵做特征重组，第一件事情就是如何填充缺失值，而这恰恰是比特征工程更困难的事情。</p>

<h2 id="toc_42">大数据</h2>

<p>大数据其实意味着大样本量，那么大样本量带来的是高置信度以及广覆盖度。例如从FM来说，大数据量意味着更全面地了解一个用户的听歌品位，从金融互联网的信用风险评估来说，大数据量意味着不仅仅从消费记录而包含了社交网络信息去对用户做更全面的评价，从用户画像来说意味着建立全面的兴趣图谱和知识图谱，这些都是大数据带给我们的实际意义。说得学术一些，我们不妨认为大数据是频率学派对于贝叶斯学派一次强有力的逆袭。那么既然说到这个份上了，我们不妨思考一下，我们是不是有希望在回归贝叶斯学派，利用先验信息+小数据完成对大数据的反击呢？</p>

<p>另外，既然我们已经说到了大数据的广覆盖度，就针对这个再额外说一下吧。诚然，大数据能够全面地覆盖到所有信息，但是从实际的工业界来看，考虑到实际的计算能力以及效果，大多数公司都会对大数据做“去噪”，那么在去噪的过程中去除的不仅仅是噪音，也包括“异常点”，而这些“异常点”，恰恰把大数据的广覆盖度给降低了，于是利用大数据反而比小数据更容易产生趋同的现象。尤其对于推荐系统来说，这些“异常点”的观察其实才是“个性化”的极致。</p>

<h2 id="toc_43">用户画像</h2>

<p>任何系统不要脱离产品而存在。先吐个槽，之前在某个公司面试，某个公司上来就问我，你觉得我们的用户画像应该怎么做？这个问题是非常业余的(这个问题就像是有人问我我们网站有性能问题，你说咋办；好吧，这个问题也是这个公司问我的)，任何数据系统都是强产品关联的，这也是太多公司去做数据系统的误区，在这里我还是用户画像为例。 用户画像到底是什么，其实说简单了他就是一个用户宽表，如果偏要我说需要注意的，就是在选择数据库的时候一定要选择列容易扩充的数据库。如果要说具体需要哪些字段，我还真的没法说，我只能把他归类成用户元属性数据，行为统计数据，潜在挖掘数据，至此而已。因为数据系统从来不是一个事先规划好的系统，而是需要随着业务增长来逐渐填充的系统，这也是数据平台难做的原因。 所以我真心无法理解有一些不太大的公司成立了一个部门，这个部门专门做用户画像(例如PPTV)。</p>

<h2 id="toc_44">实体识别</h2>

<p>命名实体识别(Named EntitiesRecognition, NER)是自然语言处理(Natural LanguageProcessing, NLP)的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而,通常把对这些词的识别从词汇形态处理(如汉语切分)任务中独立处理，称为命名实体识别。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。</p>

<p>命名实体是命名实体识别的研究主体，一般包括3大类(实体类、时间类和数字类)和7小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确；实体的类型是否标注正确。主要错误类型包括文本正确，类型可能错误；反之，文本边界错误,而其包含的主要实体词和词类标记可能正确。</p>

<p>命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。</p>

<p>1.基于规则和词典的方法</p>

<p>基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p>

<p>2.基于统计的方法</p>

<p>基于统计机器学习的方法主要包括：隐马尔可夫模型(HiddenMarkovMode,HMM)、最大熵(MaxmiumEntropy,ME)、支持向量机(Support VectorMachine,SVM)、条件随机场( ConditionalRandom Fields,CRF)等。</p>

<p>在这4种学习方法中，最大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间复杂性非常高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，最大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，但是隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用Viterbi算法求解命名实体类别序列的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用,如短文本命名实体识别。</p>

<p>基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。</p>

<p>基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。</p>

<p>3.混合方法</p>

<p>自然语言处理并不完全是一个随机过程,单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：</p>

<ol>
<li>统计学习方法之间或内部层叠融合。</li>
<li>规则、词典和机器学习方法之间的融合，其核心是融合方法技术。在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。</li>
<li>将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。</li>
</ol>

<p>这种方法在具体实现过程中需要考虑怎样高效地将两种方法结合起来，采用什么样的融合技术。由于命名实体识别在很大程度上依赖于分类技术,在分类方面可以采用的融合技术主要包括如Voting, XVoting,GradingVa,l Grading等。</p>

<h2 id="toc_45">正则化</h2>

<p>正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。</p>

<p>奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。</p>

<p>作用是：</p>

<ol>
<li>数值上更容易求解；</li>
<li>特征数目太大时更稳定；</li>
<li>控制模型的复杂度，光滑性。复杂性越小且越光滑的目标函数泛化能力越强。而加入规则项能使目标函数复杂度减小，且更光滑。</li>
<li>减小参数空间；参数空间越小，复杂度越低。</li>
<li>系数越小，模型越简单，而模型越简单则泛化能力越强（Ng宏观上给出的解释）。</li>
<li>可以看成是权值的高斯先验。</li>
</ol>

<h3 id="toc_46">L1和L2正则的区别，如何选择L1和L2正则</h3>

<blockquote>
<p>他们都是可以防止过拟合，降低模型复杂度</p>
</blockquote>

<ul>
<li>L1 是在 loss function 后面加上模型参数的1范数（也就是|xi|）</li>
<li>L2 是在 loss function 后面加上模型参数的2范数（也就是sigma(xi<sup>2)），注意L2范数的定义是sqrt(sigma(xi<sup>2))，在正则项上没有添加sqrt根号是为了更加容易优化</sup></sup></li>
<li>L1 会产生稀疏的特征</li>
<li>L2 会产生更多地特征但是都会接近于0</li>
</ul>

<p>L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L1在特征选择时候非常有用，而L2就只是一种规则化而已。</p>

<h2 id="toc_47">过拟合</h2>

<p>一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。</p>

<p>过拟合产生的原因：</p>

<p>出现这种现象的主要原因是训练数据中存在噪音或者训练数据太少。</p>

<p>预防或克服措施：</p>

<ol>
<li>增大数据量</li>
<li>减少feature个数（人工定义留多少个feature或者算法选取这些feature）</li>
<li>正则化（留下所有的feature，但对于部分feature定义其parameter非常小）</li>
<li>交叉验证</li>
</ol>

<h2 id="toc_48">对偶问题</h2>

<p>对偶性是优化问题中一个非常重要的性质，它能够神奇地将许多非凸的优化问题转化成凸的问题</p>

<p>很多凸优化问题都是通过解对偶问题来求解的，线性规划只是其中一个特例而已。在求解一个规划问题（不限于线性规划）的时候，我们常常需要知道这个问题有没有可行解（有时候约束条件很复杂，不要说最优解，找到可行解都很难），或者是估计一下目前的解离最优解还有多远（大型问题多用迭代解法，如果能大致估计当前的解的质量，就对算法什么时候运行结束有一个大致的把握，如果得到了可接受的近似解也可以提前停止），以及判断原问题的最优解是否无界（万一出现这种情况迭代就停不下来了）。</p>

<p>而对偶问题就是回答这些问题的利器：弱对偶定理给原问题的最优解定了一个界，强对偶定理给出了原问题最优解的一个判定条件。同时，还有很多别的优良性质：例如可以化难为易（把难以求解的约束条件扔到目标函数的位置上去），如果问题的形式合适还可以通过把约束变量和对偶变量互换来把大规模问题转换成小规模问题。线性规划的对偶问题也只是其中的一个特例而已。并且，假如原问题是可行的，还可以给对偶问题找到一个直观解释（经济学中的影子价格）。</p>

<p>举个例子：</p>

<p>\[min_{x,y} \; px+qy\;\; s.t. x+y \ge 2,x \ge 0,y \ge 0\]</p>

<p>我们令 \(a+b = p, a+c=q\) 于是 \(px+qy=a(x+y)+bx+cy \ge 2a,a \ge 0, b\ge0, c\ge0\)</p>

<p>2a 为元问题的下界，于是我们可以求下面这个问题：</p>

<p>\[max_{a,b,c}\;2a\;\; s.t.\;a+b=p,a+c=q,a\ge0,b\ge0,c\ge0\]</p>

<p>后一个问题的解正是前一个问题的解，这后面那个问题就叫做原问题的对偶问题。</p>

<p>来看更一般的约束优化问题</p>

<p><img src="media/14522640452315/14524679565140.jpg" alt=""/></p>

<p>我们定义一个在初等微积分中便已经听过的函数，拉格朗日函数</p>

<p><img src="media/14522640452315/14524679773223.jpg" alt=""/></p>

<p>同样我们容易发现 \(f(x)≥L(x,u,v)\)</p>

<p><img src="media/14522640452315/14524680125885.jpg" alt=""/></p>

<p>这个性质被称作弱对偶性（weak duality）。</p>

<p>理所当然，强对偶性（strong duality）是指 \(f^∗=g^∗\)的情形了。</p>

<p>有个叫做Slater’s condition的东西，它告诉我们什么时候强对偶性成立，不过我们暂时不关心它，因为在实际中，即便是只有弱对偶性，往往对偶问题的解也已经满足要求了。 还有一个重要的性质，那就是对偶问题总是凸的，这可是解决优化问题的研究者们最大的福音了，因为对于凸的问题，军火库里有着一堆不同规格的武器，而对于非凸的，人们往往绞尽脑汁试图将其转化为凸优化问题。</p>

<p><img src="media/14522640452315/14524686522243.jpg" alt=""/></p>

<p>好了，接下来我们来看KKT。</p>

<h3 id="toc_49">KKT</h3>

<p>令xx表示原问题可行域中的解，u,vu,v是对偶问题可行解，我们定义duality gap 为，</p>

<p>\[f(x)-g(u,v)\]</p>

<p>显然，我们有</p>

<p>\[f(x)-f^*\le f(x)-g(u,v)\]</p>

<p>上面这个式子，在算法的角度可是相当重要的，不过那是后话了。</p>

<p>对于一般性的约束优化问题</p>

<p><img src="media/14522640452315/14524702561821.jpg" alt=""/></p>

<p>KKT 条件是</p>

<p><img src="media/14522640452315/14524702711268.jpg" alt=""/></p>

<p>这四个条件分别被称作stationarity,complementary slackness,primal feasibility,dual feasibility。</p>

<p>可以证明KKT条件是原问题解和对偶问题解存在且gap为0的充分条件，在强对偶性下，也是必要条件。</p>

<p>好了，现在我们看到了KKT条件的面目，但是仔细琢磨，就会发现这些条件，不过是前面推导Lagrange对偶问题过程中，理所当然的一些东西罢了。</p>

<p>首先stationarity（平稳性）不过是Lagrange函数求极值得到Lagrange对偶函数的过程中，所需的步骤罢了。</p>

<p>第二个条件需要稍微推导一下（利用gap为0），不过也是很simple的，窃以为这个条件才是最关键的东西。后两个条件则更trivial。</p>

<p>有了前一部分的东西，KKT条件的出现也应该是理所当然的。然而数学总是这样，一些很简单的东西，往往出现之前就是巨大的困难。</p>

<h2 id="toc_50">梯度下降</h2>

<p>在应用机器学习算法时，我们通常采用梯度下降法来对采用的算法进行训练。其实，常用的梯度下降法还具体包含有三种不同的形式，它们也各自有着不同的优缺点。</p>

<p>一般线性回归函数的假设函数为：</p>

<p>\[h_\theta=\sum_{j=0}^n\theta_jx_j\]</p>

<p>对应的能量函数（损失函数）形式为：</p>

<p>\[J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\]</p>

<p>下图为以 \(\theta_0,\theta_1\) 为二维参数组对应能量函数的可视化</p>

<p><img src="media/14522640452315/14524327387355.jpg" alt=""/></p>

<h3 id="toc_51">牛顿法</h3>

<p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。</p>

<p>最初用于求解方程根 \(f(x)=0\)</p>

<p>首先得到一个初始解 \(x_0\)，一阶展开 \(f(x) \approx f(x_0)+(x-x_0)f&#39;(x_0)\)，令 \( f(x_0)+(x-x_0)f&#39;(x_0) = 0\) 求解得到 x，相比与 \(x_0\)，\(f(x) &lt; f(x_0)\)</p>

<p><img src="media/14522640452315/14524319966945.jpg" alt=""/></p>

<p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是<q>切线法</q>。牛顿法的搜索路径（二维情况）如下图所示：</p>

<p><img src="media/14522640452315/14524617674915.jpg" alt=""/></p>

<p>最优化问题中，牛顿法首先则是将问题转化为求 \(f&#39;(x) = 0\) 这个方程的根。</p>

<p>首先得到一个初始解 \(x_0\)，</p>

<p>一阶展开：\(f&#39;(x) \approx f&#39;(x_0)+(x-x_0)f&#39;&#39;(x_0)\)</p>

<p>令 \(f&#39;(x_0)+(x-x_0)f&#39;&#39;(x_0) = 0 \)</p>

<p>求解得到 x，相比于 \(x_0\)，\(f&#39;(x) &lt; f&#39;(x_0)\)</p>

<p>关于牛顿法和梯度下降法的效率对比：</p>

<p>从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）</p>

<p>根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</p>

<p>牛顿法的优缺点总结：</p>

<ul>
<li>优点：二阶收敛，收敛速度快；</li>
<li>缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</li>
</ul>

<h3 id="toc_52">拟牛顿法（Quasi-Newton Methods）</h3>

<p>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p>

<p>具体的数学不深究，大概知道思路 </p>

<h3 id="toc_53">共轭梯度法（Conjugate Gradient）</h3>

<p>共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p>

<p>具体不深究，下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：</p>

<p><img src="media/14522640452315/14524627687371.jpg" alt=""/></p>

<p>绿色为梯度下降法，红色代表共轭梯度法</p>

<h3 id="toc_54">批量梯度下降法BGD</h3>

<p>批量梯度下降法（Batch Gradient Descent，简称BGD）是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新，其数学形式如下：</p>

<p>(1) 对上述的能量函数求偏导：</p>

<p><img src="media/14522640452315/14524340144279.jpg" alt=""/></p>

<p>(2) 由于是最小化风险函数，所以按照每个参数θ的梯度负方向来更新每个θ：</p>

<p><img src="media/14522640452315/14524340290928.jpg" alt=""/></p>

<p>具体的伪代码形式为：</p>

<p><img src="media/14522640452315/14524342976589.jpg" alt=""/></p>

<p>从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果样本数目m很大，那么可想而知这种方法的迭代速度！所以，这就引入了另外一种方法，随机梯度下降。</p>

<ul>
<li>优点：全局最优解；易于并行实现；</li>
<li>缺点：当样本数目很多时，训练过程会很慢。</li>
</ul>

<p>从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：</p>

<p><img src="media/14522640452315/14524343213074.jpg" alt=""/></p>

<h3 id="toc_55">随机梯度下降法SGD</h3>

<p>由于批量梯度下降法在更新每一个参数时，都需要所有的训练样本，所以训练过程会随着样本数量的加大而变得异常的缓慢。随机梯度下降法（Stochastic Gradient Descent，简称SGD）正是为了解决批量梯度下降法这一弊端而提出的。</p>

<p>将上面的能量函数写为如下形式：</p>

<p><img src="media/14522640452315/14524343421086.jpg" alt=""/></p>

<p>利用每个样本的损失函数对θ求偏导得到对应的梯度，来更新θ：</p>

<p><img src="media/14522640452315/14524343544199.jpg" alt=""/></p>

<p>具体的伪代码形式为：</p>

<p><img src="media/14522640452315/14524343708863.jpg" alt=""/></p>

<p>随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p>

<ul>
<li>优点：训练速度快；</li>
<li>缺点：准确度下降，并不是全局最优；不易于并行实现。</li>
</ul>

<p>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：</p>

<p><img src="media/14522640452315/14524343949030.jpg" alt=""/></p>

<h3 id="toc_56">小批量梯度下降法MBGD</h3>

<p>有上述的两种梯度下降法可以看出，其各自均有优缺点，那么能不能在两种方法的性能之间取得一个折衷呢？即，算法的训练过程比较快，而且也要保证最终参数训练的准确率，而这正是小批量梯度下降法（Mini-batch Gradient Descent，简称MBGD）的初衷。</p>

<p>MBGD在每次更新参数时使用b个样本（b一般为10），其具体的伪代码形式为</p>

<p><img src="media/14522640452315/14524346269622.jpg" alt=""/></p>

<h2 id="toc_57">凸优化</h2>

<p>在机器学习中往往是最终要求解某个函数的最优值，但是一般情况下，任意一个函数的最优值求解比较困难，但是对于凸函数来说就可以有效的求解出全局最优值。</p>

<p><strong>凸集</strong></p>

<p>一个集合C是，当前仅当任意x,y属于C且0&lt;=theta&lt;=1，都有 \(\theta x+(1-\theta)y\) 属于C</p>

<p>用通俗的话来说C集合线段上的任意两点也在C集合中</p>

<p><strong>凸函数</strong></p>

<p>一个函数f其定义域(D(f))是凸集，并且对任意x,y属于D(f)和0&lt;=theta&lt;=1都有<br/>
\(f(\theta x+(1-\theta)y)&lt;=\theta f(x)+(1-\theta)f(y)\) —这个叫做jensen不等式</p>

<p>用通俗的话来说就是曲线上任意两点的割线都在曲线的上方</p>

<p><strong>常见的凸函数有</strong></p>

<ul>
<li>指数函数 \(f(x)=a^x\) a&gt;1</li>
<li>负对数函数 \(-log_a x\) a&gt;1,x&gt;0</li>
<li>开口向上的二次函数等</li>
</ul>

<p><strong>凸函数的判定</strong></p>

<ol>
<li>如果f是一阶可导，对于任意数据域内的x,y满足 f(y)&gt;=f(x)+f’(x)(y-x)</li>
<li>如果f是二阶可导</li>
</ol>

<p><strong>凸优化应用举例</strong></p>

<ul>
<li>SVM：其中由 max|w| 转向 <code>min(1/2*|w|^2)</code></li>
<li>最小二乘法？</li>
<li>LR的损失函数 <code>sigma(yi*log(hw(x))+(1-yi)*(log(1-hw(x))))</code></li>
</ul>

<h2 id="toc_58">特征向量</h2>

<h3 id="toc_59">归一化方法</h3>

<ol>
<li>线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)</li>
<li>对数函数转换，表达式如下：y=log10 (x)</li>
<li>反余切函数转换 ，表达式如下：y=arctan(x)*2/PI</li>
<li>减去均值，乘以方差：y=(x-means)/ variance</li>
</ol>

<h3 id="toc_60">异常值处理</h3>

<p>用均值或者其他统计量代替</p>

<h2 id="toc_61">ROC、AUC</h2>

<p>ROC和AUC通常是用来评价一个二值分类器的好坏</p>

<h3 id="toc_62">ROC</h3>

<p>曲线坐标上：</p>

<ul>
<li>X轴是FPR（表示假阳率-预测结果为positive，但是实际结果为negitive，FP/(N)）</li>
<li>Y轴式TPR（表示真阳率-预测结果为positive，而且的确真实结果也为positive的,TP/P）</li>
</ul>

<p>那么平面的上点(X,Y)：</p>

<ul>
<li>(0,1)表示所有的positive的样本都预测出来了，分类效果最好</li>
<li>(0,0)表示预测的结果全部为negitive</li>
<li>(1,0)表示预测的错过全部分错了，分类效果最差</li>
<li>(1,1)表示预测的结果全部为positive</li>
</ul>

<p>针对落在x=y上点，表示是采用随机猜测出来的结果</p>

<h3 id="toc_63">ROC曲线建立</h3>

<p>一般默认预测完成之后会有一个概率输出p，这个概率越高，表示它对positive的概率越大。</p>

<p>现在假设我们有一个threshold，如果p&gt;threshold，那么该预测结果为positive，否则为negitive，按照这个思路，我们多设置几个threshold,那么我们就可以得到多组positive和negitive的结果了，也就是我们可以得到多组FPR和TPR值了<br/>
将这些(FPR,TPR)点投射到坐标上再用线连接起来就是ROC曲线了<br/>
当threshold取1和0时，分别得到的就是(0,0)和(1,1)这两个点。（threshold=1，预测的样本全部为负样本，threshold=0，预测的样本全部为正样本）</p>

<h3 id="toc_64">AUC</h3>

<p>AUC(Area Under Curve)被定义为ROC曲线下的面积，显然这个面积不会大于1（一般情况下ROC会在x=y的上方，所以0.5&lt;AUC&lt;1）.</p>

<p>AUC越大说明分类效果越好</p>

<h3 id="toc_65">为什么要使用ROC和AUC</h3>

<p>因为当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动。</p>

<h2 id="toc_66">10折交叉验证</h2>

<p>英文名是10-fold cross-validation，用来测试算法的准确性。是常用的测试方法。将数据集分成10份。轮流将其中的9份作为训练数据，1分作为测试数据，进行试验。每次试验都会得出相应的正确率（或差错率）。10次的结果的正确率（或差错率）的平均值作为对算法精度的估计，一般还需要进行多次10折交叉验证，在求其平均值，对算法的准确性进行估计。</p>

<p>在训练数据上面，我们可以进行交叉验证(Cross-Validation)。</p>

<p>一种方法叫做K-fold Cross Validation (K折交叉验证), K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。</p>

<p><img src="media/14522640452315/14522703150352.jpg" alt=""/></p>

<ul>
<li>当K值大的时候， 我们会有更少的Bias(偏差), 更多的Variance。</li>
<li>当K值小的时候， 我们会有更多的Bias(偏差), 更少的Variance。</li>
</ul>

<p>Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</p>

<p>在一个实际系统中，Bias与Variance往往是不能兼得的。如果要降低模型的Bias，就一定程度上会提高模型的Variance，反之亦然。造成这种现象的根本原因是，我们总是希望试图用有限训练样本去估计无限的真实数据。当我们更加相信这些数据的真实性，而忽视对模型的先验知识，就会尽量保证模型在训练样本上的准确度，这样可以减少模型的Bias。但是，这样学习到的模型，很可能会失去一定的泛化能力，从而造成过拟合，降低模型在真实数据上的表现，增加模型的不确定性。相反，如果更加相信我们对于模型的先验知识，在学习模型的过程中对模型增加更多的限制，就可以降低模型的variance，提高模型的稳定性，但也会使模型的Bias增大。Bias与Variance两者之间的trade-off是机器学习的基本主题之一，机会可以在各种机器模型中发现它的影子。</p>

<p>具体到K-fold Cross Validation的场景，其实是很好的理解的。首先看Variance的变化，还是举打靶的例子。假设我把抢瞄准在10环，虽然每一次射击都有偏差，但是这个偏差的方向是随机的，也就是有可能向上，也有可能向下。那么试验次数越多，应该上下的次数越接近，那么我们把所有射击的目标取一个平均值，也应该离中心更加接近。更加微观的分析，模型的预测值与期望产生较大偏差，在模型固定的情况下，原因还是出在数据上，比如说产生了某一些异常点。在最极端情况下，我们假设只有一个点是异常的，如果只训练一个模型，那么这个点会对整个模型带来影响，使得学习出的模型具有很大的variance。但是如果采用k-fold Cross Validation进行训练，只有1个模型会受到这个异常数据的影响，而其余k-1个模型都是正常的。在平均之后，这个异常数据的影响就大大减少了。相比之下，模型的bias是可以直接建模的，只需要保证模型在训练样本上训练误差最小就可以保证bias比较小，而要达到这个目的，就必须是用所有数据一起训练，才能达到模型的最优解。因此，k-fold Cross Validation的目标函数破坏了前面的情形，所以模型的Bias必然要会增大。</p>

<h2 id="toc_67">极大似然估计</h2>

<p>极大似然估计，只是一种概率论在统计学中的应用，它是参数评估的方法之一。说的 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计通过若干次实验，观察其结果，利用结果推出参数的大概值。极大似然估计是建立在这样的思想上的：已知某个参数能使这个样本出现的概率最大。我们当然不会再去选择其他其他小概率的样本，所以干脆就把这个参数作为估计的真实值。</p>

<h2 id="toc_68">异常检测</h2>

<p>可以估计样本的密度函数，对于新样本直接计算其密度，如果密度值小于某一阈值，则表示该样本异常。而密度函数一般采用多维的高斯分布。如果样本有n维，则每一维的特征都可以看作是符合高斯分布的，即使这些特征可视化出来不太符合高斯分布，也可以对该特征进行数学转换让其看起来像高斯分布，比如说x=log(x+c), x=x<sup>1/c</sup>等。异常检测的算法流程如下</p>

<p><img src="media/14522640452315/14523461159926.jpg" alt=""/></p>

<p>其中的ε也是通过交叉验证得到的，也就是说在进行异常检测时，前面的p(x)的学习是用的无监督，后面的参数ε学习是用的有监督。那么为什么不全部使用普通有监督的方法来学习呢（即把它看做是一个普通的二分类问题）？主要是因为在异常检测中，异常的样本数量非常少而正常样本数量非常多，因此不足以学习到好的异常行为模型的参数，因为后面新来的异常样本可能完全是与训练样本中的模式不同。</p>

<p>另外，上面是将特征的每一维看成是相互独立的高斯分布，其实这样的近似并不是最好的，但是它的计算量较小，因此也常被使用。更好的方法应该是将特征拟合成多维高斯分布，这时有特征之间的相关性，但随之计算量会变复杂，且样本的协方差矩阵还可能出现不可逆的情况（主要在样本数比特征数小，或者样本特征维数之间有线性关系时）。</p>

<h2 id="toc_69">Apriori</h2>

<p>Apriori是关联分析中比较早的一种方法，主要用来挖掘那些频繁项集合。其思想是：</p>

<ol>
<li>如果一个项目集合不是频繁集合，那么任何包含它的项目集合也一定不是频繁集合；</li>
<li>如果一个项目集合是频繁集合，那么它的任何非空子集也是频繁集合；</li>
</ol>

<p>Aprioir需要扫描项目表多遍，从一个项目开始扫描，舍去掉那些不是频繁的项目，得到的集合称为L，然后对L中的每个元素进行自组合，生成比上次扫描多一个项目的集合，该集合称为C，接着又扫描去掉那些非频繁的项目，重复</p>

<p>看下面这个例子：</p>

<p>元素项目表格：</p>

<p><img src="media/14522640452315/14523461958019.jpg" alt=""/></p>

<p>如果每个步骤不去掉非频繁项目集，则其扫描过程的树形结构如下：</p>

<p><img src="media/14522640452315/14523462308911.jpg" alt=""/></p>

<p>在其中某个过程中，可能出现非频繁的项目集，将其去掉（用阴影表示）为：</p>

<p><img src="media/14522640452315/14523462438355.jpg" alt=""/></p>

<h2 id="toc_70">FP Growth</h2>

<p>FP Growth是一种比Apriori更高效的频繁项挖掘方法，它只需要扫描项目表2次。其中第1次扫描获得当个项目的频率，去掉不符合支持度要求的项，并对剩下的项排序。第2遍扫描是建立一颗FP-Tree(frequent-patten tree)。</p>

<p>接下来的工作就是在FP-Tree上进行挖掘。</p>

<p>比如说有下表：</p>

<p><img src="media/14522640452315/14523462727631.jpg" alt=""/></p>

<p>它所对应的FP_Tree如下</p>

<p><img src="media/14522640452315/14523462822854.jpg" alt=""/></p>

<p>然后从频率最小的单项P开始，找出P的条件模式基，用构造FP<u>Tree同样的方法来构造P的条件模式基的FP</u>Tree，在这棵树上找出包含P的频繁项集。</p>

<p>　　依次从m,b,a,c,f的条件模式基上挖掘频繁项集，有些项需要递归的去挖掘，比较麻烦，比如m节点，具体的过程可以参考博客：<a href="http://blog.sina.com.cn/s/blog_68ffc7a40100uebg.html">Frequent Pattern 挖掘之二(FP Growth算法)</a>，里面讲得很详细。</p>

<h2 id="toc_71">pLSA</h2>

<p>pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。pLSA的模型图如下</p>

<p><img src="media/14522640452315/14523465340281.jpg" alt=""/></p>

<p>公式中的意义如下</p>

<p><img src="media/14522640452315/14523465465335.jpg" alt=""/></p>

<h2 id="toc_72">LDA</h2>

<p>主题模型，概率图如下</p>

<p><img src="media/14522640452315/14523465552380.jpg" alt=""/></p>

<p>和pLSA不同的是LDA中假设了很多先验分布，且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同</p>

<p>狄利克雷分布是一组连续多变量概率分布，是多变量普遍化的Β分布。狄利克雷分布常作为贝叶斯统计的先验概率。当狄利克雷分布维度趋向无限时，便成为狄利克雷过程（Dirichlet process）。</p>

<p><img src="media/14522640452315/14523465670215.jpg" alt=""/></p>

<p>归一化衡量B(α)是多项Β函数，可以用Γ函数（gamma function）表示：</p>

<p><img src="media/14522640452315/14523465756711.jpg" alt=""/></p>

<h2 id="toc_73">熵</h2>

<p>在信息论中，熵表示的是不确定性的量度。信息论的创始人香农在其著作《通信的数学理论》中提出了建立在概率统计模型上的信息度量。他把信息定义为”用来消除不确定性的东西“。熵的定义为信息的期望值。</p>

<p>在信息论和概率统计中，熵是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：</p>

<p>\[P(X=x_i)=p_i, i=1,2, ... , n\]</p>

<p>则随机变量X的熵定义为：</p>

<p>\[H(X)= - \sum p_ilogp_i, i=1,2, ... , n\]</p>

<p>熵只依赖X的分布，和X的取值没有关系，熵是用来度量不确定性，当熵越大，概率说\(X=x_i\)的不确定性越大，反之越小，在机器学期中分类中说，熵越大即这个类别的不确定性更大，反之越小，当随机变量的取值为两个时，熵随概率的变化曲线如下图：</p>

<p><img src="media/14522640452315/14524357333474.jpg" alt=""/></p>

<p>当p=0或p=1时，H(p)=0,随机变量完全没有不确定性，当p=0.5时，H(p)=1,此时随机变量的不确定性最大。</p>

<p>条件熵（conditional entropy）：表示在一直随机变量X的条件下随机变量Y的不确定性度量。</p>

<p>设随机变量(X, Y)，其联合概率分布为 \(P(X, Y) = p_{ij}(i=1,2, ... , n; j=1,2, ... , m)\)，随机变量X给定的条件下随机变量Y的条件熵H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：</p>

<p>\[H(Y|X)=\sum p_iH(Y|X=x_i)\]</p>

<p>这里，\(p_i=P(X=x_i), i=1,2, ... , n\)</p>

<p>ps:熵指的是体系的混乱程度，它在控制论，概率论，数论，天体物理，生命科学等领域都有重要的应用，在不同的学科中也有引申出更为具体的定义，是各个领域十分重要的参量。熵由鲁道夫.克劳修斯提出，并应用在热力学中。后来在，克劳德.埃尔伍德.香农 第一次将熵的概念引入到信息论中来。</p>

<h3 id="toc_74">信息增益（information gain）</h3>

<p>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p>

<p>特征A对训练数据集D的信息增益g(D, A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即</p>

<p>g(D, A)=H(D)-H(D|A)</p>

<p>信息增益大的特征具有更强的分类能力。</p>

<h3 id="toc_75">信息增益比（information gain ratio）</h3>

<p>信息增益比 \(g_R(D, A)\)定义为其信息增益g(D, A)与训练数据集D关于特征A的值的熵\(H_A(D)\)之比，即</p>

<p>\(g_R(D, A)=g(D, A)/H_A(D)\)</p>

<p>其中，\(H_A(D)=-\sum \frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}\), n是特征A取值的个数。</p>

<h3 id="toc_76">基尼指数（gini index）</h3>

<p>分类问题中，假设有K个类，样本属于第k类的概率为\(p_k\)，则概率分布的基尼指数定义为：</p>

<p>\[Gini(p)=\sum p_k(1-p_k)=1-\sum p_k^2\]</p>

<p>对于二分类问题，若样本点属于第1个类的概率是p，则概率分布的基尼指数为：</p>

<p>Gini(p)=2p(1-p)</p>

<p>对于给定的样本集合D，其基尼指数为：</p>

<p>\[Gini(D)=1-\sum(\frac{|C_k|}{|D|})^2\]</p>

<p>这里，\(C_k\)是D中属于第k类的样本子集，k是类的个数。</p>

<p>如果样本集合D根据特征A是否取到某一可能值a被分割成D1和D2两部分，则在特征A的条件下，集合D的基尼指数定义为：</p>

<p>\[Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)\]</p>

<p>基尼指数Gini(D)表示集合D的不确定性，基尼指数越大，样本集合的不确定性也就越大，这一点与熵相似。</p>

<h2 id="toc_77">后验概率</h2>

<p>后验概率是信息论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验证概率。后验概率是指在得到”结果“的信息后重新修正的概率，如贝叶斯公式中的。是执果寻因的问题。后验概率和先验概率有着不可分割的联系，后验的计算要以先验概率为基础，其实说白了后验概率其实就是条件概率。</p>

<h2 id="toc_78">图片解释基本概念</h2>

<p><img src="media/14522640452315/14522703150352.jpg" alt=""/></p>

<p>Test and training error: 为什么低训练误差并不总是一件好的事情呢</p>

<p><img src="media/14522640452315/14522703431703.jpg" alt=""/></p>

<p>Under and overfitting: 低度拟合或者过度拟合的例子。PRML 图1.4.多项式曲线有各种各样的命令M，以红色曲线表示，由绿色曲线适应数据集后生成。</p>

<p><img src="media/14522640452315/14522704479463.jpg" alt=""/></p>

<p>Occam’s razor</p>

<p>这张图给了为什么复杂模型原来是小概率事件这个问题一个基本的直观的解释。水平轴代表了可能的数据集D空间。贝叶斯定理以他们预测的数据出现的程度成比例地反馈模型。这些预测被数据D上归一化概率分布量化。数据的概率给出了一种模型Hi,P(D|Hi)被称作支持Hi模型的证据。一个简单的模型H1仅可以做到一种有限预测，以P(D|H1)展示；一个更加强大的模型H2，举例来说，可以比模型H1拥有更加自由的参数，可以预测更多种类的数据集。这也表明，无论如何，H2在C1域中对数据集的预测做不到像H1那样强大。假设相等的先验概率被分配给这两种模型，之后数据集落在C1区域，不那么强大的模型H1将会是更加合适的模型。</p>

<p><img src="media/14522640452315/14522705350069.jpg" alt=""/></p>

<p>Irrelevant features:</p>

<p>为什么无关紧要的特征会损害KNN，聚类，以及其它以相似点聚集的方法。左右的图展示了两类数据很好地被分离在纵轴上。右图添加了一条不切题的横轴，它破坏了分组，并且使得许多点成为相反类的近邻。</p>

<p><img src="media/14522640452315/14522705911209.jpg" alt=""/></p>

<p>Sparsity:</p>

<p>Lasso算法（L1正规化或者拉普拉斯先验）给出了稀疏的解决方案（比如：带更多0的加权向量）</p>

<p>lasso算法的估算图像(左)以及岭回归算法的估算图像（右）。展示了错误的等值线以及约束函数。分别的，当红色椭圆是最小二乘误差函数的等高线时，实心的蓝色区域是约束区域|β1| + |β2| ≤ t 以及 \(β_1^2 + β_2^2 ≤ t_2\)</p>


</div>

<br /><br />
<hr />

<div class="row clearfix">
  <div class="large-6 columns">
	<div class="text-left" style="padding:15px 0px;">
		
	        <a href="14523493109485.html"  title="Previous Post: 编程算法面试指南">&laquo; 编程算法面试指南</a>
	    
	</div>
  </div>
  <div class="large-6 columns">
	<div class="text-right" style="padding:15px 0px;">
		
	        <a href="14520959875185.html" 
	        title="Next Post: Mac 指南">Mac 指南 &raquo;</a>
	    
	</div>
  </div>
</div>

<div class="row">
<div style="padding:0px 0.93em;" class="share-comments">

</div>
</div>
<script type="text/javascript">
	$(function(){
		var currentURL = '14522640452315.html';
		$('#side-nav a').each(function(){
			if($(this).attr('href') == currentURL){
				$(this).parent().addClass('active');
			}
		});
	});
</script>  
</div></div>


<div class="page-bottom">
  <div class="row">
  <hr />
  <div class="small-9 columns">
  <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
  <div class="small-3 columns">
  <p class="copyright text-right"><a href="#header">TOP</a></p>
  </div>
   
  </div>
</div>

        </section>
      </div>
    </div>
    
    
    <script src="asset/js/foundation.min.js"></script>
    <script src="asset/js/foundation/foundation.offcanvas.js"></script>
    <script>
      $(document).foundation();

     
    </script>
    
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  </body>
</html>
